{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e124418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n",
      "dt=0.2, g/L=4.0, damping=0.05, T=100\n",
      "Q_true diag = [0.0002 0.0008]\n",
      "Q_base diag = [0.001 0.004]\n",
      "R_base diag = [0.0025 0.0025]\n",
      "outliers: p_out=0.08, r_nom=0.05, r_out=0.8\n",
      "\n",
      "Epoch 001 | Train total=nan (MSE=0.752423) | Test MSE=0.538479\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 770\u001b[39m\n\u001b[32m    766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m    769\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m     \u001b[43mtrain_demo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 669\u001b[39m, in \u001b[36mtrain_demo\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    665\u001b[39m P0b = P0.expand(B,-\u001b[32m1\u001b[39m,-\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    667\u001b[39m opt.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m xhat, P_hist, _, param_hist = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43myb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP0b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQ_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDIFF_THROUGH_FH\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDIFF_THROUGH_FH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    671\u001b[39m loss_mse = mse_state(xhat, xb)\n\u001b[32m    672\u001b[39m loss_nll = nll_from_P(xb, xhat, P_hist)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\minhy\\anaconda3\\envs\\ukn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\minhy\\anaconda3\\envs\\ukn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 467\u001b[39m, in \u001b[36mUKNet_RCholesky.forward\u001b[39m\u001b[34m(self, y, f_fn, h_fn, x0, P0, Q_base, DIFF_THROUGH_FH)\u001b[39m\n\u001b[32m    465\u001b[39m z = torch.cat([e0, de, dy, ae, diagP.detach(), diagS.detach()], dim=-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# [B,5m+n]\u001b[39;00m\n\u001b[32m    466\u001b[39m z = \u001b[38;5;28mself\u001b[39m.fc_pre(z)\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    468\u001b[39m params = \u001b[38;5;28mself\u001b[39m.fc_post(h)  \u001b[38;5;66;03m# [B,k]\u001b[39;00m\n\u001b[32m    470\u001b[39m Lr = \u001b[38;5;28mself\u001b[39m.build_LR(params)                \u001b[38;5;66;03m# [B,m,m]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\minhy\\anaconda3\\envs\\ukn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\minhy\\anaconda3\\envs\\ukn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\minhy\\anaconda3\\envs\\ukn\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1814\u001b[39m, in \u001b[36mGRUCell.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1811\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1812\u001b[39m     hx = hx.unsqueeze(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched \u001b[38;5;28;01melse\u001b[39;00m hx\n\u001b[32m-> \u001b[39m\u001b[32m1814\u001b[39m ret = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgru_cell\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1815\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1816\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1817\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight_ih\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1818\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1819\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_ih\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1820\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1821\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1823\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[32m   1824\u001b[39m     ret = ret.squeeze(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CKN-style UKN (UKF + learned R_t via Cholesky + SPD enforcing layer on P)\n",
    "- No linearization needed: accepts arbitrary f_fn, h_fn (ODE/FEA/black-box OK)\n",
    "- Demo: pendulum, dt=0.2\n",
    "    x = [theta, omega]\n",
    "    y = [sin(theta), cos(theta)] + noise\n",
    "    noise is mixture (outlier bursts) so time-varying R_t helps a lot\n",
    "\n",
    "Baselines:\n",
    "- UKF (fixed R_base)\n",
    "- EKF (needs Jacobians; only for demo comparison)\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Repro / device\n",
    "# =========================\n",
    "def set_seed(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def wrap_angle(a: torch.Tensor) -> torch.Tensor:\n",
    "    return (a + math.pi) % (2 * math.pi) - math.pi\n",
    "\n",
    "def state_error(x_true: torch.Tensor, x_hat: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x_true, x_hat: [B,T,2]\n",
    "    theta error wrapped, omega error raw\n",
    "    \"\"\"\n",
    "    e = x_true - x_hat\n",
    "    e_theta = wrap_angle(e[..., 0])\n",
    "    e_omega = e[..., 1]\n",
    "    return torch.stack([e_theta, e_omega], dim=-1)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CKN-style SPD enforcing\n",
    "# =========================\n",
    "def symmetrize(P: torch.Tensor) -> torch.Tensor:\n",
    "    return 0.5 * (P + P.transpose(-1, -2))\n",
    "\n",
    "def spd_project_eig(P: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    CKN-style PD enforcing layer:\n",
    "    - symmetrize\n",
    "    - eigen-decomp\n",
    "    - clamp eigenvalues >= eps\n",
    "    \"\"\"\n",
    "    P = symmetrize(P)\n",
    "    # sanitize\n",
    "    if not torch.isfinite(P).all():\n",
    "        P = torch.nan_to_num(P, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        P = symmetrize(P)\n",
    "\n",
    "    evals, evecs = torch.linalg.eigh(P)           # [...,n]\n",
    "    evals = torch.clamp(evals, min=eps)\n",
    "    return evecs @ torch.diag_embed(evals) @ evecs.transpose(-1, -2)\n",
    "\n",
    "def robust_cholesky(P: torch.Tensor, eps: float = 1e-6, tries: int = 6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Robust Cholesky for batch matrices:\n",
    "    repeatedly apply SPD projection with increasing eps if needed.\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    for k in range(tries):\n",
    "        P2 = spd_project_eig(P, eps=eps * (10.0 ** k))\n",
    "        try:\n",
    "            return torch.linalg.cholesky(P2)\n",
    "        except RuntimeError as e:\n",
    "            last_err = e\n",
    "    raise last_err\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Data: pendulum + outlier bursts\n",
    "# =========================\n",
    "def sample_pendulum(\n",
    "    N: int, T: int, dt: float,\n",
    "    g_over_L: float, damping: float,\n",
    "    Q_true: torch.Tensor,             # [2,2]\n",
    "    r_nom: float, r_out: float, p_out: float,\n",
    "    device: str\n",
    "):\n",
    "    \"\"\"\n",
    "    x: [N,T,2], y: [N,T,2], out_mask: [N,T,1]\n",
    "    y = [sin(theta), cos(theta)] + v\n",
    "    v ~ mixture Gaussian:\n",
    "        prob(1-p_out): N(0, r_nom^2 I)\n",
    "        prob(p_out)  : N(0, r_out^2 I)\n",
    "    \"\"\"\n",
    "    n = 2\n",
    "    LQ = torch.linalg.cholesky(Q_true)\n",
    "\n",
    "    x = torch.zeros(N, T, n, device=device)\n",
    "    y = torch.zeros(N, T, 2, device=device)\n",
    "    out_mask = torch.zeros(N, T, 1, device=device)\n",
    "\n",
    "    # init: make it move faster than \"too slow\" case\n",
    "    x[:, 0, 0] = (torch.rand(N, device=device) * 2 - 1) * math.pi\n",
    "    x[:, 0, 1] = torch.randn(N, device=device) * 1.0  # omega init\n",
    "\n",
    "    I2 = torch.eye(2, device=device)\n",
    "\n",
    "    for t in range(T):\n",
    "        theta = x[:, t, 0]\n",
    "        omega = x[:, t, 1]\n",
    "\n",
    "        # outlier mixture\n",
    "        is_out = (torch.rand(N, device=device) < p_out).float().view(N, 1)\n",
    "        out_mask[:, t, :] = is_out\n",
    "\n",
    "        sigma = r_nom + is_out * (r_out - r_nom)  # [N,1]\n",
    "        v = torch.randn(N, 2, device=device) * sigma  # isotropic\n",
    "\n",
    "        y[:, t, 0] = torch.sin(theta) + v[:, 0]\n",
    "        y[:, t, 1] = torch.cos(theta) + v[:, 1]\n",
    "\n",
    "        if t < T - 1:\n",
    "            theta_next = theta + dt * omega\n",
    "            omega_next = omega + dt * (-g_over_L * torch.sin(theta) - damping * omega)\n",
    "\n",
    "            w = torch.randn(N, n, device=device) @ LQ.T\n",
    "            x[:, t + 1, 0] = wrap_angle(theta_next + w[:, 0])\n",
    "            x[:, t + 1, 1] = omega_next + w[:, 1]\n",
    "\n",
    "    return x, y, out_mask\n",
    "\n",
    "\n",
    "class SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y, out_mask):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.out = out_mask\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i], self.out[i]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Unscented Transform (UT)\n",
    "# =========================\n",
    "def sigma_points(x, P, alpha=0.2, beta=2.0, kappa=0.0):\n",
    "    \"\"\"\n",
    "    x: [B,n], P:[B,n,n]\n",
    "    Xi: [B,2n+1,n], weights Wm/Wc: [2n+1]\n",
    "    \"\"\"\n",
    "    B, n = x.shape\n",
    "    lam = alpha**2 * (n + kappa) - n\n",
    "    c = n + lam\n",
    "    gamma = math.sqrt(max(c, 1e-12))\n",
    "\n",
    "    Wm = x.new_zeros(2*n + 1)\n",
    "    Wc = x.new_zeros(2*n + 1)\n",
    "    Wm[0] = lam / c\n",
    "    Wc[0] = lam / c + (1 - alpha**2 + beta)\n",
    "    Wm[1:] = 1.0 / (2*c)\n",
    "    Wc[1:] = 1.0 / (2*c)\n",
    "\n",
    "    L = robust_cholesky(P, eps=1e-6)  # [B,n,n]\n",
    "    U = (gamma * L).transpose(1, 2)   # [B,n,n] columns as offsets\n",
    "\n",
    "    x0 = x.unsqueeze(1)               # [B,1,n]\n",
    "    Xi = torch.cat([x0, x0 + U, x0 - U], dim=1)  # [B,2n+1,n]\n",
    "    return Xi, Wm, Wc\n",
    "\n",
    "def ut_mean_cov(X, Wm, Wc, noise=None, eps=1e-6):\n",
    "    \"\"\"\n",
    "    X: [B,L,d]\n",
    "    mean:[B,d], cov:[B,d,d]\n",
    "    noise can be [d,d] or [B,d,d]\n",
    "    \"\"\"\n",
    "    B, L, d = X.shape\n",
    "    mean = torch.sum(Wm.view(1, L, 1) * X, dim=1)\n",
    "    Xm = X - mean.unsqueeze(1)\n",
    "\n",
    "    cov = torch.zeros(B, d, d, device=X.device, dtype=X.dtype)\n",
    "    for i in range(L):\n",
    "        vi = Xm[:, i, :].unsqueeze(-1)\n",
    "        cov = cov + Wc[i] * (vi @ vi.transpose(-1, -2))\n",
    "\n",
    "    if noise is not None:\n",
    "        cov = cov + (noise if noise.dim() == 3 else noise.unsqueeze(0))\n",
    "\n",
    "    cov = spd_project_eig(cov, eps=eps)\n",
    "    return mean, cov\n",
    "\n",
    "def cross_cov(X, Y, xm, ym, Wc):\n",
    "    \"\"\"\n",
    "    X:[B,L,n], Y:[B,L,m] -> Pxy:[B,n,m]\n",
    "    \"\"\"\n",
    "    B, L, n = X.shape\n",
    "    m = Y.shape[-1]\n",
    "    Xc = X - xm.unsqueeze(1)\n",
    "    Yc = Y - ym.unsqueeze(1)\n",
    "\n",
    "    Pxy = torch.zeros(B, n, m, device=X.device, dtype=X.dtype)\n",
    "    for i in range(L):\n",
    "        xi = Xc[:, i, :].unsqueeze(-1)\n",
    "        yi = Yc[:, i, :].unsqueeze(-1)\n",
    "        Pxy = Pxy + Wc[i] * (xi @ yi.transpose(-1, -2))\n",
    "    return Pxy\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Baseline UKF (fixed Q/R)\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def batch_ukf(y, f_fn, h_fn, Q, R, x0, P0, ut_params=(0.2,2.0,0.0), DIFF_THROUGH_FH=False):\n",
    "    alpha, beta, kappa = ut_params\n",
    "    B, T, m = y.shape\n",
    "    x = x0\n",
    "    P = spd_project_eig(P0, eps=1e-6)\n",
    "\n",
    "    xs, Ps = [], []\n",
    "    for t in range(T):\n",
    "        Xi, Wm, Wc = sigma_points(x, P, alpha, beta, kappa)\n",
    "\n",
    "        # f/h may be black-box\n",
    "        if DIFF_THROUGH_FH:\n",
    "            X_pred = f_fn(Xi)\n",
    "            Y_pred = h_fn(X_pred)\n",
    "        else:\n",
    "            X_pred = f_fn(Xi)\n",
    "            Y_pred = h_fn(X_pred)\n",
    "\n",
    "        x_pred, P_pred = ut_mean_cov(X_pred, Wm, Wc, noise=Q)\n",
    "        y_pred, S      = ut_mean_cov(Y_pred, Wm, Wc, noise=R)\n",
    "\n",
    "        Pxy = cross_cov(X_pred, Y_pred, x_pred, y_pred, Wc)\n",
    "\n",
    "        Ls = robust_cholesky(S, eps=1e-9)\n",
    "        KT = torch.cholesky_solve(Pxy.transpose(1,2), Ls)  # [B,m,n]\n",
    "        K  = KT.transpose(1,2)                              # [B,n,m]\n",
    "\n",
    "        e = y[:, t, :] - y_pred\n",
    "        x = x_pred + torch.bmm(K, e.unsqueeze(-1)).squeeze(-1)\n",
    "        x = torch.stack([wrap_angle(x[:,0]), x[:,1]], dim=-1)\n",
    "\n",
    "        P = P_pred - torch.bmm(torch.bmm(K, S), K.transpose(-1,-2))\n",
    "        P = spd_project_eig(P, eps=1e-6)\n",
    "\n",
    "        xs.append(x); Ps.append(P)\n",
    "\n",
    "    return torch.stack(xs, dim=1), torch.stack(Ps, dim=1)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# EKF baseline (demo only, needs Jacobians)\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def batch_ekf(y, dt, g_over_L, damping, Q, R, x0, P0):\n",
    "    \"\"\"\n",
    "    y=[sin(theta), cos(theta)]\n",
    "    H = [[cos(theta), 0],\n",
    "         [-sin(theta),0]]\n",
    "    \"\"\"\n",
    "    device = y.device\n",
    "    B, T, m = y.shape\n",
    "    n = 2\n",
    "    I = torch.eye(n, device=device, dtype=y.dtype).unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "    x = x0\n",
    "    P = spd_project_eig(P0, eps=1e-6)\n",
    "\n",
    "    xs, Ps = [], []\n",
    "    for t in range(T):\n",
    "        theta = x[:,0]\n",
    "        omega = x[:,1]\n",
    "\n",
    "        # predict\n",
    "        theta_p = wrap_angle(theta + dt*omega)\n",
    "        omega_p = omega + dt*(-g_over_L*torch.sin(theta) - damping*omega)\n",
    "        x_pred = torch.stack([theta_p, omega_p], dim=-1)\n",
    "\n",
    "        F = torch.zeros(B,2,2,device=device,dtype=y.dtype)\n",
    "        F[:,0,0] = 1.0\n",
    "        F[:,0,1] = dt\n",
    "        F[:,1,0] = -dt*g_over_L*torch.cos(theta)\n",
    "        F[:,1,1] = 1.0 - dt*damping\n",
    "\n",
    "        P_pred = F @ P @ F.transpose(-1,-2) + Q.unsqueeze(0).expand(B,-1,-1)\n",
    "        P_pred = spd_project_eig(P_pred, eps=1e-6)\n",
    "\n",
    "        # update\n",
    "        y_pred = torch.stack([torch.sin(theta_p), torch.cos(theta_p)], dim=-1)  # [B,2]\n",
    "        e = y[:,t,:] - y_pred\n",
    "\n",
    "        H = torch.zeros(B,2,2,device=device,dtype=y.dtype)\n",
    "        H[:,0,0] = torch.cos(theta_p)\n",
    "        H[:,1,0] = -torch.sin(theta_p)\n",
    "        H[:,0,1] = 0.0\n",
    "        H[:,1,1] = 0.0\n",
    "\n",
    "        S = H @ P_pred @ H.transpose(-1,-2) + R.unsqueeze(0).expand(B,-1,-1)\n",
    "        S = spd_project_eig(S, eps=1e-9)\n",
    "\n",
    "        Ls = robust_cholesky(S, eps=1e-9)\n",
    "        # K = P_pred H^T S^{-1}\n",
    "        KT = torch.cholesky_solve((P_pred @ H.transpose(-1,-2)).transpose(1,2), Ls)  # [B,2,2]\n",
    "        K  = KT.transpose(1,2)\n",
    "\n",
    "        x = x_pred + torch.bmm(K, e.unsqueeze(-1)).squeeze(-1)\n",
    "        x = torch.stack([wrap_angle(x[:,0]), x[:,1]], dim=-1)\n",
    "\n",
    "        # Joseph form\n",
    "        KH = torch.bmm(K, H)\n",
    "        A = I - KH\n",
    "        P = A @ P_pred @ A.transpose(-1,-2) + torch.bmm(torch.bmm(K, R.unsqueeze(0).expand(B,-1,-1)), K.transpose(-1,-2))\n",
    "        P = spd_project_eig(P, eps=1e-6)\n",
    "\n",
    "        xs.append(x); Ps.append(P)\n",
    "\n",
    "    return torch.stack(xs, dim=1), torch.stack(Ps, dim=1)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# UKN: learn R_t via Cholesky (CKN-style PD)\n",
    "# =========================\n",
    "class UKNet_RCholesky(nn.Module):\n",
    "    \"\"\"\n",
    "    UKF core + GRU predicts Cholesky factor of R_t (lower-triangular with positive diag).\n",
    "    Q is kept constant (user gives Q_base).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n: int, m: int,\n",
    "        R_base: torch.Tensor,            # [m,m] SPD\n",
    "        hidden_size: int = 64,\n",
    "        ut_params=(0.2, 2.0, 0.0),\n",
    "        diag_clip: float = 3.0,          # clip on log-diag delta\n",
    "        off_max: float = 0.25,           # tanh scaling for off-diag delta\n",
    "        r_min: float = 1e-6              # minimum diag magnitude\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.alpha, self.beta, self.kappa = ut_params\n",
    "        self.diag_clip = diag_clip\n",
    "        self.off_max = off_max\n",
    "        self.r_min = r_min\n",
    "\n",
    "        # Base Cholesky of R_base\n",
    "        with torch.no_grad():\n",
    "            Lb = torch.linalg.cholesky(R_base)\n",
    "            logd = torch.log(torch.diagonal(Lb))\n",
    "        self.register_buffer(\"L_base\", Lb)         # [m,m]\n",
    "        self.register_buffer(\"log_diag_base\", logd) # [m]\n",
    "\n",
    "        # Feature dimension: e, de, dy, |e|, diagP, diagS\n",
    "        # e/de/dy/|e|: 4m, diagP: n, diagS: m  => 5m + n\n",
    "        in_dim = 5*m + n\n",
    "\n",
    "        # \"FC - GRU - FC\" (you asked for FC on both sides)\n",
    "        self.fc_pre = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.gru = nn.GRUCell(hidden_size, hidden_size)\n",
    "\n",
    "        # outputs: diag deltas (m) + off-diag deltas (m(m-1)/2)\n",
    "        k = m + (m*(m-1))//2\n",
    "        self.fc_post = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, k),\n",
    "        )\n",
    "\n",
    "        # start near UKF baseline\n",
    "        nn.init.zeros_(self.fc_post[-1].weight)\n",
    "        nn.init.zeros_(self.fc_post[-1].bias)\n",
    "\n",
    "    def build_LR(self, params: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        params: [B, k], k = m + m(m-1)/2\n",
    "        return L_R: [B,m,m] lower-tri with positive diagonal\n",
    "        \"\"\"\n",
    "        B = params.shape[0]\n",
    "        m = self.m\n",
    "\n",
    "        diag_raw = params[:, :m]                   # [B,m]\n",
    "        off_raw  = params[:, m:]                   # [B, m(m-1)/2]\n",
    "\n",
    "        # diagonal: exp(log_diag_base + clipped_delta) + r_min\n",
    "        d = torch.clamp(diag_raw, -self.diag_clip, self.diag_clip)\n",
    "        diag = torch.exp(self.log_diag_base.view(1,m) + d) + self.r_min  # [B,m]\n",
    "\n",
    "        # start from base\n",
    "        L = self.L_base.view(1,m,m).expand(B,-1,-1).clone()\n",
    "\n",
    "        # set diag (out-of-place-ish on a fresh clone)\n",
    "        idx = torch.arange(m, device=L.device)\n",
    "        L[:, idx, idx] = diag\n",
    "\n",
    "        # set lower off-diagonals\n",
    "        k = 0\n",
    "        for i in range(1, m):\n",
    "            for j in range(i):\n",
    "                delta = torch.tanh(off_raw[:, k]) * self.off_max\n",
    "                L[:, i, j] = self.L_base[i, j] + delta\n",
    "                k += 1\n",
    "\n",
    "        return L\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        y: torch.Tensor,              # [B,T,m]\n",
    "        f_fn, h_fn,\n",
    "        x0: torch.Tensor,             # [B,n]\n",
    "        P0: torch.Tensor,             # [B,n,n]\n",
    "        Q_base: torch.Tensor,         # [n,n] (constant)\n",
    "        DIFF_THROUGH_FH: bool = False\n",
    "    ):\n",
    "        device = y.device\n",
    "        B, T, m = y.shape\n",
    "        n = self.n\n",
    "\n",
    "        x = x0\n",
    "        P = spd_project_eig(P0, eps=1e-6)\n",
    "\n",
    "        h = torch.zeros(B, self.gru.hidden_size, device=device, dtype=y.dtype)\n",
    "        e_prev = torch.zeros(B, m, device=device, dtype=y.dtype)\n",
    "        y_prev = torch.zeros(B, m, device=device, dtype=y.dtype)\n",
    "\n",
    "        x_list, P_list, R_list, param_list = [], [], [], []\n",
    "\n",
    "        # feature covariances (tiny) for stable diagP/diagS features\n",
    "        Q_feat = torch.eye(n, device=device, dtype=y.dtype).unsqueeze(0).expand(B,-1,-1) * 1e-6\n",
    "        R_feat = torch.eye(m, device=device, dtype=y.dtype).unsqueeze(0).expand(B,-1,-1) * 1e-6\n",
    "\n",
    "        for t in range(T):\n",
    "            Xi, Wm, Wc = sigma_points(x, P, self.alpha, self.beta, self.kappa)\n",
    "\n",
    "            # f/h evaluation: allow black-box by default (no grad through them)\n",
    "            if DIFF_THROUGH_FH:\n",
    "                X_pred = f_fn(Xi)\n",
    "                Y_pred0 = h_fn(X_pred)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    X_pred = f_fn(Xi)\n",
    "                    Y_pred0 = h_fn(X_pred)\n",
    "\n",
    "            # provisional stats for features\n",
    "            x_pred0, P_pred0 = ut_mean_cov(X_pred, Wm, Wc, noise=Q_feat)\n",
    "            y_pred0, S0      = ut_mean_cov(Y_pred0, Wm, Wc, noise=R_feat)\n",
    "\n",
    "            e0 = y[:, t, :] - y_pred0\n",
    "            de = e0 - e_prev\n",
    "            dy = y[:, t, :] - y_prev\n",
    "            ae = torch.abs(e0)\n",
    "\n",
    "            diagP = torch.diagonal(P_pred0, dim1=-2, dim2=-1)  # [B,n]\n",
    "            diagS = torch.diagonal(S0,      dim1=-2, dim2=-1)  # [B,m]\n",
    "\n",
    "            z = torch.cat([e0, de, dy, ae, diagP.detach(), diagS.detach()], dim=-1)  # [B,5m+n]\n",
    "            z = self.fc_pre(z)\n",
    "            h = self.gru(z, h)\n",
    "            params = self.fc_post(h)  # [B,k]\n",
    "\n",
    "            Lr = self.build_LR(params)                # [B,m,m]\n",
    "            R_t = Lr @ Lr.transpose(-1,-2)            # [B,m,m] SPD guaranteed\n",
    "            R_t = spd_project_eig(R_t, eps=1e-9)\n",
    "\n",
    "            # full UKF predict/update using Q_base & learned R_t\n",
    "            x_pred, P_pred = ut_mean_cov(X_pred, Wm, Wc, noise=Q_base)  # Q_base broadcast inside ut_mean_cov\n",
    "\n",
    "            if DIFF_THROUGH_FH:\n",
    "                Y_pred = h_fn(X_pred)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    Y_pred = h_fn(X_pred)\n",
    "\n",
    "            y_pred, S = ut_mean_cov(Y_pred, Wm, Wc, noise=R_t)\n",
    "\n",
    "            Pxy = cross_cov(X_pred, Y_pred, x_pred, y_pred, Wc)\n",
    "\n",
    "            Ls = robust_cholesky(S, eps=1e-9)\n",
    "            KT = torch.cholesky_solve(Pxy.transpose(1,2), Ls)  # [B,m,n]\n",
    "            K  = KT.transpose(1,2)                              # [B,n,m]\n",
    "\n",
    "            e = y[:, t, :] - y_pred\n",
    "            x = x_pred + torch.bmm(K, e.unsqueeze(-1)).squeeze(-1)\n",
    "            x = torch.stack([wrap_angle(x[:,0]), x[:,1]], dim=-1)\n",
    "\n",
    "            P = P_pred - torch.bmm(torch.bmm(K, S), K.transpose(-1,-2))\n",
    "            P = spd_project_eig(P, eps=1e-6)\n",
    "\n",
    "            x_list.append(x)\n",
    "            P_list.append(P)\n",
    "            R_list.append(R_t)\n",
    "            param_list.append(params)\n",
    "\n",
    "            e_prev = e.detach()\n",
    "            y_prev = y[:, t, :].detach()\n",
    "\n",
    "        return (\n",
    "            torch.stack(x_list, dim=1),         # xhat [B,T,n]\n",
    "            torch.stack(P_list, dim=1),         # P_hist [B,T,n,n]\n",
    "            torch.stack(R_list, dim=1),         # R_hist [B,T,m,m]\n",
    "            torch.stack(param_list, dim=1),     # raw params [B,T,k]\n",
    "        )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Losses + plots\n",
    "# =========================\n",
    "def mse_state(xhat, xtrue):\n",
    "    e = state_error(xtrue, xhat)\n",
    "    return (e**2).mean()\n",
    "\n",
    "def nll_from_P(xtrue, xhat, P_hist, eps=1e-9):\n",
    "    \"\"\"\n",
    "    Gaussian NLL (up to constant):\n",
    "    0.5*(e^T P^{-1} e + logdet(P))\n",
    "    Can be negative if logdet(P) < 0 and errors small -> that's OK.\n",
    "    \"\"\"\n",
    "    e = state_error(xtrue, xhat)                # [B,T,2]\n",
    "    B, T, n = e.shape\n",
    "    e2 = e.reshape(B*T, n, 1)\n",
    "    P2 = P_hist.reshape(B*T, n, n)\n",
    "    P2 = spd_project_eig(P2, eps=1e-6)\n",
    "    L  = robust_cholesky(P2, eps=1e-6)\n",
    "\n",
    "    sol = torch.cholesky_solve(e2, L)\n",
    "    maha = (e2.transpose(1,2) @ sol).reshape(B, T)\n",
    "\n",
    "    diag = torch.diagonal(L, dim1=-2, dim2=-1)\n",
    "    logdet = 2.0 * torch.sum(torch.log(diag + eps), dim=-1).reshape(B, T)\n",
    "\n",
    "    return 0.5 * (maha + logdet).mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_error_ci_99(x_true_1, x_hat_1, P_1, title_prefix=\"\"):\n",
    "    z99 = 2.5758293035489004\n",
    "    T = x_true_1.shape[0]\n",
    "    t = np.arange(T)\n",
    "\n",
    "    err_theta = ((x_true_1[:,0] - x_hat_1[:,0] + np.pi) % (2*np.pi)) - np.pi\n",
    "    err_omega = (x_true_1[:,1] - x_hat_1[:,1])\n",
    "    err = np.stack([err_theta, err_omega], axis=1)\n",
    "\n",
    "    var = np.stack([np.diag(P_1[k]) for k in range(T)], axis=0)\n",
    "    std = np.sqrt(np.maximum(var, 1e-12))\n",
    "    band = z99 * std\n",
    "\n",
    "    names = [\"theta\", \"omega\"]\n",
    "    for i in range(2):\n",
    "        plt.figure()\n",
    "        plt.plot(t, err[:,i], label=f\"error ({names[i]})\")\n",
    "        plt.fill_between(t, -band[:,i], band[:,i], alpha=0.2, label=\"99% CI from P\")\n",
    "        plt.axhline(0.0, linewidth=1)\n",
    "        plt.xlabel(\"Time step\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.title(f\"{title_prefix} Error + 99% CI ({names[i]})\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Training demo\n",
    "# =========================\n",
    "def train_demo():\n",
    "    set_seed(0)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.float32\n",
    "\n",
    "    # -------------------------\n",
    "    # Problem settings (you can tune)\n",
    "    # -------------------------\n",
    "    dt = 0.2\n",
    "    g_over_L = 4.0       # make motion faster (avoid \"too slow\")\n",
    "    damping = 0.05\n",
    "\n",
    "    T = 100              # longer horizon -> easier to separate filters\n",
    "    N_train, N_test = 6000, 1200\n",
    "\n",
    "    # True process noise\n",
    "    Q_true = torch.diag(torch.tensor([2e-4, 8e-4], device=device, dtype=dtype))\n",
    "\n",
    "    # Outlier mixture (measurement)\n",
    "    r_nom = 0.05\n",
    "    r_out = 0.8\n",
    "    p_out = 0.08\n",
    "\n",
    "    # Baseline assumed covs (intentionally optimistic for measurement)\n",
    "    Q_base = 5.0 * Q_true\n",
    "    R_base = (r_nom**2) * torch.eye(2, device=device, dtype=dtype)  # UKF baseline uses this fixed R\n",
    "\n",
    "    ut_params = (0.2, 2.0, 0.0)\n",
    "\n",
    "    print(\"device =\", device)\n",
    "    print(f\"dt={dt}, g/L={g_over_L}, damping={damping}, T={T}\")\n",
    "    print(\"Q_true diag =\", torch.diag(Q_true).detach().cpu().numpy())\n",
    "    print(\"Q_base diag =\", torch.diag(Q_base).detach().cpu().numpy())\n",
    "    print(\"R_base diag =\", torch.diag(R_base).detach().cpu().numpy())\n",
    "    print(f\"outliers: p_out={p_out}, r_nom={r_nom}, r_out={r_out}\\n\")\n",
    "\n",
    "    # f/h (vectorized over sigma points)\n",
    "    def f_fn(X):\n",
    "        th = X[...,0]\n",
    "        om = X[...,1]\n",
    "        th2 = wrap_angle(th + dt*om)\n",
    "        om2 = om + dt*(-g_over_L*torch.sin(th) - damping*om)\n",
    "        return torch.stack([th2, om2], dim=-1)\n",
    "\n",
    "    def h_fn(X):\n",
    "        th = X[...,0]\n",
    "        return torch.stack([torch.sin(th), torch.cos(th)], dim=-1)\n",
    "\n",
    "    # data\n",
    "    x_tr, y_tr, out_tr = sample_pendulum(N_train, T, dt, g_over_L, damping, Q_true, r_nom, r_out, p_out, device)\n",
    "    x_te, y_te, out_te = sample_pendulum(N_test,  T, dt, g_over_L, damping, Q_true, r_nom, r_out, p_out, device)\n",
    "\n",
    "    train_loader = DataLoader(SeqDataset(x_tr, y_tr, out_tr), batch_size=128, shuffle=True, num_workers=0)\n",
    "    test_loader  = DataLoader(SeqDataset(x_te, y_te, out_te), batch_size=256, shuffle=False, num_workers=0)\n",
    "\n",
    "    # init prior\n",
    "    x0 = torch.zeros(1,2, device=device, dtype=dtype)\n",
    "    P0 = torch.diag(torch.tensor([1.0, 1.0], device=device, dtype=dtype)).unsqueeze(0)\n",
    "\n",
    "    # model\n",
    "    model = UKNet_RCholesky(\n",
    "        n=2, m=2,\n",
    "        R_base=R_base,\n",
    "        hidden_size=64,\n",
    "        ut_params=ut_params,\n",
    "        diag_clip=3.0,\n",
    "        off_max=0.25,\n",
    "        r_min=1e-6\n",
    "    ).to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-6)\n",
    "\n",
    "    # -------------------------\n",
    "    # Epoch control: 늘리고 싶으면 여기!\n",
    "    # -------------------------\n",
    "    num_epochs = 60   # <<<<<<<<<<<<<<<<<<<<<<<< 여기서 epoch 늘리면 됨\n",
    "\n",
    "    beta_nll = 0.10   # total = (1-beta)*MSE + beta*NLL\n",
    "    lam_smooth = 1e-3 # R-parameter smoothness penalty (optional but helpful)\n",
    "\n",
    "    DIFF_THROUGH_FH = False  # black-box/FEA/ODE면 False 추천\n",
    "\n",
    "    train_hist, test_hist = [], []\n",
    "\n",
    "    for ep in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        tot_loss = 0.0\n",
    "        tot_mse  = 0.0\n",
    "\n",
    "        for xb, yb, _ in train_loader:\n",
    "            B = xb.shape[0]\n",
    "            x0b = x0.expand(B,-1).contiguous()\n",
    "            P0b = P0.expand(B,-1,-1).contiguous()\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            xhat, P_hist, _, param_hist = model(yb, f_fn, h_fn, x0b, P0b, Q_base=Q_base, DIFF_THROUGH_FH=DIFF_THROUGH_FH)\n",
    "\n",
    "            loss_mse = mse_state(xhat, xb)\n",
    "            loss_nll = nll_from_P(xb, xhat, P_hist)\n",
    "\n",
    "            # smoothness penalty on params (discourage \"Rt jumping\")\n",
    "            dp = param_hist[:,1:,:] - param_hist[:,:-1,:]\n",
    "            loss_smooth = (dp**2).mean()\n",
    "\n",
    "            loss = (1.0 - beta_nll)*loss_mse + beta_nll*loss_nll + lam_smooth*loss_smooth\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            tot_loss += loss.item() * B\n",
    "            tot_mse  += loss_mse.item() * B\n",
    "\n",
    "        train_loss = tot_loss / len(train_loader.dataset)\n",
    "        train_mse  = tot_mse  / len(train_loader.dataset)\n",
    "\n",
    "        # test MSE\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot = 0.0\n",
    "            for xb, yb, _ in test_loader:\n",
    "                B = xb.shape[0]\n",
    "                x0b = x0.expand(B,-1).contiguous()\n",
    "                P0b = P0.expand(B,-1,-1).contiguous()\n",
    "                xhat, _, _, _ = model(yb, f_fn, h_fn, x0b, P0b, Q_base=Q_base, DIFF_THROUGH_FH=DIFF_THROUGH_FH)\n",
    "                tot += mse_state(xhat, xb).item() * B\n",
    "            test_mse = tot / len(test_loader.dataset)\n",
    "\n",
    "        train_hist.append(train_loss)\n",
    "        test_hist.append(test_mse)\n",
    "\n",
    "        if ep % 5 == 0 or ep == 1:\n",
    "            print(f\"Epoch {ep:03d} | Train total={train_loss:.6f} (MSE={train_mse:.6f}) | Test MSE={test_mse:.6f}\")\n",
    "\n",
    "    # plot losses\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(1, num_epochs+1), train_hist, label=\"Train total\")\n",
    "    plt.plot(np.arange(1, num_epochs+1), test_hist, label=\"Test MSE\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss/MSE\")\n",
    "    plt.title(\"Training history (UKN: R_t via Cholesky + SPD layer)\")\n",
    "    plt.legend(); plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    # final comparison\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        B = y_te.shape[0]\n",
    "        x0b = x0.expand(B,-1).contiguous()\n",
    "        P0b = P0.expand(B,-1,-1).contiguous()\n",
    "\n",
    "        # UKN\n",
    "        x_ukn, P_ukn, _, _ = model(y_te, f_fn, h_fn, x0b, P0b, Q_base=Q_base, DIFF_THROUGH_FH=DIFF_THROUGH_FH)\n",
    "        mse_ukn = mse_state(x_ukn, x_te).item()\n",
    "\n",
    "        # UKF fixed\n",
    "        x_ukf, P_ukf = batch_ukf(y_te, f_fn, h_fn, Q_base, R_base, x0b, P0b, ut_params=ut_params, DIFF_THROUGH_FH=False)\n",
    "        mse_ukf = mse_state(x_ukf, x_te).item()\n",
    "\n",
    "        # EKF fixed (demo)\n",
    "        x_ekf, P_ekf = batch_ekf(y_te, dt, g_over_L, damping, Q_base, R_base, x0b, P0b)\n",
    "        mse_ekf = mse_state(x_ekf, x_te).item()\n",
    "\n",
    "        print(\"\\n===== Final Test MSE =====\")\n",
    "        print(f\"UKN : {mse_ukn:.6e}\")\n",
    "        print(f\"UKF : {mse_ukf:.6e}\")\n",
    "        print(f\"EKF : {mse_ekf:.6e}\")\n",
    "\n",
    "        # sample trajectory\n",
    "        idx = 0\n",
    "        t = np.arange(T)\n",
    "\n",
    "        x_true_1 = x_te[idx].cpu().numpy()\n",
    "        x_ukn_1  = x_ukn[idx].cpu().numpy()\n",
    "        x_ukf_1  = x_ukf[idx].cpu().numpy()\n",
    "        x_ekf_1  = x_ekf[idx].cpu().numpy()\n",
    "\n",
    "        names = [\"theta\", \"omega\"]\n",
    "        for i in range(2):\n",
    "            plt.figure()\n",
    "            plt.plot(t, x_true_1[:,i], label=f\"True {names[i]}\")\n",
    "            plt.plot(t, x_ukn_1[:,i],  label=f\"UKN  {names[i]}\")\n",
    "            plt.plot(t, x_ukf_1[:,i],  label=f\"UKF  {names[i]}\")\n",
    "            plt.plot(t, x_ekf_1[:,i],  label=f\"EKF  {names[i]}\")\n",
    "            plt.xlabel(\"Time step\"); plt.ylabel(names[i])\n",
    "            plt.title(f\"Sample trajectory (idx={idx}) - {names[i]}\")\n",
    "            plt.legend(); plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "            plt.show()\n",
    "\n",
    "        # LAST PLOT: per-state error + 99% CI from P (UKN)\n",
    "        P_ukn_1 = P_ukn[idx].cpu().numpy()\n",
    "        plot_error_ci_99(x_true_1, x_ukn_1, P_ukn_1, title_prefix=\"UKN\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_demo()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
