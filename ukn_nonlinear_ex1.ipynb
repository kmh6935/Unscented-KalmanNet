{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e124418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Adopted Nonlinear Problem (dt=0.2): Pendulum + Outlier Measurements + y=[sin,cos] ===\n",
      "device=cpu\n",
      "dt=0.2, g/L=1.5, damping=0.03\n",
      "Q_true diag = [0.0005 0.002 ]\n",
      "Q_base diag = [0.004 0.016] (gamma_Q=8.0)\n",
      "R_base diag = [0.0064 0.0064] (~ r_nom^2 I)\n",
      "Measurement noise: nominal std=0.08, outlier std=1.0, p_out=0.06\n",
      "\n",
      "Epoch 01 | Train total: 1.738306 (MSE=1.060322) | Test MSE: 1.104195\n",
      "Epoch 02 | Train total: 1.754052 (MSE=1.067480) | Test MSE: 1.126536\n",
      "Epoch 03 | Train total: 1.829426 (MSE=1.123686) | Test MSE: 1.221302\n",
      "Epoch 04 | Train total: 1.888822 (MSE=1.164145) | Test MSE: 1.253752\n",
      "Epoch 05 | Train total: 1.861185 (MSE=1.193488) | Test MSE: 1.186606\n",
      "Epoch 06 | Train total: 1.695645 (MSE=1.333348) | Test MSE: 1.458809\n",
      "Epoch 07 | Train total: 1.726726 (MSE=1.449187) | Test MSE: 1.444406\n",
      "Epoch 08 | Train total: 1.705366 (MSE=1.377525) | Test MSE: 1.391967\n",
      "Epoch 09 | Train total: 1.659776 (MSE=1.271194) | Test MSE: 1.182407\n",
      "Epoch 10 | Train total: 1.731441 (MSE=1.184647) | Test MSE: 1.172999\n",
      "Epoch 11 | Train total: 1.826818 (MSE=1.222989) | Test MSE: 1.222397\n",
      "Epoch 12 | Train total: 1.747226 (MSE=1.266409) | Test MSE: 1.294067\n",
      "Epoch 13 | Train total: 1.719789 (MSE=1.273469) | Test MSE: 1.324399\n",
      "Epoch 14 | Train total: 1.722579 (MSE=1.308238) | Test MSE: 1.413292\n",
      "Epoch 15 | Train total: 1.738398 (MSE=1.282505) | Test MSE: 1.287231\n",
      "Epoch 16 | Train total: 1.678464 (MSE=1.231597) | Test MSE: 1.198765\n",
      "Epoch 17 | Train total: 1.680200 (MSE=1.137815) | Test MSE: 1.115123\n",
      "Epoch 18 | Train total: 1.738742 (MSE=1.081780) | Test MSE: 1.130192\n",
      "Epoch 19 | Train total: 2.019347 (MSE=1.129657) | Test MSE: 1.200652\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Nonlinear demo (dt=0.2, y=[sin(theta), cos(theta)]) where\n",
    "UKN = UKF + learned time-varying R_t (Cholesky-param) and constant Q\n",
    "compared with UKF (fixed R) and EKF (fixed R).\n",
    "\n",
    "State: x = [theta, omega]^T\n",
    "Dynamics:\n",
    "  theta_{t+1} = theta_t + dt * omega_t\n",
    "  omega_{t+1} = omega_t + dt * (-g/L*sin(theta_t) - c*omega_t) + w_t\n",
    "\n",
    "Measurement:\n",
    "  y_t = [sin(theta_t), cos(theta_t)] + v_t\n",
    "Measurement noise is a mixture (outliers):\n",
    "  with prob (1-p_out): v ~ N(0, r_nom^2 I)\n",
    "  with prob p_out     : v ~ N(0, r_out^2 I)\n",
    "\n",
    "UKF/EKF baselines assume fixed R_base ~ r_nom^2 I (optimistic vs outliers).\n",
    "UKN learns R_t via Cholesky scheme (NOT scalar*R_base).\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Repro / device\n",
    "# -------------------------\n",
    "def set_seed(seed: int = 0):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Angle utils\n",
    "# -------------------------\n",
    "def wrap_angle(a: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Wrap to [-pi, pi].\"\"\"\n",
    "    return (a + math.pi) % (2 * math.pi) - math.pi\n",
    "\n",
    "\n",
    "def state_error(x_true: torch.Tensor, x_hat: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x_true, x_hat: [...,2] -> error [...,2]\n",
    "    theta error wrapped, omega error standard.\n",
    "    \"\"\"\n",
    "    e = x_true - x_hat\n",
    "    e_theta = wrap_angle(e[..., 0])\n",
    "    e_omega = e[..., 1]\n",
    "    return torch.stack([e_theta, e_omega], dim=-1)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# SPD / numerics (autograd-safe)\n",
    "# -------------------------\n",
    "def symmetrize(P: torch.Tensor) -> torch.Tensor:\n",
    "    return 0.5 * (P + P.transpose(-1, -2))\n",
    "\n",
    "\n",
    "def ensure_spd_batch(P: torch.Tensor, eps: float = 1e-6, max_shift: float = 1e6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Per-batch diagonal shift so min eigenvalue >= eps.\n",
    "    Also sanitizes NaN/Inf.\n",
    "    \"\"\"\n",
    "    P = symmetrize(P)\n",
    "\n",
    "    if not torch.isfinite(P).all():\n",
    "        P = torch.nan_to_num(P, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        P = symmetrize(P)\n",
    "\n",
    "    B, n, _ = P.shape\n",
    "    I = torch.eye(n, device=P.device, dtype=P.dtype).unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        eigmin = torch.linalg.eigvalsh(P).min(dim=-1).values  # [B]\n",
    "        shift = torch.clamp(eps - eigmin, min=0.0, max=max_shift)  # [B]\n",
    "\n",
    "    return P + shift.view(B, 1, 1) * I\n",
    "\n",
    "\n",
    "def robust_cholesky(P: torch.Tensor, eps: float = 1e-6, tries: int = 7) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Robust batch Cholesky: retries with increasing eps.\n",
    "    Avoid in-place patching -> safer for autograd.\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    for k in range(tries):\n",
    "        P2 = ensure_spd_batch(P, eps=eps * (10.0 ** k))\n",
    "        P2 = symmetrize(P2)\n",
    "        try:\n",
    "            return torch.linalg.cholesky(P2)\n",
    "        except RuntimeError as e:\n",
    "            last_err = e\n",
    "    raise last_err\n",
    "\n",
    "\n",
    "def l2_normalize(x: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    n = torch.norm(x, dim=-1, keepdim=True)\n",
    "    return x / (n + eps)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Nonlinear data generation (pendulum + outliers) with y=[sin,cos]\n",
    "# -------------------------\n",
    "def sample_pendulum_sequences_sincos(\n",
    "    num_seq: int,\n",
    "    T: int,\n",
    "    dt: float,\n",
    "    g_over_L: float,\n",
    "    damping: float,\n",
    "    Q_true: torch.Tensor,  # [2,2]\n",
    "    r_nom: float,\n",
    "    r_out: float,\n",
    "    p_out: float,\n",
    "    device: str = \"cpu\",\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x: [N,T,2]  (theta, omega)\n",
    "      y: [N,T,2]  ([sin(theta), cos(theta)] + noise)\n",
    "      out_mask: [N,T,1]  (1 if outlier used)\n",
    "    \"\"\"\n",
    "    N = num_seq\n",
    "    n = 2\n",
    "    m = 2\n",
    "\n",
    "    LQ = torch.linalg.cholesky(Q_true)\n",
    "\n",
    "    x = torch.zeros(N, T, n, device=device)\n",
    "    y = torch.zeros(N, T, m, device=device)\n",
    "    out_mask = torch.zeros(N, T, 1, device=device)\n",
    "\n",
    "    # init (조금 더 움직이게)\n",
    "    x[:, 0, 0] = (torch.rand(N, device=device) * 2.0 - 1.0) * math.pi  # theta in [-pi,pi]\n",
    "    x[:, 0, 1] = torch.randn(N, device=device) * 1.0                   # omega\n",
    "\n",
    "    for t in range(T):\n",
    "        theta = x[:, t, 0]\n",
    "        omega = x[:, t, 1]\n",
    "\n",
    "        # measurement noise mixture (same sigma for both channels)\n",
    "        is_out = (torch.rand(N, device=device) < p_out).float().view(N, 1)\n",
    "        sigma = r_nom + is_out * (r_out - r_nom)  # [N,1]\n",
    "        v = torch.randn(N, m, device=device) * sigma  # [N,2]\n",
    "        out_mask[:, t, :] = is_out\n",
    "\n",
    "        y[:, t, 0] = torch.sin(theta) + v[:, 0]\n",
    "        y[:, t, 1] = torch.cos(theta) + v[:, 1]\n",
    "\n",
    "        if t < T - 1:\n",
    "            theta_next = theta + dt * omega\n",
    "            omega_next = omega + dt * (-g_over_L * torch.sin(theta) - damping * omega)\n",
    "\n",
    "            w = torch.randn(N, n, device=device) @ LQ.T\n",
    "            x[:, t + 1, 0] = wrap_angle(theta_next + w[:, 0])\n",
    "            x[:, t + 1, 1] = omega_next + w[:, 1]\n",
    "\n",
    "    return x, y, out_mask\n",
    "\n",
    "\n",
    "class SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x: torch.Tensor, y: torch.Tensor, out_mask: torch.Tensor):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.out_mask = out_mask\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.x[idx], self.y[idx], self.out_mask[idx]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Unscented Transform\n",
    "# -------------------------\n",
    "def sigma_points(x, P, alpha=0.2, beta=2.0, kappa=0.0):\n",
    "    \"\"\"\n",
    "    x: [B,n], P:[B,n,n]\n",
    "    returns Xi:[B,2n+1,n], Wm,Wc:[2n+1]\n",
    "    \"\"\"\n",
    "    B, n = x.shape\n",
    "    lam = alpha**2 * (n + kappa) - n\n",
    "    c = n + lam\n",
    "    gamma = math.sqrt(max(c, 1e-12))\n",
    "\n",
    "    Wm = x.new_zeros(2 * n + 1)\n",
    "    Wc = x.new_zeros(2 * n + 1)\n",
    "    Wm[0] = lam / c\n",
    "    Wc[0] = lam / c + (1 - alpha**2 + beta)\n",
    "    Wm[1:] = 1.0 / (2 * c)\n",
    "    Wc[1:] = 1.0 / (2 * c)\n",
    "\n",
    "    P = ensure_spd_batch(P, eps=1e-6)\n",
    "    S = robust_cholesky(P, eps=1e-6)  # [B,n,n]\n",
    "\n",
    "    S_scaled = gamma * S\n",
    "    U = S_scaled.transpose(1, 2)  # [B,n,n]\n",
    "    x0 = x.unsqueeze(1)           # [B,1,n]\n",
    "    Xi = torch.cat([x0, x0 + U, x0 - U], dim=1)  # [B,2n+1,n]\n",
    "    return Xi, Wm, Wc\n",
    "\n",
    "\n",
    "def unscented_mean_cov(X, Wm, Wc, noise=None, eps=1e-6):\n",
    "    \"\"\"\n",
    "    X: [B,L,d]\n",
    "    return mean:[B,d], cov:[B,d,d]\n",
    "    noise can be [d,d] or [B,d,d]\n",
    "    \"\"\"\n",
    "    B, L, d = X.shape\n",
    "    mean = torch.sum(Wm.view(1, L, 1) * X, dim=1)\n",
    "\n",
    "    Xm = X - mean.unsqueeze(1)\n",
    "    cov = torch.zeros(B, d, d, device=X.device, dtype=X.dtype)\n",
    "    for i in range(L):\n",
    "        wi = Wc[i]\n",
    "        vi = Xm[:, i, :].unsqueeze(-1)\n",
    "        cov = cov + wi * (vi @ vi.transpose(-1, -2))\n",
    "\n",
    "    if noise is not None:\n",
    "        if noise.dim() == 2:\n",
    "            cov = cov + noise.unsqueeze(0)\n",
    "        else:\n",
    "            cov = cov + noise\n",
    "\n",
    "    cov = symmetrize(cov)\n",
    "    cov = ensure_spd_batch(cov, eps=eps)\n",
    "    return mean, cov\n",
    "\n",
    "\n",
    "def cross_cov(X, Y, x_mean, y_mean, Wc):\n",
    "    \"\"\"\n",
    "    X:[B,L,n], Y:[B,L,m] -> Pxy:[B,n,m]\n",
    "    \"\"\"\n",
    "    B, L, n = X.shape\n",
    "    m = Y.shape[-1]\n",
    "    Xc = X - x_mean.unsqueeze(1)\n",
    "    Yc = Y - y_mean.unsqueeze(1)\n",
    "    Pxy = torch.zeros(B, n, m, device=X.device, dtype=X.dtype)\n",
    "    for i in range(L):\n",
    "        wi = Wc[i]\n",
    "        xi = Xc[:, i, :].unsqueeze(-1)   # [B,n,1]\n",
    "        yi = Yc[:, i, :].unsqueeze(-1)   # [B,m,1]\n",
    "        Pxy = Pxy + wi * (xi @ yi.transpose(-1, -2))\n",
    "    return Pxy\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Baseline UKF (fixed Q, fixed R)\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def batch_ukf_filter(\n",
    "    y: torch.Tensor,          # [B,T,m]\n",
    "    f_fn,\n",
    "    h_fn,\n",
    "    Q: torch.Tensor,          # [2,2]\n",
    "    R: torch.Tensor,          # [m,m]\n",
    "    x0: torch.Tensor,         # [B,2]\n",
    "    P0: torch.Tensor,         # [B,2,2]\n",
    "    ut_params=(0.2, 2.0, 0.0),\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    alpha, beta, kappa = ut_params\n",
    "    B, T, _ = y.shape\n",
    "\n",
    "    x = x0\n",
    "    P = ensure_spd_batch(P0, eps=1e-6)\n",
    "\n",
    "    x_list, P_list = [], []\n",
    "    for t in range(T):\n",
    "        P = ensure_spd_batch(P, eps=1e-6)\n",
    "        Xi, Wm, Wc = sigma_points(x, P, alpha, beta, kappa)\n",
    "\n",
    "        X_pred = f_fn(Xi)\n",
    "        x_pred, P_pred = unscented_mean_cov(X_pred, Wm, Wc, noise=Q)\n",
    "\n",
    "        Y_pred = h_fn(X_pred)\n",
    "        y_pred, S = unscented_mean_cov(Y_pred, Wm, Wc, noise=R)\n",
    "\n",
    "        Pxy = cross_cov(X_pred, Y_pred, x_pred, y_pred, Wc)\n",
    "\n",
    "        S = ensure_spd_batch(S, eps=1e-9)\n",
    "        Ls = robust_cholesky(S, eps=1e-9)\n",
    "        KT = torch.cholesky_solve(Pxy.transpose(1, 2), Ls)  # [B,m,n]\n",
    "        K = KT.transpose(1, 2)  # [B,n,m]\n",
    "\n",
    "        e = y[:, t, :] - y_pred\n",
    "        x = x_pred + torch.bmm(K, e.unsqueeze(-1)).squeeze(-1)\n",
    "        x = torch.stack([wrap_angle(x[:, 0]), x[:, 1]], dim=-1)\n",
    "\n",
    "        P = P_pred - torch.bmm(torch.bmm(K, S), K.transpose(-1, -2))\n",
    "        P = ensure_spd_batch(symmetrize(P), eps=1e-6)\n",
    "\n",
    "        x_list.append(x)\n",
    "        P_list.append(P)\n",
    "\n",
    "    return torch.stack(x_list, dim=1), torch.stack(P_list, dim=1)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Baseline EKF (fixed Q, fixed R) for y=[sin,cos]\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def batch_ekf_filter_sincos(\n",
    "    y: torch.Tensor,          # [B,T,2]\n",
    "    dt: float,\n",
    "    g_over_L: float,\n",
    "    damping: float,\n",
    "    Q: torch.Tensor,          # [2,2]\n",
    "    R: torch.Tensor,          # [2,2]\n",
    "    x0: torch.Tensor,         # [B,2]\n",
    "    P0: torch.Tensor,         # [B,2,2]\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    EKF for pendulum with measurement h(x)=[sin(theta), cos(theta)].\n",
    "    Joseph form covariance update for stability.\n",
    "    \"\"\"\n",
    "    device = y.device\n",
    "    B, T, m = y.shape\n",
    "    n = 2\n",
    "    I = torch.eye(n, device=device, dtype=y.dtype).unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "    x = x0\n",
    "    P = ensure_spd_batch(P0, eps=1e-6)\n",
    "\n",
    "    x_list, P_list = [], []\n",
    "\n",
    "    Rb = R.unsqueeze(0).expand(B, -1, -1)  # [B,2,2]\n",
    "    Qb = Q.unsqueeze(0).expand(B, -1, -1)  # [B,2,2]\n",
    "\n",
    "    for t in range(T):\n",
    "        theta = x[:, 0]\n",
    "        omega = x[:, 1]\n",
    "\n",
    "        # ---- Predict ----\n",
    "        theta_pred = wrap_angle(theta + dt * omega)\n",
    "        omega_pred = omega + dt * (-g_over_L * torch.sin(theta) - damping * omega)\n",
    "        x_pred = torch.stack([theta_pred, omega_pred], dim=-1)\n",
    "\n",
    "        # Jacobian F=df/dx at x\n",
    "        F = torch.zeros(B, 2, 2, device=device, dtype=y.dtype)\n",
    "        F[:, 0, 0] = 1.0\n",
    "        F[:, 0, 1] = dt\n",
    "        F[:, 1, 0] = -dt * g_over_L * torch.cos(theta)\n",
    "        F[:, 1, 1] = 1.0 - dt * damping\n",
    "\n",
    "        P_pred = F @ P @ F.transpose(-1, -2) + Qb\n",
    "        P_pred = ensure_spd_batch(P_pred, eps=1e-6)\n",
    "\n",
    "        # ---- Update ----\n",
    "        y_pred = torch.stack([torch.sin(theta_pred), torch.cos(theta_pred)], dim=-1)  # [B,2]\n",
    "        e = y[:, t, :] - y_pred  # [B,2]\n",
    "\n",
    "        # H = dh/dx at x_pred\n",
    "        # h1=sin(theta) -> [cos(theta), 0]\n",
    "        # h2=cos(theta) -> [-sin(theta),0]\n",
    "        H = torch.zeros(B, 2, 2, device=device, dtype=y.dtype)\n",
    "        H[:, 0, 0] = torch.cos(theta_pred)\n",
    "        H[:, 0, 1] = 0.0\n",
    "        H[:, 1, 0] = -torch.sin(theta_pred)\n",
    "        H[:, 1, 1] = 0.0\n",
    "\n",
    "        S = H @ P_pred @ H.transpose(-1, -2) + Rb\n",
    "        S = ensure_spd_batch(S, eps=1e-9)\n",
    "        Ls = robust_cholesky(S, eps=1e-9)\n",
    "\n",
    "        # K^T = S^{-1} (H P_pred)\n",
    "        HP = H @ P_pred  # [B,2,2]\n",
    "        KT = torch.cholesky_solve(HP, Ls)  # [B,2,2]\n",
    "        K = KT.transpose(1, 2)             # [B,2,2]\n",
    "\n",
    "        x = x_pred + torch.bmm(K, e.unsqueeze(-1)).squeeze(-1)\n",
    "        x = torch.stack([wrap_angle(x[:, 0]), x[:, 1]], dim=-1)\n",
    "\n",
    "        # Joseph: P=(I-KH)P_pred(I-KH)^T + K R K^T\n",
    "        KH = torch.bmm(K, H)\n",
    "        A = I - KH\n",
    "        P = A @ P_pred @ A.transpose(-1, -2) + torch.bmm(torch.bmm(K, Rb), K.transpose(-1, -2))\n",
    "        P = ensure_spd_batch(symmetrize(P), eps=1e-6)\n",
    "\n",
    "        x_list.append(x)\n",
    "        P_list.append(P)\n",
    "\n",
    "    return torch.stack(x_list, dim=1), torch.stack(P_list, dim=1)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Cholesky parameterization helper for R_t\n",
    "# -------------------------\n",
    "def build_L_from_params(p: torch.Tensor, m: int, eps_diag: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    p: [B, pdim] where pdim = m + m(m-1)/2\n",
    "    Returns L: [B,m,m] lower-triangular with positive diag via softplus + eps.\n",
    "    Order: diag then strict-lower (row-wise).\n",
    "    \"\"\"\n",
    "    B = p.shape[0]\n",
    "    L = p.new_zeros(B, m, m)\n",
    "\n",
    "    # diag part\n",
    "    diag_raw = p[:, :m]\n",
    "    diag = torch.nn.functional.softplus(diag_raw) + eps_diag\n",
    "    for i in range(m):\n",
    "        L[:, i, i] = diag[:, i]\n",
    "\n",
    "    # off-diag part\n",
    "    off = p[:, m:]\n",
    "    k = 0\n",
    "    for i in range(1, m):\n",
    "        for j in range(i):\n",
    "            L[:, i, j] = off[:, k]\n",
    "            k += 1\n",
    "\n",
    "    return L\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# UKNet: learn time-varying R_t via Cholesky; Q is constant\n",
    "# (GRU with pre/post FC layers)\n",
    "# -------------------------\n",
    "class UKNet_RCholesky(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n: int,\n",
    "        m: int,\n",
    "        hidden_size: int = 64,\n",
    "        emb_size: int = 64,\n",
    "        ut_params=(0.2, 2.0, 0.0),\n",
    "        clip: float = 2.5,\n",
    "        delta_max: float = 0.15,\n",
    "        eps_diag: float = 1e-4,\n",
    "        r_min: float = 1e-6,          # <<< FIX: allow r_min as in your call\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.alpha, self.beta, self.kappa = ut_params\n",
    "\n",
    "        self.clip = clip\n",
    "        self.delta_max = delta_max\n",
    "        self.eps_diag = eps_diag\n",
    "        self.r_min = r_min\n",
    "\n",
    "        self.pdim = m + (m * (m - 1)) // 2\n",
    "\n",
    "        # features: e(m), de(m), dy(m), |e|(m), diagP(n), diagS(m) -> 5m + n\n",
    "        in_dim = 5 * m + n\n",
    "\n",
    "        # Pre-MLP (GRU input)\n",
    "        self.pre = nn.Sequential(\n",
    "            nn.Linear(in_dim, emb_size),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(emb_size, emb_size),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        self.gru = nn.GRUCell(emb_size, hidden_size)\n",
    "\n",
    "        # Post-MLP (GRU output)\n",
    "        self.post = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        self.fc_dp = nn.Linear(hidden_size, self.pdim)\n",
    "\n",
    "        nn.init.zeros_(self.fc_dp.weight)\n",
    "        nn.init.zeros_(self.fc_dp.bias)\n",
    "\n",
    "    def forward(self, y, f_fn, h_fn, x0, P0, Q_const: torch.Tensor):\n",
    "        \"\"\"\n",
    "        y: [B,T,m], x0:[B,n], P0:[B,n,n], Q_const:[n,n]\n",
    "        returns xhat:[B,T,n], P_hist:[B,T,n,n], p_hist:[B,T,pdim], R_hist:[B,T,m,m]\n",
    "        \"\"\"\n",
    "        device = y.device\n",
    "        B, T, m = y.shape\n",
    "        n = self.n\n",
    "\n",
    "        x = x0\n",
    "        P = ensure_spd_batch(P0, eps=1e-6)\n",
    "\n",
    "        h = torch.zeros(B, self.gru.hidden_size, device=device, dtype=y.dtype)\n",
    "\n",
    "        e_prev = torch.zeros(B, m, device=device, dtype=y.dtype)\n",
    "        y_prev = torch.zeros(B, m, device=device, dtype=y.dtype)\n",
    "\n",
    "        # p is the Cholesky-parameter vector for R\n",
    "        p = torch.zeros(B, self.pdim, device=device, dtype=y.dtype)\n",
    "\n",
    "        x_list, P_list, p_list, R_list = [], [], [], []\n",
    "\n",
    "        # tiny covs for feature preview stability\n",
    "        Q_feat = torch.eye(n, device=device, dtype=y.dtype) * 1e-6\n",
    "        R_feat = torch.eye(m, device=device, dtype=y.dtype) * 1e-6\n",
    "\n",
    "        Qb = Q_const.unsqueeze(0).expand(B, -1, -1)  # [B,n,n]\n",
    "\n",
    "        for t in range(T):\n",
    "            P = ensure_spd_batch(P, eps=1e-6)\n",
    "\n",
    "            Xi, Wm, Wc = sigma_points(x, P, self.alpha, self.beta, self.kappa)\n",
    "            X_pred = f_fn(Xi)\n",
    "\n",
    "            # preview for features\n",
    "            x_pred0, P_pred0 = unscented_mean_cov(X_pred, Wm, Wc, noise=Q_feat)\n",
    "            Y_pred0 = h_fn(X_pred)\n",
    "            y_pred0, S0 = unscented_mean_cov(Y_pred0, Wm, Wc, noise=R_feat)\n",
    "\n",
    "            e0 = y[:, t, :] - y_pred0\n",
    "            de = e0 - e_prev\n",
    "            dy = y[:, t, :] - y_prev\n",
    "            ae = torch.abs(e0)\n",
    "\n",
    "            diagP = torch.diagonal(P_pred0, dim1=-2, dim2=-1)\n",
    "            diagS = torch.diagonal(S0, dim1=-2, dim2=-1)\n",
    "\n",
    "            z = torch.cat([\n",
    "                l2_normalize(e0),\n",
    "                l2_normalize(de),\n",
    "                l2_normalize(dy),\n",
    "                l2_normalize(ae),\n",
    "                l2_normalize(diagP.detach()),\n",
    "                l2_normalize(diagS.detach()),\n",
    "            ], dim=-1)\n",
    "\n",
    "            z_emb = self.pre(z)\n",
    "            h = self.gru(z_emb, h)\n",
    "            h2 = self.post(h)\n",
    "\n",
    "            dp = torch.tanh(self.fc_dp(h2)) * self.delta_max\n",
    "            p = torch.clamp(p + dp, -self.clip, self.clip)\n",
    "\n",
    "            L = build_L_from_params(p, m=self.m, eps_diag=self.eps_diag)\n",
    "            R_t = torch.bmm(L, L.transpose(-1, -2))\n",
    "            R_t = ensure_spd_batch(R_t, eps=self.r_min)  # <<< enforce min eig\n",
    "\n",
    "            # UKF predict/update with Q_const and R_t\n",
    "            x_pred, P_pred = unscented_mean_cov(X_pred, Wm, Wc, noise=Qb)\n",
    "\n",
    "            Y_pred = h_fn(X_pred)\n",
    "            y_pred, S = unscented_mean_cov(Y_pred, Wm, Wc, noise=R_t)\n",
    "\n",
    "            Pxy = cross_cov(X_pred, Y_pred, x_pred, y_pred, Wc)\n",
    "\n",
    "            S = ensure_spd_batch(S, eps=1e-9)\n",
    "            Ls = robust_cholesky(S, eps=1e-9)\n",
    "            KT = torch.cholesky_solve(Pxy.transpose(1, 2), Ls)  # [B,m,n]\n",
    "            K = KT.transpose(1, 2)  # [B,n,m]\n",
    "\n",
    "            innov = y[:, t, :] - y_pred\n",
    "            x = x_pred + torch.bmm(K, innov.unsqueeze(-1)).squeeze(-1)\n",
    "            x = torch.stack([wrap_angle(x[:, 0]), x[:, 1]], dim=-1)\n",
    "\n",
    "            P = P_pred - torch.bmm(torch.bmm(K, S), K.transpose(-1, -2))\n",
    "            P = ensure_spd_batch(symmetrize(P), eps=1e-6)\n",
    "\n",
    "            x_list.append(x)\n",
    "            P_list.append(P)\n",
    "            p_list.append(p)\n",
    "            R_list.append(R_t)\n",
    "\n",
    "            e_prev = innov.detach()\n",
    "            y_prev = y[:, t, :].detach()\n",
    "\n",
    "        return (\n",
    "            torch.stack(x_list, dim=1),\n",
    "            torch.stack(P_list, dim=1),\n",
    "            torch.stack(p_list, dim=1),\n",
    "            torch.stack(R_list, dim=1),\n",
    "        )\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Loss: state NLL from P (theta wrapped)\n",
    "# -------------------------\n",
    "def state_nll_from_P(e: torch.Tensor, P: torch.Tensor, eps: float = 1e-9) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    e:[B,T,2], P:[B,T,2,2]\n",
    "    NLL = 0.5*(e^T P^{-1} e + logdet(P))\n",
    "    \"\"\"\n",
    "    B, T, n = e.shape\n",
    "    e2 = e.reshape(B * T, n, 1)\n",
    "    P2 = P.reshape(B * T, n, n)\n",
    "\n",
    "    P2 = ensure_spd_batch(P2, eps=1e-6)\n",
    "    L = robust_cholesky(P2, eps=1e-6)\n",
    "\n",
    "    sol = torch.cholesky_solve(e2, L)  # [B*T,n,1]\n",
    "    maha = (e2.transpose(1, 2) @ sol).reshape(B, T)  # [B,T]\n",
    "\n",
    "    diag = torch.diagonal(L, dim1=-2, dim2=-1)\n",
    "    logdet = 2.0 * torch.sum(torch.log(diag + eps), dim=-1)  # [B*T]\n",
    "    logdet = logdet.reshape(B, T)\n",
    "\n",
    "    return 0.5 * (maha + logdet).mean()\n",
    "\n",
    "\n",
    "def rmse_over_time(xhat: torch.Tensor, x_true: torch.Tensor) -> torch.Tensor:\n",
    "    e = state_error(x_true, xhat)  # [N,T,2]\n",
    "    mse_t = (e ** 2).mean(dim=(0, 2))\n",
    "    return torch.sqrt(mse_t + 1e-12)\n",
    "\n",
    "\n",
    "def mse_state(xhat: torch.Tensor, x_true: torch.Tensor) -> torch.Tensor:\n",
    "    e = state_error(x_true, xhat)\n",
    "    return (e ** 2).mean()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Plots + coverage\n",
    "# -------------------------\n",
    "def plot_loss_history(train_hist: List[float], test_hist: List[float], title: str):\n",
    "    epochs = np.arange(1, len(train_hist) + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_hist, label=\"Train total\")\n",
    "    plt.plot(epochs, test_hist, label=\"Test MSE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss / MSE\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_error_with_ci_99(x_true_1, x_hat_1, P_1, title_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Plot per-state error with 99% CI band from covariance.\n",
    "    x_true_1: [T,2], x_hat_1: [T,2], P_1: [T,2,2]\n",
    "    \"\"\"\n",
    "    z99 = 2.5758293035489004\n",
    "    T = x_true_1.shape[0]\n",
    "    t = np.arange(T)\n",
    "\n",
    "    # error (wrap theta)\n",
    "    err_theta = ((x_true_1[:, 0] - x_hat_1[:, 0] + np.pi) % (2*np.pi)) - np.pi\n",
    "    err_omega = (x_true_1[:, 1] - x_hat_1[:, 1])\n",
    "    err = np.stack([err_theta, err_omega], axis=1)\n",
    "\n",
    "    var = np.stack([np.diag(P_1[k]) for k in range(T)], axis=0)  # [T,2]\n",
    "    std = np.sqrt(np.maximum(var, 1e-12))\n",
    "    band = z99 * std\n",
    "\n",
    "    names = [\"theta\", \"omega\"]\n",
    "    for i in range(2):\n",
    "        plt.figure()\n",
    "        plt.plot(t, err[:, i], label=f\"error ({names[i]})\")\n",
    "        plt.fill_between(t, -band[:, i], band[:, i], alpha=0.2, label=\"99% CI from P\")\n",
    "        plt.axhline(0.0, linewidth=1)\n",
    "        plt.xlabel(\"Time step\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.title(f\"{title_prefix} Error + 99% CI ({names[i]})\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def coverage_99(x_true: torch.Tensor, x_hat: torch.Tensor, P: torch.Tensor) -> np.ndarray:\n",
    "    z99 = 2.5758293035489004\n",
    "    e = state_error(x_true, x_hat)  # [N,T,2]\n",
    "    var = torch.diagonal(P, dim1=-2, dim2=-1)  # [N,T,2]\n",
    "    band = z99 * torch.sqrt(torch.clamp(var, min=1e-12))\n",
    "    inside = (torch.abs(e) <= band).float().mean(dim=(0, 1))  # [2]\n",
    "    return inside.cpu().numpy()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Train + compare (dt=0.2, y=[sin,cos])\n",
    "# -------------------------\n",
    "def train_pendulum_demo_dt02_y_sincos():\n",
    "    set_seed(0)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.float32\n",
    "\n",
    "    # --- Nonlinear problem params ---\n",
    "    dt = 0.2\n",
    "    g_over_L = 1.5\n",
    "    damping = 0.03\n",
    "\n",
    "    # UT params\n",
    "    ut_params = (0.25, 2.0, 0.0)\n",
    "\n",
    "    # True process noise\n",
    "    Q_true = torch.diag(torch.tensor([5e-4, 2e-3], device=device, dtype=dtype))\n",
    "\n",
    "    # Measurement outlier mixture\n",
    "    r_nom = 0.08\n",
    "    r_out = 1.00\n",
    "    p_out = 0.06\n",
    "\n",
    "    # Baseline assumed covariances (fixed)\n",
    "    gamma_Q = 8.0\n",
    "    Q_base = gamma_Q * Q_true\n",
    "    R_base = torch.eye(2, device=device, dtype=dtype) * (r_nom ** 2)\n",
    "\n",
    "    print(\"=== Adopted Nonlinear Problem (dt=0.2): Pendulum + Outlier Measurements + y=[sin,cos] ===\")\n",
    "    print(f\"device={device}\")\n",
    "    print(f\"dt={dt}, g/L={g_over_L}, damping={damping}\")\n",
    "    print(\"Q_true diag =\", torch.diag(Q_true).detach().cpu().numpy())\n",
    "    print(\"Q_base diag =\", torch.diag(Q_base).detach().cpu().numpy(), f\"(gamma_Q={gamma_Q})\")\n",
    "    print(\"R_base diag =\", torch.diag(R_base).detach().cpu().numpy(), \"(~ r_nom^2 I)\")\n",
    "    print(f\"Measurement noise: nominal std={r_nom}, outlier std={r_out}, p_out={p_out}\")\n",
    "    print()\n",
    "\n",
    "    # f/h for sigma points\n",
    "    def f_fn(X):\n",
    "        # X:[B,L,2]\n",
    "        theta = X[..., 0]\n",
    "        omega = X[..., 1]\n",
    "        theta_next = wrap_angle(theta + dt * omega)\n",
    "        omega_next = omega + dt * (-g_over_L * torch.sin(theta) - damping * omega)\n",
    "        return torch.stack([theta_next, omega_next], dim=-1)\n",
    "\n",
    "    def h_fn(X):\n",
    "        theta = X[..., 0]\n",
    "        return torch.stack([torch.sin(theta), torch.cos(theta)], dim=-1)  # [B,L,2]\n",
    "\n",
    "    # --- Data ---\n",
    "    T = 80\n",
    "    N_train, N_test = 7000, 1400\n",
    "\n",
    "    x_train, y_train, out_train = sample_pendulum_sequences_sincos(\n",
    "        N_train, T, dt, g_over_L, damping, Q_true, r_nom, r_out, p_out, device=device\n",
    "    )\n",
    "    x_test, y_test, out_test = sample_pendulum_sequences_sincos(\n",
    "        N_test, T, dt, g_over_L, damping, Q_true, r_nom, r_out, p_out, device=device\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(SeqDataset(x_train, y_train, out_train), batch_size=128, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(SeqDataset(x_test, y_test, out_test), batch_size=256, shuffle=False, num_workers=0)\n",
    "\n",
    "    # --- Init x0, P0 ---\n",
    "    x0 = torch.zeros(1, 2, device=device, dtype=dtype)\n",
    "    P0 = torch.diag(torch.tensor([1.0, 1.0], device=device, dtype=dtype)).unsqueeze(0)\n",
    "\n",
    "    # --- Model (m=2) ---\n",
    "    model = UKNet_RCholesky(\n",
    "        n=2, m=2,\n",
    "        hidden_size=64,\n",
    "        emb_size=64,\n",
    "        ut_params=ut_params,\n",
    "        clip=2.5,\n",
    "        delta_max=0.10,    # 튐 억제하려면 0.05~0.10 추천\n",
    "        eps_diag=1e-4,\n",
    "        r_min=1e-6,        # <<< 이제 에러 없이 받음\n",
    "        dropout=0.05,\n",
    "    ).to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-6)\n",
    "\n",
    "    # losses\n",
    "    beta_cov = 0.10         # MSE vs NLL mixing\n",
    "    lam_dp = 5e-4           # dp penalty (R_t 너무 튀는 것 억제)\n",
    "    num_epochs = 40         # <<< epoch 늘리려면 여기\n",
    "\n",
    "    train_hist_total: List[float] = []\n",
    "    test_hist_mse: List[float] = []\n",
    "\n",
    "    # --------- epoch loop starts ----------\n",
    "    for ep in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        tot_loss = 0.0\n",
    "        tot_mse = 0.0\n",
    "\n",
    "        for xb, yb, _ in train_loader:\n",
    "            B = xb.shape[0]\n",
    "            x0b = x0.expand(B, -1).contiguous()\n",
    "            P0b = P0.expand(B, -1, -1).contiguous()\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            xhat, P_hist, p_hist, _ = model(yb, f_fn, h_fn, x0b, P0b, Q_const=Q_base)\n",
    "\n",
    "            loss_state = mse_state(xhat, xb)\n",
    "            e = state_error(xb, xhat)\n",
    "            loss_cov = state_nll_from_P(e, P_hist)\n",
    "\n",
    "            # dp penalty: p_hist[t]-p_hist[t-1]\n",
    "            dp = p_hist[:, 1:, :] - p_hist[:, :-1, :]\n",
    "            loss_dp = (dp ** 2).mean()\n",
    "\n",
    "            loss = (1.0 - beta_cov) * loss_state + beta_cov * loss_cov + lam_dp * loss_dp\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            tot_loss += loss.item() * B\n",
    "            tot_mse += loss_state.item() * B\n",
    "\n",
    "        train_total = tot_loss / len(train_loader.dataset)\n",
    "        train_mse = tot_mse / len(train_loader.dataset)\n",
    "\n",
    "        # test MSE\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot = 0.0\n",
    "            for xb, yb, _ in test_loader:\n",
    "                B = xb.shape[0]\n",
    "                x0b = x0.expand(B, -1).contiguous()\n",
    "                P0b = P0.expand(B, -1, -1).contiguous()\n",
    "                xhat, _, _, _ = model(yb, f_fn, h_fn, x0b, P0b, Q_const=Q_base)\n",
    "                tot += mse_state(xhat, xb).item() * B\n",
    "            test_mse = tot / len(test_loader.dataset)\n",
    "\n",
    "        train_hist_total.append(train_total)\n",
    "        test_hist_mse.append(test_mse)\n",
    "\n",
    "        print(f\"Epoch {ep:02d} | Train total: {train_total:.6f} (MSE={train_mse:.6f}) | Test MSE: {test_mse:.6f}\")\n",
    "    # --------- epoch loop ends ----------\n",
    "\n",
    "    plot_loss_history(train_hist_total, test_hist_mse, title=\"Pendulum(dt=0.2, sincos): UKN(R-Cholesky) - Loss History\")\n",
    "\n",
    "    # --- Final comparison ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        B = y_test.shape[0]\n",
    "        x0b = x0.expand(B, -1).contiguous()\n",
    "        P0b = P0.expand(B, -1, -1).contiguous()\n",
    "\n",
    "        # UKN\n",
    "        xhat_ukn, P_ukn, p_hist, R_hist = model(y_test, f_fn, h_fn, x0b, P0b, Q_const=Q_base)\n",
    "        ukn_mse = mse_state(xhat_ukn, x_test).item()\n",
    "\n",
    "        # UKF fixed\n",
    "        xhat_ukf, P_ukf = batch_ukf_filter(\n",
    "            y=y_test, f_fn=f_fn, h_fn=h_fn, Q=Q_base, R=R_base, x0=x0b, P0=P0b, ut_params=ut_params\n",
    "        )\n",
    "        ukf_mse = mse_state(xhat_ukf, x_test).item()\n",
    "\n",
    "        # EKF fixed\n",
    "        xhat_ekf, P_ekf = batch_ekf_filter_sincos(\n",
    "            y=y_test, dt=dt, g_over_L=g_over_L, damping=damping,\n",
    "            Q=Q_base, R=R_base, x0=x0b, P0=P0b\n",
    "        )\n",
    "        ekf_mse = mse_state(xhat_ekf, x_test).item()\n",
    "\n",
    "        print(\"\\n===== Final (Test Set) =====\")\n",
    "        print(f\"UKN (R-Chol) MSE: {ukn_mse:.6e}\")\n",
    "        print(f\"UKF (fixed)  MSE: {ukf_mse:.6e}\")\n",
    "        print(f\"EKF (fixed)  MSE: {ekf_mse:.6e}\")\n",
    "\n",
    "        # RMSE(t)\n",
    "        rmse_ukn = rmse_over_time(xhat_ukn, x_test).cpu().numpy()\n",
    "        rmse_ukf = rmse_over_time(xhat_ukf, x_test).cpu().numpy()\n",
    "        rmse_ekf = rmse_over_time(xhat_ekf, x_test).cpu().numpy()\n",
    "\n",
    "        t = np.arange(len(rmse_ukn))\n",
    "        plt.figure()\n",
    "        plt.plot(t, rmse_ukn, label=\"UKN RMSE(t)\")\n",
    "        plt.plot(t, rmse_ukf, label=\"UKF RMSE(t)\")\n",
    "        plt.plot(t, rmse_ekf, label=\"EKF RMSE(t)\")\n",
    "        plt.xlabel(\"Time step\")\n",
    "        plt.ylabel(\"RMSE\")\n",
    "        plt.title(\"RMSE over time (test avg)\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "        # 99% coverage\n",
    "        cov_ukn = coverage_99(x_test, xhat_ukn, P_ukn)\n",
    "        cov_ukf = coverage_99(x_test, xhat_ukf, P_ukf)\n",
    "        cov_ekf = coverage_99(x_test, xhat_ekf, P_ekf)\n",
    "        print(\"\\n99% CI Coverage (fraction inside band)\")\n",
    "        print(f\"UKN: theta={cov_ukn[0]:.3f}, omega={cov_ukn[1]:.3f}\")\n",
    "        print(f\"UKF: theta={cov_ukf[0]:.3f}, omega={cov_ukf[1]:.3f}\")\n",
    "        print(f\"EKF: theta={cov_ekf[0]:.3f}, omega={cov_ekf[1]:.3f}\")\n",
    "\n",
    "        # Sample trajectories + learned R diag\n",
    "        sample_idx = 0\n",
    "        x_true_1 = x_test[sample_idx].cpu().numpy()\n",
    "        x_ukn_1 = xhat_ukn[sample_idx].cpu().numpy()\n",
    "        x_ukf_1 = xhat_ukf[sample_idx].cpu().numpy()\n",
    "        x_ekf_1 = xhat_ekf[sample_idx].cpu().numpy()\n",
    "\n",
    "        names = [\"theta\", \"omega\"]\n",
    "        for i in range(2):\n",
    "            plt.figure()\n",
    "            plt.plot(t, x_true_1[:, i], label=f\"True {names[i]}\")\n",
    "            plt.plot(t, x_ukn_1[:, i], label=f\"UKN  {names[i]}\")\n",
    "            plt.plot(t, x_ukf_1[:, i], label=f\"UKF  {names[i]}\")\n",
    "            plt.plot(t, x_ekf_1[:, i], label=f\"EKF  {names[i]}\")\n",
    "            plt.xlabel(\"Time step\")\n",
    "            plt.ylabel(names[i])\n",
    "            plt.title(f\"Sample trajectory (idx={sample_idx}) - {names[i]}\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "            plt.show()\n",
    "\n",
    "        # outlier mask vs learned R diag\n",
    "        out_1 = out_test[sample_idx].cpu().numpy().squeeze(-1)  # [T]\n",
    "        Rdiag_1 = torch.diagonal(R_hist[sample_idx], dim1=-2, dim2=-1).cpu().numpy()  # [T,2]\n",
    "        plt.figure()\n",
    "        plt.plot(t, Rdiag_1[:, 0], label=\"R_t[0,0]\")\n",
    "        plt.plot(t, Rdiag_1[:, 1], label=\"R_t[1,1]\")\n",
    "        plt.plot(t, out_1 * max(Rdiag_1.max(), 1e-6), label=\"outlier mask (scaled)\")\n",
    "        plt.xlabel(\"Time step\")\n",
    "        plt.title(\"Sample: learned R diag vs outlier mask\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "        # LAST PLOT: error + 99% CI band from P (UKN)\n",
    "        P_ukn_1 = P_ukn[sample_idx].cpu().numpy()\n",
    "        plot_error_with_ci_99(x_true_1, x_ukn_1, P_ukn_1, title_prefix=\"UKN\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_pendulum_demo_dt02_y_sincos()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
