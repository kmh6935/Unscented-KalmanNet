{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e124418",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Nonlinear demo where UKN (UKF + learned scalar scales for Q/R) >> UKF (fixed Q/R)\n",
    "\n",
    "State: x_t = [theta, omega]^T\n",
    "Dynamics (discrete):\n",
    "    theta_{t+1} = theta_t + dt * omega_t\n",
    "    omega_{t+1} = omega_t + dt * ( -g/L * sin(theta_t) - c*omega_t ) + w_t\n",
    "Measurement:\n",
    "    y_t = sin(theta_t) + v_t\n",
    "Measurement noise v_t is a mixture:\n",
    "    with prob (1-p_out): N(0, r_nom^2)\n",
    "    with prob p_out     : N(0, r_out^2)   (outlier bursts)\n",
    "\n",
    "UKF baseline assumes fixed (often too small) R_base ~ r_nom^2.\n",
    "UKN learns scalar scales:\n",
    "    Q_t = exp(sq_t) * Q_base\n",
    "    R_t = exp(sr_t) * R_base\n",
    "\n",
    "Outputs:\n",
    "- Loss history (train total = (1-beta)*MSE + beta*NLL, test MSE)\n",
    "- Final comparison: UKN vs UKF (MSE/RMSE(t))\n",
    "- Sample trajectories\n",
    "- LAST PLOT: per-state error with 99% CI bands from P\n",
    "- (extra) outlier mask vs exp(sr_t) for a sample\n",
    "- (extra) 99% coverage computed from P\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Repro / device\n",
    "# -------------------------\n",
    "def set_seed(seed: int = 0):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Angle utils\n",
    "# -------------------------\n",
    "def wrap_angle(a: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Wrap to [-pi, pi].\"\"\"\n",
    "    return (a + math.pi) % (2 * math.pi) - math.pi\n",
    "\n",
    "\n",
    "def state_error(x_true: torch.Tensor, x_hat: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x_true, x_hat: [...,2] -> error [...,2]\n",
    "    theta error wrapped, omega error standard.\n",
    "    \"\"\"\n",
    "    e = x_true - x_hat\n",
    "    e_theta = wrap_angle(e[..., 0])\n",
    "    e_omega = e[..., 1]\n",
    "    return torch.stack([e_theta, e_omega], dim=-1)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# SPD / numerics (autograd-safe)\n",
    "# -------------------------\n",
    "def symmetrize(P: torch.Tensor) -> torch.Tensor:\n",
    "    return 0.5 * (P + P.transpose(-1, -2))\n",
    "\n",
    "\n",
    "def ensure_spd_batch(P: torch.Tensor, eps: float = 1e-6, max_shift: float = 1e6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Per-batch diagonal shift so min eigenvalue >= eps.\n",
    "    Also sanitizes NaN/Inf.\n",
    "    \"\"\"\n",
    "    P = symmetrize(P)\n",
    "\n",
    "    if not torch.isfinite(P).all():\n",
    "        P = torch.nan_to_num(P, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        P = symmetrize(P)\n",
    "\n",
    "    B, n, _ = P.shape\n",
    "    I = torch.eye(n, device=P.device, dtype=P.dtype).unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        eigmin = torch.linalg.eigvalsh(P).min(dim=-1).values  # [B]\n",
    "        shift = torch.clamp(eps - eigmin, min=0.0, max=max_shift)  # [B]\n",
    "\n",
    "    return P + shift.view(B, 1, 1) * I\n",
    "\n",
    "\n",
    "def robust_cholesky(P: torch.Tensor, eps: float = 1e-6, tries: int = 7) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Robust batch Cholesky: retries with increasing eps.\n",
    "    Avoid cholesky_ex & in-place patching -> safe for autograd.\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    for k in range(tries):\n",
    "        P2 = ensure_spd_batch(P, eps=eps * (10.0 ** k))\n",
    "        P2 = symmetrize(P2)\n",
    "        try:\n",
    "            return torch.linalg.cholesky(P2)\n",
    "        except RuntimeError as e:\n",
    "            last_err = e\n",
    "    raise last_err\n",
    "\n",
    "\n",
    "def l2_normalize(x: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    n = torch.norm(x, dim=-1, keepdim=True)\n",
    "    return x / (n + eps)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Nonlinear data generation (pendulum + outliers)\n",
    "# -------------------------\n",
    "def sample_pendulum_sequences(\n",
    "    num_seq: int,\n",
    "    T: int,\n",
    "    dt: float,\n",
    "    g_over_L: float,\n",
    "    damping: float,\n",
    "    Q_true: torch.Tensor,  # [2,2]\n",
    "    r_nom: float,\n",
    "    r_out: float,\n",
    "    p_out: float,\n",
    "    device: str = \"cpu\",\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x: [N,T,2]  (theta, omega)\n",
    "      y: [N,T,1]  (sin(theta) + noise)\n",
    "      out_mask: [N,T,1]  (1 if outlier used)\n",
    "    \"\"\"\n",
    "    N = num_seq\n",
    "    n = 2\n",
    "\n",
    "    LQ = torch.linalg.cholesky(Q_true)\n",
    "\n",
    "    x = torch.zeros(N, T, n, device=device)\n",
    "    y = torch.zeros(N, T, 1, device=device)\n",
    "    out_mask = torch.zeros(N, T, 1, device=device)\n",
    "\n",
    "    # init\n",
    "    x[:, 0, 0] = (torch.rand(N, device=device) * 2.0 - 1.0) * math.pi  # theta in [-pi,pi]\n",
    "    x[:, 0, 1] = torch.randn(N, device=device) * 0.5                   # omega\n",
    "\n",
    "    for t in range(T):\n",
    "        theta = x[:, t, 0]\n",
    "        omega = x[:, t, 1]\n",
    "\n",
    "        # measurement noise mixture\n",
    "        is_out = (torch.rand(N, device=device) < p_out).float().view(N, 1)\n",
    "        sigma = r_nom + is_out * (r_out - r_nom)  # [N,1]\n",
    "        v = torch.randn(N, 1, device=device) * sigma\n",
    "        out_mask[:, t, :] = is_out\n",
    "\n",
    "        y[:, t, 0] = torch.sin(theta) + v[:, 0]\n",
    "\n",
    "        if t < T - 1:\n",
    "            theta_next = theta + dt * omega\n",
    "            omega_next = omega + dt * (-g_over_L * torch.sin(theta) - damping * omega)\n",
    "\n",
    "            w = torch.randn(N, n, device=device) @ LQ.T\n",
    "            x[:, t + 1, 0] = wrap_angle(theta_next + w[:, 0])  # keep theta wrapped\n",
    "            x[:, t + 1, 1] = omega_next + w[:, 1]\n",
    "\n",
    "    return x, y, out_mask\n",
    "\n",
    "\n",
    "class SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x: torch.Tensor, y: torch.Tensor, out_mask: torch.Tensor):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.out_mask = out_mask\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.x[idx], self.y[idx], self.out_mask[idx]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Unscented Transform\n",
    "# -------------------------\n",
    "def sigma_points(x, P, alpha=0.2, beta=2.0, kappa=0.0):\n",
    "    \"\"\"\n",
    "    x: [B,n], P:[B,n,n]\n",
    "    returns Xi:[B,2n+1,n], Wm,Wc:[2n+1]\n",
    "    \"\"\"\n",
    "    B, n = x.shape\n",
    "    lam = alpha**2 * (n + kappa) - n\n",
    "    c = n + lam\n",
    "    gamma = math.sqrt(max(c, 1e-12))\n",
    "\n",
    "    Wm = x.new_zeros(2 * n + 1)\n",
    "    Wc = x.new_zeros(2 * n + 1)\n",
    "    Wm[0] = lam / c\n",
    "    Wc[0] = lam / c + (1 - alpha**2 + beta)\n",
    "    Wm[1:] = 1.0 / (2 * c)\n",
    "    Wc[1:] = 1.0 / (2 * c)\n",
    "\n",
    "    P = ensure_spd_batch(P, eps=1e-6)\n",
    "    S = robust_cholesky(P, eps=1e-6)  # [B,n,n]\n",
    "\n",
    "    S_scaled = gamma * S\n",
    "    U = S_scaled.transpose(1, 2)  # [B,n,n]\n",
    "    x0 = x.unsqueeze(1)           # [B,1,n]\n",
    "    Xi = torch.cat([x0, x0 + U, x0 - U], dim=1)  # [B,2n+1,n]\n",
    "    return Xi, Wm, Wc\n",
    "\n",
    "\n",
    "def unscented_mean_cov(X, Wm, Wc, noise=None, eps=1e-6):\n",
    "    \"\"\"\n",
    "    X: [B,L,d]\n",
    "    return mean:[B,d], cov:[B,d,d]\n",
    "    \"\"\"\n",
    "    B, L, d = X.shape\n",
    "    mean = torch.sum(Wm.view(1, L, 1) * X, dim=1)\n",
    "\n",
    "    Xm = X - mean.unsqueeze(1)\n",
    "    cov = torch.zeros(B, d, d, device=X.device, dtype=X.dtype)\n",
    "    for i in range(L):\n",
    "        wi = Wc[i]\n",
    "        vi = Xm[:, i, :].unsqueeze(-1)\n",
    "        cov = cov + wi * (vi @ vi.transpose(-1, -2))\n",
    "\n",
    "    if noise is not None:\n",
    "        cov = cov + (noise.unsqueeze(0) if noise.dim() == 2 else noise)\n",
    "\n",
    "    cov = symmetrize(cov)\n",
    "    cov = ensure_spd_batch(cov, eps=eps)\n",
    "    return mean, cov\n",
    "\n",
    "\n",
    "def cross_cov(X, Y, x_mean, y_mean, Wc):\n",
    "    \"\"\"\n",
    "    X:[B,L,n], Y:[B,L,m] -> Pxy:[B,n,m]\n",
    "    \"\"\"\n",
    "    B, L, n = X.shape\n",
    "    m = Y.shape[-1]\n",
    "    Xc = X - x_mean.unsqueeze(1)\n",
    "    Yc = Y - y_mean.unsqueeze(1)\n",
    "    Pxy = torch.zeros(B, n, m, device=X.device, dtype=X.dtype)\n",
    "    for i in range(L):\n",
    "        wi = Wc[i]\n",
    "        xi = Xc[:, i, :].unsqueeze(-1)   # [B,n,1]\n",
    "        yi = Yc[:, i, :].unsqueeze(-1)   # [B,m,1]\n",
    "        Pxy = Pxy + wi * (xi @ yi.transpose(-1, -2))\n",
    "    return Pxy\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Baseline UKF (fixed Q_base, R_base)\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def batch_ukf_filter(\n",
    "    y: torch.Tensor,          # [B,T,1]\n",
    "    f_fn,\n",
    "    h_fn,\n",
    "    Q: torch.Tensor,          # [2,2]\n",
    "    R: torch.Tensor,          # [1,1]\n",
    "    x0: torch.Tensor,         # [B,2]\n",
    "    P0: torch.Tensor,         # [B,2,2]\n",
    "    ut_params=(0.2, 2.0, 0.0),\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    alpha, beta, kappa = ut_params\n",
    "    B, T, _ = y.shape\n",
    "\n",
    "    x = x0\n",
    "    P = ensure_spd_batch(P0, eps=1e-6)\n",
    "\n",
    "    x_list, P_list = [], []\n",
    "    for t in range(T):\n",
    "        P = ensure_spd_batch(P, eps=1e-6)\n",
    "        Xi, Wm, Wc = sigma_points(x, P, alpha, beta, kappa)\n",
    "\n",
    "        X_pred = f_fn(Xi)\n",
    "        x_pred, P_pred = unscented_mean_cov(X_pred, Wm, Wc, noise=Q)\n",
    "\n",
    "        Y_pred = h_fn(X_pred)\n",
    "        y_pred, S = unscented_mean_cov(Y_pred, Wm, Wc, noise=R)\n",
    "\n",
    "        Pxy = cross_cov(X_pred, Y_pred, x_pred, y_pred, Wc)\n",
    "\n",
    "        S = ensure_spd_batch(S, eps=1e-6)\n",
    "        Ls = robust_cholesky(S, eps=1e-6)\n",
    "        KT = torch.cholesky_solve(Pxy.transpose(1, 2), Ls)  # [B,1,2]\n",
    "        K = KT.transpose(1, 2)  # [B,2,1]\n",
    "\n",
    "        e = y[:, t, :] - y_pred\n",
    "        x = x_pred + torch.bmm(K, e.unsqueeze(-1)).squeeze(-1)\n",
    "        x = torch.stack([wrap_angle(x[:, 0]), x[:, 1]], dim=-1)\n",
    "\n",
    "        P = P_pred - torch.bmm(torch.bmm(K, S), K.transpose(-1, -2))\n",
    "        P = symmetrize(P)\n",
    "        P = ensure_spd_batch(P, eps=1e-6)\n",
    "\n",
    "        x_list.append(x)\n",
    "        P_list.append(P)\n",
    "\n",
    "    return torch.stack(x_list, dim=1), torch.stack(P_list, dim=1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def batch_ekf_filter(\n",
    "    y: torch.Tensor,          # [B,T,1]\n",
    "    dt: float,\n",
    "    g_over_L: float,\n",
    "    damping: float,\n",
    "    Q: torch.Tensor,          # [2,2]\n",
    "    R: torch.Tensor,          # [1,1]\n",
    "    x0: torch.Tensor,         # [B,2]\n",
    "    P0: torch.Tensor,         # [B,2,2]\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    EKF for pendulum:\n",
    "      x=[theta, omega]\n",
    "      f(x) = [theta + dt*omega,\n",
    "              omega + dt*(-g/L*sin(theta) - damping*omega)]\n",
    "      h(x) = sin(theta)\n",
    "\n",
    "    Uses Joseph form covariance update for better SPD stability.\n",
    "    \"\"\"\n",
    "    device = y.device\n",
    "    B, T, _ = y.shape\n",
    "    n = 2\n",
    "    I = torch.eye(n, device=device, dtype=y.dtype).unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "    x = x0\n",
    "    P = ensure_spd_batch(P0, eps=1e-6)\n",
    "\n",
    "    x_list, P_list = [], []\n",
    "\n",
    "    for t in range(T):\n",
    "        theta = x[:, 0]\n",
    "        omega = x[:, 1]\n",
    "\n",
    "        # ---- Predict (nonlinear) ----\n",
    "        theta_pred = theta + dt * omega\n",
    "        omega_pred = omega + dt * (-g_over_L * torch.sin(theta) - damping * omega)\n",
    "        theta_pred = wrap_angle(theta_pred)\n",
    "\n",
    "        x_pred = torch.stack([theta_pred, omega_pred], dim=-1)\n",
    "\n",
    "        # Jacobian F = df/dx evaluated at x\n",
    "        # f1 = theta + dt*omega -> [1, dt]\n",
    "        # f2 = omega + dt*(-g/L*sin(theta) - damping*omega)\n",
    "        #    df2/dtheta = -dt*g/L*cos(theta), df2/domega = 1 - dt*damping\n",
    "        F = torch.zeros(B, 2, 2, device=device, dtype=y.dtype)\n",
    "        F[:, 0, 0] = 1.0\n",
    "        F[:, 0, 1] = dt\n",
    "        F[:, 1, 0] = -dt * g_over_L * torch.cos(theta)\n",
    "        F[:, 1, 1] = 1.0 - dt * damping\n",
    "\n",
    "        P_pred = F @ P @ F.transpose(-1, -2) + Q.unsqueeze(0)\n",
    "        P_pred = ensure_spd_batch(P_pred, eps=1e-6)\n",
    "\n",
    "        # ---- Update ----\n",
    "        # h(x)=sin(theta)\n",
    "        y_pred = torch.sin(theta_pred).unsqueeze(-1)  # [B,1]\n",
    "        e = y[:, t, :] - y_pred                       # innovation [B,1]\n",
    "\n",
    "        # Jacobian H = dh/dx at x_pred -> [cos(theta_pred), 0]\n",
    "        H = torch.zeros(B, 1, 2, device=device, dtype=y.dtype)\n",
    "        H[:, 0, 0] = torch.cos(theta_pred)\n",
    "        H[:, 0, 1] = 0.0\n",
    "\n",
    "        S = H @ P_pred @ H.transpose(-1, -2) + R.unsqueeze(0)  # [B,1,1]\n",
    "        S = ensure_spd_batch(S, eps=1e-9)\n",
    "\n",
    "        # K = P_pred H^T S^{-1}\n",
    "        # S is 1x1 so inverse is cheap/stable\n",
    "        S_inv = 1.0 / S\n",
    "        K = (P_pred @ H.transpose(-1, -2)) * S_inv  # [B,2,1]\n",
    "\n",
    "        x = x_pred + torch.bmm(K, e.unsqueeze(-1)).squeeze(-1)\n",
    "        x = torch.stack([wrap_angle(x[:, 0]), x[:, 1]], dim=-1)\n",
    "\n",
    "        # Joseph form: P = (I-KH)P_pred(I-KH)^T + K R K^T\n",
    "        KH = torch.bmm(K, H)  # [B,2,2]\n",
    "        A = I - KH\n",
    "        P = A @ P_pred @ A.transpose(-1, -2) + torch.bmm(torch.bmm(K, R.unsqueeze(0)), K.transpose(-1, -2))\n",
    "        P = symmetrize(P)\n",
    "        P = ensure_spd_batch(P, eps=1e-6)\n",
    "\n",
    "        x_list.append(x)\n",
    "        P_list.append(P)\n",
    "\n",
    "    return torch.stack(x_list, dim=1), torch.stack(P_list, dim=1)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# UKNet: learn scalar scales for Q/R\n",
    "# -------------------------\n",
    "class UKNet_ScalarQR(nn.Module):\n",
    "    \"\"\"\n",
    "    UKF structure + learn per-step scalar scales sq_t, sr_t:\n",
    "        Q_t = exp(sq_t) * Q_base\n",
    "        R_t = exp(sr_t) * R_base\n",
    "    \"\"\"\n",
    "    def __init__(self, n: int, m: int, hidden_size: int = 64, ut_params=(0.2, 2.0, 0.0),\n",
    "                 log_scale_clip: float = 8.0):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.alpha, self.beta, self.kappa = ut_params\n",
    "        self.log_scale_clip = log_scale_clip\n",
    "\n",
    "        # features: e(m), de(m), dy(m), |e|(m), diagP(n), diagS(m) -> 4m + n + m = 5m + n\n",
    "        in_dim = 5 * m + n\n",
    "        self.gru = nn.GRUCell(in_dim, hidden_size)\n",
    "\n",
    "        self.fc_sq = nn.Linear(hidden_size, 1)  # log-scale for Q\n",
    "        self.fc_sr = nn.Linear(hidden_size, 1)  # log-scale for R\n",
    "\n",
    "        nn.init.zeros_(self.fc_sq.weight); nn.init.zeros_(self.fc_sq.bias)\n",
    "        nn.init.zeros_(self.fc_sr.weight); nn.init.zeros_(self.fc_sr.bias)\n",
    "\n",
    "    def forward(self, y, f_fn, h_fn, x0, P0, Q_base: torch.Tensor, R_base: torch.Tensor):\n",
    "        \"\"\"\n",
    "        y:[B,T,1], x0:[B,2], P0:[B,2,2]\n",
    "        Q_base:[2,2], R_base:[1,1]\n",
    "        returns xhat:[B,T,2], P_hist:[B,T,2,2], sq_hist:[B,T,1], sr_hist:[B,T,1]\n",
    "        \"\"\"\n",
    "        device = y.device\n",
    "        B, T, m = y.shape\n",
    "        n = self.n\n",
    "\n",
    "        x = x0\n",
    "        P = ensure_spd_batch(P0, eps=1e-6)\n",
    "        h = torch.zeros(B, self.gru.hidden_size, device=device, dtype=y.dtype)\n",
    "\n",
    "        e_prev = torch.zeros(B, m, device=device, dtype=y.dtype)\n",
    "        y_prev = torch.zeros(B, m, device=device, dtype=y.dtype)\n",
    "\n",
    "        x_list, P_list = [], []\n",
    "        sq_list, sr_list = [], []\n",
    "\n",
    "        # tiny covs for feature extraction stability\n",
    "        Q_feat = torch.eye(n, device=device, dtype=y.dtype) * 1e-6\n",
    "        R_feat = torch.eye(m, device=device, dtype=y.dtype) * 1e-6\n",
    "\n",
    "        for t in range(T):\n",
    "            P = ensure_spd_batch(P, eps=1e-6)\n",
    "            Xi, Wm, Wc = sigma_points(x, P, self.alpha, self.beta, self.kappa)\n",
    "\n",
    "            X_pred = f_fn(Xi)\n",
    "\n",
    "            # provisional for features (cheap/stable)\n",
    "            x_pred0, P_pred0 = unscented_mean_cov(X_pred, Wm, Wc, noise=Q_feat)\n",
    "            Y_pred0 = h_fn(X_pred)\n",
    "            y_pred0, S0 = unscented_mean_cov(Y_pred0, Wm, Wc, noise=R_feat)\n",
    "\n",
    "            e0 = y[:, t, :] - y_pred0\n",
    "            de = e0 - e_prev\n",
    "            dy = y[:, t, :] - y_prev\n",
    "            ae = torch.abs(e0)\n",
    "\n",
    "            diagP = torch.diagonal(P_pred0, dim1=-2, dim2=-1)  # [B,n]\n",
    "            diagS = torch.diagonal(S0, dim1=-2, dim2=-1)       # [B,m]\n",
    "\n",
    "            z = torch.cat([\n",
    "                l2_normalize(e0),\n",
    "                l2_normalize(de),\n",
    "                l2_normalize(dy),\n",
    "                l2_normalize(ae),\n",
    "                l2_normalize(diagP.detach()),\n",
    "                l2_normalize(diagS.detach()),\n",
    "            ], dim=-1)\n",
    "\n",
    "            h = self.gru(z, h)\n",
    "\n",
    "            sq = torch.clamp(self.fc_sq(h), -self.log_scale_clip, self.log_scale_clip)  # [B,1]\n",
    "            sr = torch.clamp(self.fc_sr(h), -self.log_scale_clip, self.log_scale_clip)  # [B,1]\n",
    "            q_scale = torch.exp(sq)\n",
    "            r_scale = torch.exp(sr)\n",
    "\n",
    "            Q_t = q_scale.view(B, 1, 1) * Q_base.unsqueeze(0)  # [B,2,2]\n",
    "            R_t = r_scale.view(B, 1, 1) * R_base.unsqueeze(0)  # [B,1,1]\n",
    "            Q_t = ensure_spd_batch(Q_t, eps=1e-6)\n",
    "            R_t = ensure_spd_batch(R_t, eps=1e-6)\n",
    "\n",
    "            # UKF predict/update with Q_t,R_t\n",
    "            x_pred, P_pred = unscented_mean_cov(X_pred, Wm, Wc, noise=Q_t)\n",
    "\n",
    "            Y_pred = h_fn(X_pred)\n",
    "            y_pred, S = unscented_mean_cov(Y_pred, Wm, Wc, noise=R_t)\n",
    "\n",
    "            Pxy = cross_cov(X_pred, Y_pred, x_pred, y_pred, Wc)\n",
    "\n",
    "            S = ensure_spd_batch(S, eps=1e-6)\n",
    "            Ls = robust_cholesky(S, eps=1e-6)\n",
    "            KT = torch.cholesky_solve(Pxy.transpose(1, 2), Ls)  # [B,1,2]\n",
    "            K = KT.transpose(1, 2)  # [B,2,1]\n",
    "\n",
    "            e = y[:, t, :] - y_pred\n",
    "            x = x_pred + torch.bmm(K, e.unsqueeze(-1)).squeeze(-1)\n",
    "            x = torch.stack([wrap_angle(x[:, 0]), x[:, 1]], dim=-1)\n",
    "\n",
    "            P = P_pred - torch.bmm(torch.bmm(K, S), K.transpose(-1, -2))\n",
    "            P = symmetrize(P)\n",
    "            P = ensure_spd_batch(P, eps=1e-6)\n",
    "\n",
    "            x_list.append(x)\n",
    "            P_list.append(P)\n",
    "            sq_list.append(sq)\n",
    "            sr_list.append(sr)\n",
    "\n",
    "            e_prev = e.detach()\n",
    "            y_prev = y[:, t, :].detach()\n",
    "\n",
    "        return (\n",
    "            torch.stack(x_list, dim=1),\n",
    "            torch.stack(P_list, dim=1),\n",
    "            torch.stack(sq_list, dim=1),\n",
    "            torch.stack(sr_list, dim=1),\n",
    "        )\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Loss: state NLL from P (with wrapped theta error)\n",
    "# -------------------------\n",
    "def state_nll_from_P(e: torch.Tensor, P: torch.Tensor, eps: float = 1e-9) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    e:[B,T,2], P:[B,T,2,2]\n",
    "    NLL = 0.5*(e^T P^{-1} e + logdet(P))\n",
    "    \"\"\"\n",
    "    B, T, n = e.shape\n",
    "    e2 = e.reshape(B * T, n, 1)\n",
    "    P2 = P.reshape(B * T, n, n)\n",
    "\n",
    "    P2 = ensure_spd_batch(P2, eps=1e-6)\n",
    "    L = robust_cholesky(P2, eps=1e-6)\n",
    "\n",
    "    sol = torch.cholesky_solve(e2, L)  # [B*T,n,1]\n",
    "    maha = (e2.transpose(1, 2) @ sol).reshape(B, T)  # [B,T]\n",
    "\n",
    "    diag = torch.diagonal(L, dim1=-2, dim2=-1)\n",
    "    logdet = 2.0 * torch.sum(torch.log(diag + eps), dim=-1)  # [B*T]\n",
    "    logdet = logdet.reshape(B, T)\n",
    "\n",
    "    return 0.5 * (maha + logdet).mean()\n",
    "\n",
    "\n",
    "def rmse_over_time(xhat: torch.Tensor, x_true: torch.Tensor) -> torch.Tensor:\n",
    "    e = state_error(x_true, xhat)  # [N,T,2]\n",
    "    mse_t = (e ** 2).mean(dim=(0, 2))\n",
    "    return torch.sqrt(mse_t + 1e-12)\n",
    "\n",
    "\n",
    "def mse_state(xhat: torch.Tensor, x_true: torch.Tensor) -> torch.Tensor:\n",
    "    e = state_error(x_true, xhat)\n",
    "    return (e ** 2).mean()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Plots + coverage\n",
    "# -------------------------\n",
    "def plot_loss_history(train_hist: List[float], test_hist: List[float], title: str):\n",
    "    epochs = np.arange(1, len(train_hist) + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_hist, label=\"Train total\")\n",
    "    plt.plot(epochs, test_hist, label=\"Test MSE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss / MSE\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_error_with_ci_99(x_true_1, x_hat_1, P_1, title_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Plot per-state error with 99% CI band from covariance.\n",
    "    x_true_1: [T,2], x_hat_1: [T,2], P_1: [T,2,2]\n",
    "    \"\"\"\n",
    "    z99 = 2.5758293035489004\n",
    "    T = x_true_1.shape[0]\n",
    "    t = np.arange(T)\n",
    "\n",
    "    # error (wrap theta)\n",
    "    err_theta = ((x_true_1[:, 0] - x_hat_1[:, 0] + np.pi) % (2*np.pi)) - np.pi\n",
    "    err_omega = (x_true_1[:, 1] - x_hat_1[:, 1])\n",
    "    err = np.stack([err_theta, err_omega], axis=1)\n",
    "\n",
    "    var = np.stack([np.diag(P_1[k]) for k in range(T)], axis=0)  # [T,2]\n",
    "    std = np.sqrt(np.maximum(var, 1e-12))\n",
    "    band = z99 * std\n",
    "\n",
    "    names = [\"theta\", \"omega\"]\n",
    "    for i in range(2):\n",
    "        plt.figure()\n",
    "        plt.plot(t, err[:, i], label=f\"error ({names[i]})\")\n",
    "        plt.fill_between(t, -band[:, i], band[:, i], alpha=0.2, label=\"99% CI from P\")\n",
    "        plt.axhline(0.0, linewidth=1)\n",
    "        plt.xlabel(\"Time step\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.title(f\"{title_prefix} Error + 99% CI ({names[i]})\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def coverage_99(x_true: torch.Tensor, x_hat: torch.Tensor, P: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns coverage per state (theta, omega): fraction of time points where |error_i| <= z*sqrt(P_ii)\n",
    "    \"\"\"\n",
    "    z99 = 2.5758293035489004\n",
    "    e = state_error(x_true, x_hat)  # [N,T,2]\n",
    "    var = torch.diagonal(P, dim1=-2, dim2=-1)  # [N,T,2]\n",
    "    band = z99 * torch.sqrt(torch.clamp(var, min=1e-12))\n",
    "    inside = (torch.abs(e) <= band).float().mean(dim=(0, 1))  # [2]\n",
    "    return inside.cpu().numpy()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Train + compare (NONLINEAR demo)\n",
    "# -------------------------\n",
    "def train_pendulum_demo():\n",
    "    set_seed(0)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.float32\n",
    "\n",
    "    # --- Nonlinear problem params ---\n",
    "    dt = 0.05\n",
    "    g_over_L = 1.0\n",
    "    damping = 0.05\n",
    "\n",
    "    # UT params (slightly larger alpha can help nonlinearity)\n",
    "    ut_params = (0.2, 2.0, 0.0)\n",
    "\n",
    "    # True process noise\n",
    "    Q_true = torch.diag(torch.tensor([1e-4, 5e-4], device=device, dtype=dtype))\n",
    "\n",
    "    # Measurement outlier mixture\n",
    "    r_nom = 0.10\n",
    "    r_out = 1.50\n",
    "    p_out = 0.05\n",
    "\n",
    "    # Baseline assumed covariances (intentionally \"too optimistic\" for measurements)\n",
    "    # UKF baseline uses fixed R_base ~ r_nom^2, so it suffers on outliers.\n",
    "    gamma_Q = 10.0   # can be larger than true\n",
    "    gamma_R = 1.0    # keep at nominal (optimistic vs outliers)\n",
    "    Q_base = gamma_Q * Q_true\n",
    "    R_base = torch.tensor([[r_nom**2]], device=device, dtype=dtype) * gamma_R\n",
    "\n",
    "    print(\"=== Adopted Nonlinear Problem: Pendulum + Outlier Measurements ===\")\n",
    "    print(f\"device={device}\")\n",
    "    print(f\"dt={dt}, g/L={g_over_L}, damping={damping}\")\n",
    "    print(\"Q_true diag =\", torch.diag(Q_true).detach().cpu().numpy())\n",
    "    print(\"Q_base diag =\", torch.diag(Q_base).detach().cpu().numpy(), f\"(gamma_Q={gamma_Q})\")\n",
    "    print(\"R_base =\", R_base.item(), f\"(~ r_nom^2 * gamma_R, gamma_R={gamma_R})\")\n",
    "    print(f\"Measurement noise: nominal std={r_nom}, outlier std={r_out}, p_out={p_out}\")\n",
    "    print()\n",
    "\n",
    "    # f/h for sigma points\n",
    "    def f_fn(X):\n",
    "        # X:[B,L,2]\n",
    "        theta = X[..., 0]\n",
    "        omega = X[..., 1]\n",
    "        theta_next = theta + dt * omega\n",
    "        omega_next = omega + dt * (-g_over_L * torch.sin(theta) - damping * omega)\n",
    "        # keep theta wrapped\n",
    "        theta_next = (theta_next + math.pi) % (2 * math.pi) - math.pi\n",
    "        return torch.stack([theta_next, omega_next], dim=-1)\n",
    "\n",
    "    def h_fn(X):\n",
    "        theta = X[..., 0]\n",
    "        return torch.sin(theta).unsqueeze(-1)\n",
    "\n",
    "    # --- Data ---\n",
    "    T = 80\n",
    "    N_train, N_test = 6000, 1200\n",
    "\n",
    "    x_train, y_train, out_train = sample_pendulum_sequences(\n",
    "        N_train, T, dt, g_over_L, damping, Q_true, r_nom, r_out, p_out, device=device\n",
    "    )\n",
    "    x_test, y_test, out_test = sample_pendulum_sequences(\n",
    "        N_test, T, dt, g_over_L, damping, Q_true, r_nom, r_out, p_out, device=device\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(SeqDataset(x_train, y_train, out_train), batch_size=128, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(SeqDataset(x_test, y_test, out_test), batch_size=256, shuffle=False, num_workers=0)\n",
    "\n",
    "    # --- Init x0, P0 ---\n",
    "    x0 = torch.zeros(1, 2, device=device, dtype=dtype)\n",
    "    P0 = torch.diag(torch.tensor([1.0, 1.0], device=device, dtype=dtype)).unsqueeze(0)\n",
    "\n",
    "    # --- Model ---\n",
    "    model = UKNet_ScalarQR(n=2, m=1, hidden_size=64, ut_params=ut_params, log_scale_clip=8.0).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-6)\n",
    "\n",
    "    beta_cov = 0.10\n",
    "    num_epochs = 25\n",
    "\n",
    "    train_hist_total: List[float] = []\n",
    "    test_hist_mse: List[float] = []\n",
    "\n",
    "    for ep in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        tot_loss = 0.0\n",
    "        tot_mse = 0.0\n",
    "\n",
    "        for xb, yb, _ in train_loader:\n",
    "            B = xb.shape[0]\n",
    "            x0b = x0.expand(B, -1).contiguous()\n",
    "            P0b = P0.expand(B, -1, -1).contiguous()\n",
    "\n",
    "            opt.zero_grad()\n",
    "            xhat, P_hist, _, _ = model(yb, f_fn, h_fn, x0b, P0b, Q_base=Q_base, R_base=R_base)\n",
    "\n",
    "            loss_state = mse_state(xhat, xb)\n",
    "            e = state_error(xb, xhat)\n",
    "            loss_cov = state_nll_from_P(e, P_hist)\n",
    "\n",
    "            loss = (1.0 - beta_cov) * loss_state + beta_cov * loss_cov\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            tot_loss += loss.item() * B\n",
    "            tot_mse += loss_state.item() * B\n",
    "\n",
    "        train_total = tot_loss / len(train_loader.dataset)\n",
    "        train_mse = tot_mse / len(train_loader.dataset)\n",
    "\n",
    "        # test MSE\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot = 0.0\n",
    "            for xb, yb, _ in test_loader:\n",
    "                B = xb.shape[0]\n",
    "                x0b = x0.expand(B, -1).contiguous()\n",
    "                P0b = P0.expand(B, -1, -1).contiguous()\n",
    "                xhat, _, _, _ = model(yb, f_fn, h_fn, x0b, P0b, Q_base=Q_base, R_base=R_base)\n",
    "                tot += mse_state(xhat, xb).item() * B\n",
    "            test_mse = tot / len(test_loader.dataset)\n",
    "\n",
    "        train_hist_total.append(train_total)\n",
    "        test_hist_mse.append(test_mse)\n",
    "        print(f\"Epoch {ep:02d} | Train total: {train_total:.6f} (MSE={train_mse:.6f}) | Test MSE: {test_mse:.6f}\")\n",
    "\n",
    "    # --- Loss plot ---\n",
    "    plot_loss_history(train_hist_total, test_hist_mse, title=\"Pendulum: UKNet(Scalar Q/R) - Loss History\")\n",
    "\n",
    "    # --- Final comparison (UKN vs UKF baseline) ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        B = y_test.shape[0]\n",
    "        x0b = x0.expand(B, -1).contiguous()\n",
    "        P0b = P0.expand(B, -1, -1).contiguous()\n",
    "\n",
    "        # UKNet\n",
    "        xhat_ukn, P_ukn, sq_hist, sr_hist = model(y_test, f_fn, h_fn, x0b, P0b, Q_base=Q_base, R_base=R_base)\n",
    "        ukn_mse = mse_state(xhat_ukn, x_test).item()\n",
    "\n",
    "        # UKF baseline (fixed)\n",
    "        xhat_ukf, P_ukf = batch_ukf_filter(\n",
    "            y=y_test, f_fn=f_fn, h_fn=h_fn, Q=Q_base, R=R_base, x0=x0b, P0=P0b, ut_params=ut_params\n",
    "        )\n",
    "        ukf_mse = mse_state(xhat_ukf, x_test).item()\n",
    "\n",
    "        # EKF baseline (fixed)\n",
    "        xhat_ekf, P_ekf = batch_ekf_filter(\n",
    "            y=y_test,\n",
    "            dt=dt,\n",
    "            g_over_L=g_over_L,\n",
    "            damping=damping,\n",
    "            Q=Q_base,\n",
    "            R=R_base,\n",
    "            x0=x0b,\n",
    "            P0=P0b,\n",
    "        )\n",
    "        ekf_mse = mse_state(xhat_ekf, x_test).item()\n",
    "\n",
    "        print(\"\\n===== Final (Test Set) =====\")\n",
    "        print(f\"UKNet MSE: {ukn_mse:.6e}\")\n",
    "        print(f\"UKF  MSE: {ukf_mse:.6e}\")\n",
    "        print(f\"EKF  MSE: {ekf_mse:.6e}\")\n",
    "\n",
    "        # RMSE(t)\n",
    "        rmse_ukn = rmse_over_time(xhat_ukn, x_test).cpu().numpy()\n",
    "        rmse_ukf = rmse_over_time(xhat_ukf, x_test).cpu().numpy()\n",
    "        rmse_ekf = rmse_over_time(xhat_ekf, x_test).cpu().numpy()\n",
    "\n",
    "        t = np.arange(len(rmse_ukn))\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(t, rmse_ukn, label=\"UKNet RMSE(t)\")\n",
    "        plt.plot(t, rmse_ukf, label=\"UKF  RMSE(t)\")\n",
    "        plt.plot(t, rmse_ekf, label=\"EKF  RMSE(t)\")\n",
    "        plt.xlabel(\"Time step\")\n",
    "        plt.ylabel(\"RMSE\")\n",
    "        plt.title(\"RMSE over time (test avg)\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "        # Coverage 99%\n",
    "        cov_ukn = coverage_99(x_test, xhat_ukn, P_ukn)\n",
    "        cov_ukf = coverage_99(x_test, xhat_ukf, P_ukf)\n",
    "        cov_ekf = coverage_99(x_test, xhat_ekf, P_ekf)\n",
    "        print(\"\\n99% CI Coverage (fraction inside band)\")\n",
    "        print(f\"UKNet: theta={cov_ukn[0]:.3f}, omega={cov_ukn[1]:.3f}\")\n",
    "        print(f\"UKF : theta={cov_ukf[0]:.3f}, omega={cov_ukf[1]:.3f}\")\n",
    "        print(f\"EKF : theta={cov_ekf[0]:.3f}, omega={cov_ekf[1]:.3f}\")\n",
    "\n",
    "        # --- Sample plots ---\n",
    "        sample_idx = 0\n",
    "        x_true_1 = x_test[sample_idx].cpu().numpy()           # [T,2]\n",
    "        x_ukn_1 = xhat_ukn[sample_idx].cpu().numpy()\n",
    "        x_ukf_1 = xhat_ukf[sample_idx].cpu().numpy()\n",
    "        x_ekf_1 = xhat_ekf[sample_idx].cpu().numpy()\n",
    "\n",
    "        out_1 = out_test[sample_idx].cpu().numpy().squeeze(-1)  # [T]\n",
    "        sr_1 = torch.exp(sr_hist[sample_idx]).cpu().numpy().squeeze(-1)  # [T]\n",
    "\n",
    "        names = [\"theta\", \"omega\"]\n",
    "        for i in range(2):\n",
    "            plt.figure()\n",
    "            plt.plot(t, x_true_1[:, i], label=f\"True {names[i]}\")\n",
    "            plt.plot(t, x_ukn_1[:, i], label=f\"UKNet {names[i]}\")\n",
    "            plt.plot(t, x_ukf_1[:, i], label=f\"UKF  {names[i]}\")\n",
    "            plt.plot(t, x_ekf_1[:, i], label=f\"EKF  {names[i]}\")\n",
    "            plt.xlabel(\"Time step\")\n",
    "            plt.ylabel(names[i])\n",
    "            plt.title(f\"Sample trajectory (idx={sample_idx}) - {names[i]}\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "            plt.show()\n",
    "\n",
    "        # (extra) outlier mask vs learned R-scale\n",
    "        plt.figure()\n",
    "        plt.plot(t, sr_1, label=\"exp(sr_t) (R scale)\")\n",
    "        plt.plot(t, out_1 * (sr_1.max() if sr_1.max() > 0 else 1.0), label=\"outlier mask (scaled)\")\n",
    "        plt.xlabel(\"Time step\")\n",
    "        plt.title(\"Sample: outlier mask vs learned R scale\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "        # ---- LAST PLOT: error + 99% CI from covariance (UKNet) ----\n",
    "        P_ukn_1 = P_ukn[sample_idx].cpu().numpy()  # [T,2,2]\n",
    "        plot_error_with_ci_99(x_true_1, x_ukn_1, P_ukn_1, title_prefix=\"UKNet\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_pendulum_demo()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
