{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unscented KalmanNet (UKN) Toy Pipeline\n",
        "Duffing oscillator (state **x=[p,v]**, nx=2) with **2 unknown parameters θ=[k, α]** (nθ=2) that enter **only the dynamics f**.\n",
        "\n",
        "**Measurement is nonlinear (ny=2):**\n",
        "$$y_1 = p + a_p p^3 + v_{1},\\quad y_2 = v + a_v v^3 + v_{2}$$\n",
        "\n",
        "This notebook:\n",
        "1. Generates and saves **train/test** datasets (`.pt`)\n",
        "2. Trains **UKN = UKF backbone + GainNet(MLP→GRU→heads)**\n",
        "3. Evaluates vs baseline **Augmented UKF**\n",
        "4. Saves histories & predictions\n",
        "5. Plots: training curve, 99% error bands, and one example trajectory\n",
        "\n",
        "---\n",
        "✅ GPU is auto-detected (CUDA → MPS → CPU).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ====== Setup: imports, device, seeds ======\n",
        "import os, json, math, time\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from typing import Dict, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_best_device(force_cpu: bool = False) -> torch.device:\n",
        "    if force_cpu:\n",
        "        return torch.device(\"cpu\")\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "DEVICE = get_best_device(force_cpu=False)\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Device:\", DEVICE)\n",
        "if DEVICE.type == \"cuda\":\n",
        "    print(\"CUDA GPU:\", torch.cuda.get_device_name(0))\n",
        "elif DEVICE.type == \"mps\":\n",
        "    print(\"Apple MPS enabled\")\n",
        "\n",
        "# Reproducibility (note: exact reproducibility on GPU can still vary)\n",
        "SEED = 0\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Math/Linear Algebra helpers (SPD, Cholesky, UT weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def symmetrize(A: torch.Tensor) -> torch.Tensor:\n",
        "    return 0.5 * (A + A.transpose(-1, -2))\n",
        "\n",
        "def project_spd(A: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    A = symmetrize(A)\n",
        "    evals, evecs = torch.linalg.eigh(A)\n",
        "    evals = torch.clamp(evals, min=eps)\n",
        "    return (evecs * evals.unsqueeze(-2)) @ evecs.transpose(-1, -2)\n",
        "\n",
        "def safe_cholesky(A: torch.Tensor, jitter: float = 1e-6, max_tries: int = 6) -> torch.Tensor:\n",
        "    A = symmetrize(A)\n",
        "    n = A.shape[-1]\n",
        "    if A.dim() == 2:\n",
        "        I = torch.eye(n, device=A.device, dtype=A.dtype)\n",
        "    else:\n",
        "        B = A.shape[0]\n",
        "        I = torch.eye(n, device=A.device, dtype=A.dtype).expand(B, -1, -1)\n",
        "\n",
        "    for i in range(max_tries):\n",
        "        try:\n",
        "            return torch.linalg.cholesky(A + (jitter * (10**i)) * I)\n",
        "        except RuntimeError:\n",
        "            continue\n",
        "\n",
        "    A_spd = project_spd(A, eps=jitter)\n",
        "    return torch.linalg.cholesky(A_spd)\n",
        "\n",
        "@dataclass\n",
        "class UTParams:\n",
        "    alpha: float = 0.8\n",
        "    beta: float = 2.0\n",
        "    kappa: float = 0.0\n",
        "\n",
        "def unscented_weights(n: int, params: UTParams, device=None, dtype=None):\n",
        "    alpha, beta, kappa = params.alpha, params.beta, params.kappa\n",
        "    lam = alpha**2 * (n + kappa) - n\n",
        "    c = n + lam\n",
        "    wm = torch.full((2*n+1,), 1.0/(2.0*c), device=device, dtype=dtype)\n",
        "    wc = torch.full((2*n+1,), 1.0/(2.0*c), device=device, dtype=dtype)\n",
        "    wm[0] = lam / c\n",
        "    wc[0] = lam / c + (1.0 - alpha**2 + beta)\n",
        "    sqrt_c = math.sqrt(c)\n",
        "    return wm, wc, sqrt_c\n",
        "\n",
        "def sigma_points(mean: torch.Tensor, cov: torch.Tensor, sqrt_c: float, jitter: float = 1e-6):\n",
        "    # mean: (B,n), cov: (B,n,n) -> sigma: (B,2n+1,n)\n",
        "    B, n = mean.shape\n",
        "    L = safe_cholesky(cov, jitter=jitter)  # (B,n,n)\n",
        "    scaled = sqrt_c * L\n",
        "    A = scaled.transpose(1, 2)  # columns as rows\n",
        "    m = mean.unsqueeze(1)\n",
        "    return torch.cat([m, m + A, m - A], dim=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Toy system: Duffing dynamics + nonlinear measurement\n",
        "- Augmented state: z=[p, v, k, α] (n=4)\n",
        "- Dynamics f: Duffing oscillator (θ only in f)\n",
        "- Measurement h (nonlinear): y=[p+ap*p³, v+av*v³] + noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "@dataclass\n",
        "class ToyConfig:\n",
        "    dt: float = 0.05\n",
        "    c: float = 0.25\n",
        "    ap: float = 0.1\n",
        "    av: float = 0.1\n",
        "\n",
        "    # simulation process noise (x only)\n",
        "    Qx_p: float = 1e-5\n",
        "    Qx_v: float = 1e-4\n",
        "\n",
        "    # measurement noise (in measurement space)\n",
        "    sigma_meas: float = 0.05\n",
        "    outlier: bool = True\n",
        "    p_out: float = 0.05\n",
        "    outlier_scale: float = 25.0  # Rout = scale * R0\n",
        "\n",
        "    # input\n",
        "    u_refresh: int = 5\n",
        "    u_min: float = -1.0\n",
        "    u_max: float = 1.0\n",
        "\n",
        "    # parameter distribution\n",
        "    k_min: float = 0.8\n",
        "    k_max: float = 1.2\n",
        "    alpha_min: float = 0.2\n",
        "    alpha_max: float = 0.6\n",
        "\n",
        "    # initial state distribution\n",
        "    p0_mean: float = 1.0\n",
        "    p0_std: float = 0.2\n",
        "    v0_mean: float = 0.0\n",
        "    v0_std: float = 0.2\n",
        "\n",
        "def f_duffing(z: torch.Tensor, u: torch.Tensor, dt: float, c: float) -> torch.Tensor:\n",
        "    # z: (...,4), u: broadcastable\n",
        "    p = z[..., 0]\n",
        "    v = z[..., 1]\n",
        "    k = z[..., 2]\n",
        "    alpha = z[..., 3]\n",
        "    p_next = p + dt * v\n",
        "    v_next = v + dt * (-c * v - k * p - alpha * (p**3) + u)\n",
        "    return torch.stack([p_next, v_next, k, alpha], dim=-1)\n",
        "\n",
        "def h_nonlinear(z: torch.Tensor, ap: float, av: float) -> torch.Tensor:\n",
        "    p = z[..., 0]\n",
        "    v = z[..., 1]\n",
        "    y1 = p + ap * (p**3)\n",
        "    y2 = v + av * (v**3)\n",
        "    return torch.stack([y1, y2], dim=-1)\n",
        "\n",
        "def invert_cubic_measurement(y: torch.Tensor, a: float, n_iter: int = 6, eps: float = 1e-6) -> torch.Tensor:\n",
        "    # Solve x + a x^3 = y via Newton\n",
        "    x = y.clone()\n",
        "    for _ in range(n_iter):\n",
        "        f = x + a * (x**3) - y\n",
        "        fp = 1.0 + 3.0 * a * (x**2)\n",
        "        x = x - f / (fp + eps)\n",
        "    return x\n",
        "\n",
        "def approx_inverse_h(y: torch.Tensor, ap: float, av: float) -> torch.Tensor:\n",
        "    p_est = invert_cubic_measurement(y[..., 0], ap)\n",
        "    v_est = invert_cubic_measurement(y[..., 1], av)\n",
        "    return torch.stack([p_est, v_est], dim=-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) UKF backbone (predict + measurement stats + generalized Joseph update)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def ukf_predict(z: torch.Tensor, P: torch.Tensor, u_prev: torch.Tensor, Q: torch.Tensor,\n",
        "                wm: torch.Tensor, wc: torch.Tensor, sqrt_c: float,\n",
        "                dt: float, c: float, jitter: float = 1e-6):\n",
        "    # z: (B,n), P:(B,n,n), u_prev:(B,) -> z_pred,P_pred,X_sigma\n",
        "    B, n = z.shape\n",
        "    sig = sigma_points(z, P, sqrt_c, jitter=jitter)  # (B,S,n)\n",
        "    u_b = u_prev.view(B, 1)                          # (B,1)\n",
        "    X_sigma = f_duffing(sig, u_b, dt=dt, c=c)        # (B,S,n)\n",
        "\n",
        "    z_pred = torch.sum(X_sigma * wm.view(1, -1, 1), dim=1)\n",
        "    dX = X_sigma - z_pred.unsqueeze(1)\n",
        "    P_pred = torch.einsum('bsi,bsj,s->bij', dX, dX, wc)\n",
        "\n",
        "    if Q.dim() == 2:\n",
        "        P_pred = P_pred + Q.unsqueeze(0)\n",
        "    else:\n",
        "        P_pred = P_pred + Q\n",
        "    P_pred = symmetrize(P_pred) + jitter * torch.eye(n, device=z.device, dtype=z.dtype).unsqueeze(0)\n",
        "    return z_pred, P_pred, X_sigma\n",
        "\n",
        "def ukf_measurement_stats(z_pred: torch.Tensor, P_pred: torch.Tensor, X_sigma: torch.Tensor,\n",
        "                          R: torch.Tensor, wm: torch.Tensor, wc: torch.Tensor,\n",
        "                          ap: float, av: float, jitter: float = 1e-6):\n",
        "    # y_pred, S, P_zy from sigma points\n",
        "    B, S, n = X_sigma.shape\n",
        "    ny = R.shape[-1]\n",
        "\n",
        "    Y_sigma = h_nonlinear(X_sigma, ap=ap, av=av)      # (B,S,ny)\n",
        "    y_pred = torch.sum(Y_sigma * wm.view(1, -1, 1), dim=1)\n",
        "\n",
        "    dY = Y_sigma - y_pred.unsqueeze(1)\n",
        "    dX = X_sigma - z_pred.unsqueeze(1)\n",
        "\n",
        "    S_cov = torch.einsum('bsi,bsj,s->bij', dY, dY, wc)\n",
        "    if R.dim() == 2:\n",
        "        S_cov = S_cov + R.unsqueeze(0)\n",
        "    else:\n",
        "        S_cov = S_cov + R\n",
        "    S_cov = symmetrize(S_cov) + jitter * torch.eye(ny, device=z_pred.device, dtype=z_pred.dtype).unsqueeze(0)\n",
        "\n",
        "    P_zy = torch.einsum('bsi,bsj,s->bij', dX, dY, wc)  # (B,n,ny)\n",
        "    return y_pred, S_cov, P_zy\n",
        "\n",
        "def compute_K_ukf(P_zy: torch.Tensor, S: torch.Tensor, jitter: float = 1e-6):\n",
        "    cholS = safe_cholesky(S, jitter=jitter)\n",
        "    K_T = torch.cholesky_solve(P_zy.transpose(-1, -2), cholS)  # (B,ny,n)\n",
        "    K = K_T.transpose(-1, -2)                                  # (B,n,ny)\n",
        "    return K, cholS\n",
        "\n",
        "def generalized_joseph(P_pred: torch.Tensor, K: torch.Tensor, S: torch.Tensor, P_zy: torch.Tensor,\n",
        "                       jitter: float = 1e-6):\n",
        "    P_yz = P_zy.transpose(-1, -2)  # (B,ny,n)\n",
        "    P_post = P_pred - (K @ P_yz) - (P_zy @ K.transpose(-1, -2)) + (K @ S @ K.transpose(-1, -2))\n",
        "    P_post = symmetrize(P_post) + jitter * torch.eye(P_pred.shape[-1], device=P_pred.device, dtype=P_pred.dtype).unsqueeze(0)\n",
        "    return P_post\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) GainNet (MLP → GRU → heads) + UKN Filter\n",
        "**Outputs:** ΔK (4×2) and row-wise gates (ρx, ρθ).\n",
        "\n",
        "**Features (Core-20):**\n",
        "- whitened innovation, NIS, logdet(S)\n",
        "- Δz⁻, log diag(P⁻)\n",
        "- whitened cross-cov C_{θy}\n",
        "- δz_ukf = K_ukf ν\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class GainNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_in: int = 20,\n",
        "                 hidden: int = 32,\n",
        "                 embed: int = 32,\n",
        "                 deltaK_scale: float = 0.1,\n",
        "                 rho_theta_max: float = 0.3,\n",
        "                 init_rho_bias: Tuple[float, float] = (-3.0, -5.0)):\n",
        "        super().__init__()\n",
        "        self.deltaK_scale = deltaK_scale\n",
        "        self.rho_theta_max = rho_theta_max\n",
        "\n",
        "        self.ln = nn.LayerNorm(d_in)\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Linear(d_in, embed), nn.ReLU(),\n",
        "            nn.Linear(embed, embed), nn.ReLU(),\n",
        "        )\n",
        "        self.gru = nn.GRUCell(embed, hidden)\n",
        "\n",
        "        self.trunk = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU())\n",
        "\n",
        "        self.dk_head = nn.Sequential(\n",
        "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
        "            nn.Linear(hidden, 8),\n",
        "        )\n",
        "        self.gate_head = nn.Linear(hidden, 2)\n",
        "\n",
        "        # Init: start near UKF (ΔK ~ 0, rho ~ small)\n",
        "        nn.init.zeros_(self.dk_head[-1].weight)\n",
        "        nn.init.zeros_(self.dk_head[-1].bias)\n",
        "        nn.init.zeros_(self.gate_head.weight)\n",
        "        self.gate_head.bias.data[:] = torch.tensor(init_rho_bias, dtype=self.gate_head.bias.dtype)\n",
        "\n",
        "    def forward(self, feat: torch.Tensor, h: torch.Tensor, K_ukf: torch.Tensor):\n",
        "        x = self.ln(feat)\n",
        "        x = self.enc(x)\n",
        "        h_new = self.gru(x, h)\n",
        "        t = self.trunk(h_new)\n",
        "\n",
        "        dk_raw = self.dk_head(t)     # (B,8)\n",
        "        gate_raw = self.gate_head(t) # (B,2)\n",
        "\n",
        "        rho_x = torch.sigmoid(gate_raw[:, 0:1])                    # (B,1)\n",
        "        rho_theta = self.rho_theta_max * torch.sigmoid(gate_raw[:, 1:2])\n",
        "\n",
        "        scale = self.deltaK_scale * torch.linalg.norm(K_ukf.reshape(K_ukf.shape[0], -1),\n",
        "                                                      dim=-1, keepdim=True)  # (B,1)\n",
        "        deltaK = (scale * torch.tanh(dk_raw)).view(-1, 4, 2)\n",
        "        return deltaK, rho_x, rho_theta, h_new\n",
        "\n",
        "class UKNFilter(nn.Module):\n",
        "    def __init__(self,\n",
        "                 toy: ToyConfig,\n",
        "                 ut_params: UTParams = UTParams(alpha=0.8, beta=2.0, kappa=0.0),\n",
        "                 Q: Optional[torch.Tensor] = None,\n",
        "                 R: Optional[torch.Tensor] = None,\n",
        "                 jitter: float = 1e-6,\n",
        "                 feature_eps: float = 1e-8,\n",
        "                 hidden: int = 32,\n",
        "                 deltaK_scale: float = 0.1,\n",
        "                 rho_theta_max: float = 0.3):\n",
        "        super().__init__()\n",
        "        self.toy = toy\n",
        "        self.jitter = jitter\n",
        "        self.feature_eps = feature_eps\n",
        "        self.n_z = 4\n",
        "        self.n_y = 2\n",
        "\n",
        "        if Q is None:\n",
        "            Q = torch.diag(torch.tensor([1e-5, 1e-4, 1e-6, 1e-6], dtype=torch.float32))\n",
        "        if R is None:\n",
        "            sigma = toy.sigma_meas\n",
        "            R = torch.diag(torch.tensor([sigma**2, sigma**2], dtype=torch.float32))\n",
        "\n",
        "        self.register_buffer(\"Q\", Q)\n",
        "        self.register_buffer(\"R\", R)\n",
        "\n",
        "        wm, wc, sqrt_c = unscented_weights(self.n_z, ut_params, device=Q.device, dtype=Q.dtype)\n",
        "        self.register_buffer(\"wm\", wm)\n",
        "        self.register_buffer(\"wc\", wc)\n",
        "        self.sqrt_c = sqrt_c\n",
        "\n",
        "        self.hidden = hidden\n",
        "        self.gain_net = GainNet(d_in=20, hidden=hidden, embed=32,\n",
        "                                deltaK_scale=deltaK_scale,\n",
        "                                rho_theta_max=rho_theta_max)\n",
        "\n",
        "    def _build_features(self, nu, cholS, z_pred, P_pred, P_zy, K_ukf, z_prev_ref):\n",
        "        B = nu.shape[0]\n",
        "        # whitened innovation\n",
        "        tilde_nu = torch.linalg.solve_triangular(cholS, nu.unsqueeze(-1), upper=False).squeeze(-1)  # (B,2)\n",
        "        nis = torch.sum(tilde_nu**2, dim=-1, keepdim=True)                                          # (B,1)\n",
        "        logdetS = 2.0 * torch.sum(torch.log(torch.diagonal(cholS, dim1=-2, dim2=-1) + self.feature_eps),\n",
        "                                  dim=-1, keepdim=True)                                             # (B,1)\n",
        "\n",
        "        delta_z_pred = z_pred - z_prev_ref                                                           # (B,4)\n",
        "        logdiagP = torch.log(torch.diagonal(P_pred, dim1=-2, dim2=-1) + self.feature_eps)            # (B,4)\n",
        "\n",
        "        # whitened cross-cov for theta-y:\n",
        "        P_theta_y = P_zy[:, 2:4, :]         # (B,2,2)\n",
        "        P_theta_theta = P_pred[:, 2:4, 2:4] # (B,2,2)\n",
        "        L_theta = safe_cholesky(P_theta_theta, jitter=self.jitter)\n",
        "        C_temp = torch.linalg.solve_triangular(L_theta, P_theta_y, upper=False)                      # (B,2,2)\n",
        "\n",
        "        I = torch.eye(self.n_y, device=nu.device, dtype=nu.dtype).expand(B, -1, -1)\n",
        "        inv_LS = torch.linalg.solve_triangular(cholS, I, upper=False)                                # (B,2,2)\n",
        "        C_theta_y = C_temp @ inv_LS.transpose(-1, -2)                                                # (B,2,2)\n",
        "        vecC = C_theta_y.reshape(B, -1)                                                              # (B,4)\n",
        "\n",
        "        delta_z_ukf = (K_ukf @ nu.unsqueeze(-1)).squeeze(-1)                                         # (B,4)\n",
        "\n",
        "        feat = torch.cat([tilde_nu, nis, logdetS, delta_z_pred, logdiagP, vecC, delta_z_ukf], dim=-1)\n",
        "        return feat\n",
        "\n",
        "    def forward(self, y: torch.Tensor, u: torch.Tensor, z0: torch.Tensor, P0: torch.Tensor):\n",
        "        B, T, _ = y.shape\n",
        "        z, P = z0, P0\n",
        "        h = torch.zeros(B, self.hidden, device=y.device, dtype=y.dtype)\n",
        "\n",
        "        z_hist = []\n",
        "        z_prev = z.clone()\n",
        "\n",
        "        for t in range(T):\n",
        "            if t == 0:\n",
        "                z_pred, P_pred = z, P\n",
        "                X_sigma = sigma_points(z_pred, P_pred, self.sqrt_c, jitter=self.jitter)\n",
        "                z_prev_ref = z_pred\n",
        "            else:\n",
        "                z_pred, P_pred, X_sigma = ukf_predict(z, P, u[:, t-1], self.Q,\n",
        "                                                      self.wm, self.wc, self.sqrt_c,\n",
        "                                                      dt=self.toy.dt, c=self.toy.c, jitter=self.jitter)\n",
        "                z_prev_ref = z_prev\n",
        "\n",
        "            y_pred, S, P_zy = ukf_measurement_stats(z_pred, P_pred, X_sigma, self.R,\n",
        "                                                    self.wm, self.wc,\n",
        "                                                    ap=self.toy.ap, av=self.toy.av,\n",
        "                                                    jitter=self.jitter)\n",
        "            nu = y[:, t, :] - y_pred\n",
        "            K_ukf, cholS = compute_K_ukf(P_zy, S, jitter=self.jitter)\n",
        "\n",
        "            feat = self._build_features(nu, cholS, z_pred, P_pred, P_zy, K_ukf, z_prev_ref)\n",
        "            deltaK, rho_x, rho_theta, h = self.gain_net(feat, h, K_ukf)\n",
        "\n",
        "            g_row = torch.cat([rho_x, rho_x, rho_theta, rho_theta], dim=-1)  # (B,4)\n",
        "            K = K_ukf + deltaK * g_row.unsqueeze(-1)\n",
        "\n",
        "            z_post = z_pred + (K @ nu.unsqueeze(-1)).squeeze(-1)\n",
        "            P_post = generalized_joseph(P_pred, K, S, P_zy, jitter=self.jitter)\n",
        "\n",
        "            z_hist.append(z_post)\n",
        "            z_prev = z_post\n",
        "            z, P = z_post, P_post\n",
        "\n",
        "        return torch.stack(z_hist, dim=1)\n",
        "\n",
        "class AugmentedUKF(nn.Module):\n",
        "    def __init__(self, toy: ToyConfig,\n",
        "                 ut_params: UTParams = UTParams(alpha=0.8, beta=2.0, kappa=0.0),\n",
        "                 Q: Optional[torch.Tensor] = None,\n",
        "                 R: Optional[torch.Tensor] = None,\n",
        "                 jitter: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.toy = toy\n",
        "        self.jitter = jitter\n",
        "        self.n_z = 4\n",
        "        if Q is None:\n",
        "            Q = torch.diag(torch.tensor([1e-5, 1e-4, 1e-6, 1e-6], dtype=torch.float32))\n",
        "        if R is None:\n",
        "            sigma = toy.sigma_meas\n",
        "            R = torch.diag(torch.tensor([sigma**2, sigma**2], dtype=torch.float32))\n",
        "        self.register_buffer(\"Q\", Q)\n",
        "        self.register_buffer(\"R\", R)\n",
        "        wm, wc, sqrt_c = unscented_weights(self.n_z, ut_params, device=Q.device, dtype=Q.dtype)\n",
        "        self.register_buffer(\"wm\", wm)\n",
        "        self.register_buffer(\"wc\", wc)\n",
        "        self.sqrt_c = sqrt_c\n",
        "\n",
        "    def forward(self, y: torch.Tensor, u: torch.Tensor, z0: torch.Tensor, P0: torch.Tensor):\n",
        "        B, T, _ = y.shape\n",
        "        z, P = z0, P0\n",
        "        z_hist = []\n",
        "        for t in range(T):\n",
        "            if t == 0:\n",
        "                z_pred, P_pred = z, P\n",
        "                X_sigma = sigma_points(z_pred, P_pred, self.sqrt_c, jitter=self.jitter)\n",
        "            else:\n",
        "                z_pred, P_pred, X_sigma = ukf_predict(z, P, u[:, t-1], self.Q,\n",
        "                                                      self.wm, self.wc, self.sqrt_c,\n",
        "                                                      dt=self.toy.dt, c=self.toy.c, jitter=self.jitter)\n",
        "\n",
        "            y_pred, S, P_zy = ukf_measurement_stats(z_pred, P_pred, X_sigma, self.R,\n",
        "                                                    self.wm, self.wc,\n",
        "                                                    ap=self.toy.ap, av=self.toy.av,\n",
        "                                                    jitter=self.jitter)\n",
        "            nu = y[:, t, :] - y_pred\n",
        "            K_ukf, _ = compute_K_ukf(P_zy, S, jitter=self.jitter)\n",
        "            z_post = z_pred + (K_ukf @ nu.unsqueeze(-1)).squeeze(-1)\n",
        "            P_post = generalized_joseph(P_pred, K_ukf, S, P_zy, jitter=self.jitter)\n",
        "            z_hist.append(z_post)\n",
        "            z, P = z_post, P_post\n",
        "        return torch.stack(z_hist, dim=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Dataset generation, saving, loading\n",
        "We generate `train.pt` and `test.pt` so training/testing are repeatable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, pt_path: str):\n",
        "        blob = torch.load(pt_path, map_location=\"cpu\")\n",
        "        self.x = blob[\"x\"]          # (N,T,2)\n",
        "        self.y = blob[\"y\"]          # (N,T,2)\n",
        "        self.theta = blob[\"theta\"]  # (N,2)\n",
        "        self.u = blob[\"u\"]          # (N,T)\n",
        "        self.config = blob[\"config\"]\n",
        "        self.meta = blob.get(\"meta\", {})\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\"x\": self.x[idx], \"y\": self.y[idx], \"theta\": self.theta[idx], \"u\": self.u[idx]}\n",
        "\n",
        "def simulate_duffing_dataset(N: int, T: int, cfg: ToyConfig, seed: int) -> Dict[str, torch.Tensor]:\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    Qx = np.diag([cfg.Qx_p, cfg.Qx_v]).astype(np.float32)\n",
        "\n",
        "    sigma = cfg.sigma_meas\n",
        "    R0 = np.diag([sigma**2, sigma**2]).astype(np.float32)\n",
        "    Rout = (cfg.outlier_scale * R0).astype(np.float32)\n",
        "\n",
        "    x = np.zeros((N, T, 2), dtype=np.float32)\n",
        "    y = np.zeros((N, T, 2), dtype=np.float32)\n",
        "    u = np.zeros((N, T), dtype=np.float32)\n",
        "    theta = np.zeros((N, 2), dtype=np.float32)\n",
        "\n",
        "    for n in range(N):\n",
        "        k = rng.uniform(cfg.k_min, cfg.k_max)\n",
        "        alpha = rng.uniform(cfg.alpha_min, cfg.alpha_max)\n",
        "        theta[n] = [k, alpha]\n",
        "\n",
        "        # input (piecewise constant)\n",
        "        for t in range(T):\n",
        "            if t % cfg.u_refresh == 0:\n",
        "                u[n, t] = rng.uniform(cfg.u_min, cfg.u_max)\n",
        "            else:\n",
        "                u[n, t] = u[n, t-1]\n",
        "\n",
        "        # init\n",
        "        x[n, 0, 0] = rng.normal(cfg.p0_mean, cfg.p0_std)\n",
        "        x[n, 0, 1] = rng.normal(cfg.v0_mean, cfg.v0_std)\n",
        "\n",
        "        # simulate dynamics\n",
        "        for t in range(T-1):\n",
        "            p, v = x[n, t]\n",
        "            w = rng.multivariate_normal(np.zeros(2), Qx)\n",
        "            p_next = p + cfg.dt * v + w[0]\n",
        "            v_next = v + cfg.dt * (-cfg.c*v - k*p - alpha*(p**3) + u[n, t]) + w[1]\n",
        "            x[n, t+1] = [p_next, v_next]\n",
        "\n",
        "        # measurement (nonlinear + additive noise in measurement space)\n",
        "        for t in range(T):\n",
        "            p, v = x[n, t]\n",
        "            y_clean = np.array([p + cfg.ap*(p**3), v + cfg.av*(v**3)], dtype=np.float32)\n",
        "            if cfg.outlier and (rng.uniform() < cfg.p_out):\n",
        "                v_meas = rng.multivariate_normal(np.zeros(2), Rout)\n",
        "            else:\n",
        "                v_meas = rng.multivariate_normal(np.zeros(2), R0)\n",
        "            y[n, t] = y_clean + v_meas.astype(np.float32)\n",
        "\n",
        "    return {\"x\": torch.from_numpy(x), \"y\": torch.from_numpy(y), \"theta\": torch.from_numpy(theta), \"u\": torch.from_numpy(u)}\n",
        "\n",
        "def save_dataset(pt_path: str, data: Dict[str, torch.Tensor], cfg: ToyConfig, meta: Dict):\n",
        "    os.makedirs(os.path.dirname(pt_path), exist_ok=True)\n",
        "    blob = {\"x\": data[\"x\"].contiguous(), \"y\": data[\"y\"].contiguous(),\n",
        "            \"theta\": data[\"theta\"].contiguous(), \"u\": data[\"u\"].contiguous(),\n",
        "            \"config\": cfg.__dict__, \"meta\": meta}\n",
        "    torch.save(blob, pt_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Priors for filtering\n",
        "We build the initial prior from the first measurement `y[:,0]` using an approximate inverse of h.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def make_initial_prior(y0: torch.Tensor,\n",
        "                       toy: ToyConfig,\n",
        "                       theta_prior=(1.0, 0.4),\n",
        "                       P0_diag=(0.2**2, 0.2**2, 0.5**2, 0.5**2)):\n",
        "    B = y0.shape[0]\n",
        "    z0 = torch.zeros((B, 4), dtype=y0.dtype, device=y0.device)\n",
        "\n",
        "    x0 = approx_inverse_h(y0, ap=toy.ap, av=toy.av)\n",
        "    z0[:, 0:2] = x0\n",
        "    z0[:, 2] = float(theta_prior[0])\n",
        "    z0[:, 3] = float(theta_prior[1])\n",
        "\n",
        "    P0 = torch.diag(torch.tensor(P0_diag, dtype=y0.dtype, device=y0.device)).unsqueeze(0).repeat(B, 1, 1)\n",
        "    return z0, P0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Training helpers + evaluation\n",
        "Includes optional **dominance loss vs UKF** to encourage UKN to beat baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def batch_loss(z_hat: torch.Tensor, x_true: torch.Tensor, theta_true: torch.Tensor,\n",
        "               lambda_theta: float = 1.0, lambda_smooth: float = 0.1):\n",
        "    x_hat = z_hat[..., 0:2]\n",
        "    th_hat = z_hat[..., 2:4]\n",
        "    th_true_seq = theta_true[:, None, :].expand_as(th_hat)\n",
        "\n",
        "    loss_x = F.mse_loss(x_hat, x_true)\n",
        "    loss_th = F.mse_loss(th_hat, th_true_seq)\n",
        "\n",
        "    dth = th_hat[:, 1:, :] - th_hat[:, :-1, :]\n",
        "    loss_smooth = F.mse_loss(dth, torch.zeros_like(dth))\n",
        "\n",
        "    loss = loss_x + lambda_theta * loss_th + lambda_smooth * loss_smooth\n",
        "    metrics = {\"loss_x\": float(loss_x.detach().cpu()),\n",
        "               \"loss_th\": float(loss_th.detach().cpu()),\n",
        "               \"loss_smooth\": float(loss_smooth.detach().cpu()),\n",
        "               \"loss_total\": float(loss.detach().cpu())}\n",
        "    return loss, metrics\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_full_prediction(model: nn.Module, dataset: SequenceDataset, toy: ToyConfig,\n",
        "                        device: torch.device, batch_size: int = 256):\n",
        "    model.eval()\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "    outs = []\n",
        "    for batch in loader:\n",
        "        y = batch[\"y\"].to(device)\n",
        "        u = batch[\"u\"].to(device)\n",
        "        z0, P0 = make_initial_prior(y[:, 0, :], toy=toy)\n",
        "        z_hat = model(y, u, z0, P0)\n",
        "        outs.append(z_hat.detach().cpu())\n",
        "    return torch.cat(outs, dim=0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_mse(model: nn.Module, loader: DataLoader, toy: ToyConfig, device: torch.device):\n",
        "    model.eval()\n",
        "    tot_x, tot_th, n = 0.0, 0.0, 0\n",
        "    for batch in loader:\n",
        "        x = batch[\"x\"].to(device)\n",
        "        y = batch[\"y\"].to(device)\n",
        "        u = batch[\"u\"].to(device)\n",
        "        theta = batch[\"theta\"].to(device)\n",
        "        z0, P0 = make_initial_prior(y[:, 0, :], toy=toy)\n",
        "        z_hat = model(y, u, z0, P0)\n",
        "        x_hat = z_hat[..., 0:2]\n",
        "        th_hat = z_hat[..., 2:4]\n",
        "        th_true_seq = theta[:, None, :].expand_as(th_hat)\n",
        "        tot_x += float(F.mse_loss(x_hat, x).cpu())\n",
        "        tot_th += float(F.mse_loss(th_hat, th_true_seq).cpu())\n",
        "        n += 1\n",
        "    return {\"mse_x\": tot_x/max(1,n), \"mse_theta\": tot_th/max(1,n), \"mse_total\": (tot_x+tot_th)/max(1,n)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Generate & save datasets\n",
        "Adjust parameters then run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "DATA_DIR = \"data_ukn_nonlinear_meas\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "T = 200\n",
        "TRAIN_N = 5000   # increase to 20000+ for stronger results\n",
        "TEST_N  = 1000\n",
        "\n",
        "toy_cfg = ToyConfig(\n",
        "    dt=0.05,\n",
        "    c=0.25,\n",
        "    ap=0.15,\n",
        "    av=0.15,\n",
        "    sigma_meas=0.05,\n",
        "    outlier=True,\n",
        "    p_out=0.08,\n",
        "    outlier_scale=49.0,\n",
        ")\n",
        "\n",
        "train_path = os.path.join(DATA_DIR, \"train.pt\")\n",
        "test_path  = os.path.join(DATA_DIR, \"test.pt\")\n",
        "\n",
        "if (not os.path.exists(train_path)) or (not os.path.exists(test_path)):\n",
        "    print(\"Generating datasets...\")\n",
        "    train_data = simulate_duffing_dataset(TRAIN_N, T, toy_cfg, seed=SEED)\n",
        "    save_dataset(train_path, train_data, toy_cfg, meta={\"split\":\"train\",\"seed\":SEED})\n",
        "    test_data = simulate_duffing_dataset(TEST_N, T, toy_cfg, seed=SEED+999)\n",
        "    save_dataset(test_path, test_data, toy_cfg, meta={\"split\":\"test\",\"seed\":SEED+999})\n",
        "    print(\"Saved:\", train_path, test_path)\n",
        "else:\n",
        "    print(\"Datasets already exist:\", train_path, test_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Load datasets + build DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "train_ds = SequenceDataset(os.path.join(DATA_DIR, \"train.pt\"))\n",
        "test_ds  = SequenceDataset(os.path.join(DATA_DIR, \"test.pt\"))\n",
        "toy = ToyConfig(**train_ds.config)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0,\n",
        "                          pin_memory=(DEVICE.type==\"cuda\"))\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0,\n",
        "                          pin_memory=(DEVICE.type==\"cuda\"))\n",
        "\n",
        "print(\"Train size:\", len(train_ds), \"Test size:\", len(test_ds))\n",
        "print(\"Toy config:\", toy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Train UKN (Plan B) vs baseline UKF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "EPOCHS = 30\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 0.0\n",
        "GRAD_CLIP = 5.0\n",
        "\n",
        "LAMBDA_THETA = 1.0\n",
        "LAMBDA_SMOOTH = 0.1\n",
        "\n",
        "USE_DOM_LOSS = True\n",
        "LAMBDA_DOM = 1.0\n",
        "DOM_MARGIN = 0.0\n",
        "\n",
        "ukn = UKNFilter(toy=toy, hidden=32, deltaK_scale=0.1, rho_theta_max=0.3).to(DEVICE)\n",
        "ukf = AugmentedUKF(toy=toy).to(DEVICE)\n",
        "\n",
        "opt = torch.optim.Adam(ukn.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "history = {\"epoch\": [], \"train_loss\": [], \"train_dom\": [],\n",
        "           \"test_mse_total\": [], \"test_mse_x\": [], \"test_mse_theta\": []}\n",
        "\n",
        "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "RESULTS_DIR = os.path.join(\"results_ukn\", f\"run_{RUN_ID}\")\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(RESULTS_DIR, \"config.json\"), \"w\") as f:\n",
        "    json.dump({\"toy\": toy.__dict__,\n",
        "               \"TRAIN_N\": TRAIN_N, \"TEST_N\": TEST_N, \"T\": T,\n",
        "               \"EPOCHS\": EPOCHS, \"BATCH_SIZE\": BATCH_SIZE, \"LR\": LR,\n",
        "               \"USE_DOM_LOSS\": USE_DOM_LOSS, \"LAMBDA_DOM\": LAMBDA_DOM, \"DOM_MARGIN\": DOM_MARGIN,\n",
        "               \"DEVICE\": str(DEVICE)}, f, indent=2)\n",
        "\n",
        "best = float(\"inf\")\n",
        "best_state = None\n",
        "\n",
        "t0 = time.time()\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    ukn.train()\n",
        "    epoch_loss = 0.0\n",
        "    epoch_dom = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        x = batch[\"x\"].to(DEVICE, non_blocking=True)\n",
        "        y = batch[\"y\"].to(DEVICE, non_blocking=True)\n",
        "        u = batch[\"u\"].to(DEVICE, non_blocking=True)\n",
        "        theta = batch[\"theta\"].to(DEVICE, non_blocking=True)\n",
        "\n",
        "        z0, P0 = make_initial_prior(y[:, 0, :], toy=toy)\n",
        "        z_hat = ukn(y, u, z0, P0)\n",
        "\n",
        "        loss_ukn, _ = batch_loss(z_hat, x, theta, lambda_theta=LAMBDA_THETA, lambda_smooth=LAMBDA_SMOOTH)\n",
        "\n",
        "        dom = torch.tensor(0.0, device=DEVICE)\n",
        "        if USE_DOM_LOSS:\n",
        "            with torch.no_grad():\n",
        "                z_ukf = ukf(y, u, z0, P0)\n",
        "                x_ukf = z_ukf[..., 0:2]\n",
        "                th_ukf = z_ukf[..., 2:4]\n",
        "                th_true_seq = theta[:, None, :].expand_as(th_ukf)\n",
        "                loss_ukf = F.mse_loss(x_ukf, x) + LAMBDA_THETA * F.mse_loss(th_ukf, th_true_seq)\n",
        "            dom = F.relu(loss_ukn - loss_ukf + DOM_MARGIN)\n",
        "            loss = loss_ukn + LAMBDA_DOM * dom\n",
        "        else:\n",
        "            loss = loss_ukn\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(ukn.parameters(), GRAD_CLIP)\n",
        "        opt.step()\n",
        "\n",
        "        epoch_loss += float(loss_ukn.detach().cpu())\n",
        "        epoch_dom += float(dom.detach().cpu())\n",
        "        n_batches += 1\n",
        "\n",
        "    epoch_loss /= max(1, n_batches)\n",
        "    epoch_dom /= max(1, n_batches)\n",
        "\n",
        "    test_m = eval_mse(ukn, test_loader, toy=toy, device=DEVICE)\n",
        "\n",
        "    history[\"epoch\"].append(epoch)\n",
        "    history[\"train_loss\"].append(epoch_loss)\n",
        "    history[\"train_dom\"].append(epoch_dom)\n",
        "    history[\"test_mse_total\"].append(test_m[\"mse_total\"])\n",
        "    history[\"test_mse_x\"].append(test_m[\"mse_x\"])\n",
        "    history[\"test_mse_theta\"].append(test_m[\"mse_theta\"])\n",
        "\n",
        "    print(f\"Epoch {epoch:03d} | train={epoch_loss:.6f} dom={epoch_dom:.6f} | test={test_m['mse_total']:.6f}\")\n",
        "\n",
        "    if test_m[\"mse_total\"] < best:\n",
        "        best = test_m[\"mse_total\"]\n",
        "        best_state = {k: v.detach().cpu().clone() for k, v in ukn.state_dict().items()}\n",
        "\n",
        "print(\"Training time (s):\", time.time() - t0)\n",
        "print(\"Best test mse_total:\", best)\n",
        "\n",
        "torch.save(history, os.path.join(RESULTS_DIR, \"history.pt\"))\n",
        "torch.save({\"state_dict\": best_state, \"best\": best}, os.path.join(RESULTS_DIR, \"model_best.pt\"))\n",
        "torch.save({\"state_dict\": ukn.state_dict(), \"best\": best}, os.path.join(RESULTS_DIR, \"model_last.pt\"))\n",
        "print(\"Saved to:\", RESULTS_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Run full test predictions (UKN vs UKF) and save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "ckpt = torch.load(os.path.join(RESULTS_DIR, \"model_best.pt\"), map_location=\"cpu\")\n",
        "ukn.load_state_dict(ckpt[\"state_dict\"])\n",
        "ukn.to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z_hat_test = run_full_prediction(ukn, test_ds, toy=toy, device=DEVICE, batch_size=256)\n",
        "    z_ukf_test = run_full_prediction(ukf, test_ds, toy=toy, device=DEVICE, batch_size=256)\n",
        "\n",
        "out_blob = {\n",
        "    \"z_hat_test\": z_hat_test,\n",
        "    \"z_ukf_test\": z_ukf_test,\n",
        "    \"x_test\": test_ds.x,\n",
        "    \"y_test\": test_ds.y,\n",
        "    \"theta_test\": test_ds.theta,\n",
        "    \"u_test\": test_ds.u,\n",
        "    \"toy_config\": toy.__dict__,\n",
        "    \"best_mse_total\": float(ckpt[\"best\"]),\n",
        "}\n",
        "torch.save(out_blob, os.path.join(RESULTS_DIR, \"test_outputs.pt\"))\n",
        "print(\"Saved:\", os.path.join(RESULTS_DIR, \"test_outputs.pt\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Plot results\n",
        "- Training curve\n",
        "- Error + 99% band\n",
        "- One example trajectory overlay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "history = torch.load(os.path.join(RESULTS_DIR, \"history.pt\"), map_location=\"cpu\")\n",
        "blob = torch.load(os.path.join(RESULTS_DIR, \"test_outputs.pt\"), map_location=\"cpu\")\n",
        "\n",
        "z_hat = blob[\"z_hat_test\"]   # (N,T,4)\n",
        "z_ukf = blob[\"z_ukf_test\"]\n",
        "x = blob[\"x_test\"]           # (N,T,2)\n",
        "theta = blob[\"theta_test\"]   # (N,2)\n",
        "\n",
        "N, T, _ = z_hat.shape\n",
        "t_axis = np.arange(T)\n",
        "\n",
        "# Training curve\n",
        "plt.figure()\n",
        "plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"train loss (UKN)\")\n",
        "plt.plot(history[\"epoch\"], history[\"test_mse_total\"], label=\"test mse_total (UKN)\")\n",
        "plt.xlabel(\"epoch\"); plt.ylabel(\"loss / mse\"); plt.title(\"Training curve\")\n",
        "plt.legend(); plt.tight_layout()\n",
        "plt.savefig(os.path.join(RESULTS_DIR, \"training_curve.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "def mean_band(err: np.ndarray, q_low=0.005, q_high=0.995):\n",
        "    mean = np.mean(err, axis=0)\n",
        "    lo = np.quantile(err, q_low, axis=0)\n",
        "    hi = np.quantile(err, q_high, axis=0)\n",
        "    return mean, lo, hi\n",
        "\n",
        "# State error norm + 99% band\n",
        "err_x_ukn = (z_hat[..., 0:2] - x).numpy()\n",
        "err_x_ukf = (z_ukf[..., 0:2] - x).numpy()\n",
        "errnorm_ukn = np.linalg.norm(err_x_ukn, axis=-1)\n",
        "errnorm_ukf = np.linalg.norm(err_x_ukf, axis=-1)\n",
        "\n",
        "m_u, lo_u, hi_u = mean_band(errnorm_ukn)\n",
        "m_f, lo_f, hi_f = mean_band(errnorm_ukf)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(t_axis, m_u, label=\"UKN mean |err_x|\")\n",
        "plt.fill_between(t_axis, lo_u, hi_u, alpha=0.2, label=\"UKN 99% band\")\n",
        "plt.plot(t_axis, m_f, label=\"UKF mean |err_x|\")\n",
        "plt.fill_between(t_axis, lo_f, hi_f, alpha=0.2, label=\"UKF 99% band\")\n",
        "plt.xlabel(\"time\"); plt.ylabel(\"state error norm\"); plt.title(\"State error (99% band)\")\n",
        "plt.legend(); plt.tight_layout()\n",
        "plt.savefig(os.path.join(RESULTS_DIR, \"error_band_state.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Parameter error norm + 99% band\n",
        "th_hat = z_hat[..., 2:4].numpy()\n",
        "th_ukf = z_ukf[..., 2:4].numpy()\n",
        "th_true = theta.numpy()[:, None, :]\n",
        "\n",
        "err_th_ukn = np.linalg.norm(th_hat - th_true, axis=-1)\n",
        "err_th_ukf = np.linalg.norm(th_ukf - th_true, axis=-1)\n",
        "\n",
        "m_u, lo_u, hi_u = mean_band(err_th_ukn)\n",
        "m_f, lo_f, hi_f = mean_band(err_th_ukf)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(t_axis, m_u, label=\"UKN mean |err_theta|\")\n",
        "plt.fill_between(t_axis, lo_u, hi_u, alpha=0.2, label=\"UKN 99% band\")\n",
        "plt.plot(t_axis, m_f, label=\"UKF mean |err_theta|\")\n",
        "plt.fill_between(t_axis, lo_f, hi_f, alpha=0.2, label=\"UKF 99% band\")\n",
        "plt.xlabel(\"time\"); plt.ylabel(\"parameter error norm\"); plt.title(\"Parameter error (99% band)\")\n",
        "plt.legend(); plt.tight_layout()\n",
        "plt.savefig(os.path.join(RESULTS_DIR, \"error_band_theta.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# RMSE summary\n",
        "rmse_x_ukn = float(np.sqrt(np.mean(err_x_ukn**2)))\n",
        "rmse_x_ukf = float(np.sqrt(np.mean(err_x_ukf**2)))\n",
        "rmse_th_ukn = float(np.sqrt(np.mean((th_hat - th_true)**2)))\n",
        "rmse_th_ukf = float(np.sqrt(np.mean((th_ukf - th_true)**2)))\n",
        "print(f\"RMSE_x   UKN={rmse_x_ukn:.6f} | UKF={rmse_x_ukf:.6f}\")\n",
        "print(f\"RMSE_th  UKN={rmse_th_ukn:.6f} | UKF={rmse_th_ukf:.6f}\")\n",
        "\n",
        "with open(os.path.join(RESULTS_DIR, \"summary.txt\"), \"w\") as f:\n",
        "    f.write(f\"RMSE_x   UKN={rmse_x_ukn:.6f} | UKF={rmse_x_ukf:.6f}\\n\")\n",
        "    f.write(f\"RMSE_th  UKN={rmse_th_ukn:.6f} | UKF={rmse_th_ukf:.6f}\\n\")\n",
        "\n",
        "# Example trajectory\n",
        "EXAMPLE_IDX = 0\n",
        "zhat_i = z_hat[EXAMPLE_IDX].numpy()\n",
        "zukf_i = z_ukf[EXAMPLE_IDX].numpy()\n",
        "x_i = x[EXAMPLE_IDX].numpy()\n",
        "th_i = theta[EXAMPLE_IDX].numpy()\n",
        "\n",
        "# p\n",
        "plt.figure()\n",
        "plt.plot(t_axis, x_i[:,0], label=\"GT p\")\n",
        "plt.plot(t_axis, zhat_i[:,0], label=\"UKN p\")\n",
        "plt.plot(t_axis, zukf_i[:,0], label=\"UKF p\")\n",
        "plt.title(f\"Example {EXAMPLE_IDX}: position\"); plt.xlabel(\"time\"); plt.ylabel(\"p\")\n",
        "plt.legend(); plt.tight_layout()\n",
        "plt.savefig(os.path.join(RESULTS_DIR, f\"example_{EXAMPLE_IDX:04d}_p.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# v\n",
        "plt.figure()\n",
        "plt.plot(t_axis, x_i[:,1], label=\"GT v\")\n",
        "plt.plot(t_axis, zhat_i[:,1], label=\"UKN v\")\n",
        "plt.plot(t_axis, zukf_i[:,1], label=\"UKF v\")\n",
        "plt.title(f\"Example {EXAMPLE_IDX}: velocity\"); plt.xlabel(\"time\"); plt.ylabel(\"v\")\n",
        "plt.legend(); plt.tight_layout()\n",
        "plt.savefig(os.path.join(RESULTS_DIR, f\"example_{EXAMPLE_IDX:04d}_v.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# k\n",
        "plt.figure()\n",
        "plt.plot(t_axis, np.full_like(t_axis, th_i[0], dtype=float), label=\"GT k\")\n",
        "plt.plot(t_axis, zhat_i[:,2], label=\"UKN k\")\n",
        "plt.plot(t_axis, zukf_i[:,2], label=\"UKF k\")\n",
        "plt.title(f\"Example {EXAMPLE_IDX}: k\"); plt.xlabel(\"time\"); plt.ylabel(\"k\")\n",
        "plt.legend(); plt.tight_layout()\n",
        "plt.savefig(os.path.join(RESULTS_DIR, f\"example_{EXAMPLE_IDX:04d}_k.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# alpha\n",
        "plt.figure()\n",
        "plt.plot(t_axis, np.full_like(t_axis, th_i[1], dtype=float), label=\"GT alpha\")\n",
        "plt.plot(t_axis, zhat_i[:,3], label=\"UKN alpha\")\n",
        "plt.plot(t_axis, zukf_i[:,3], label=\"UKF alpha\")\n",
        "plt.title(f\"Example {EXAMPLE_IDX}: alpha\"); plt.xlabel(\"time\"); plt.ylabel(\"alpha\")\n",
        "plt.legend(); plt.tight_layout()\n",
        "plt.savefig(os.path.join(RESULTS_DIR, f\"example_{EXAMPLE_IDX:04d}_alpha.png\"), dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Plots saved under:\", RESULTS_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13) Reload-only plotting (optional)\n",
        "If you already trained and have a `RESULTS_DIR`, set it and re-run the plotting cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example:\n",
        "# RESULTS_DIR = \"results_ukn/run_YYYYMMDD_HHMMSS\"\n",
        "# Then re-run the plotting cell above.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
