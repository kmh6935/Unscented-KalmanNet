{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ffaf1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Adopted Problem: 1D Constant-Velocity ===\n",
      "device=cuda, dt=0.1, state=[pos, vel], measurement=pos only\n",
      "F=\n",
      " [[1.  0.1]\n",
      " [0.  1. ]]\n",
      "H=\n",
      " [[1. 0.]]\n",
      "diag(Q_true) = [0.01 0.01] (q_scale=0.1)\n",
      "diag(R_true) = [1.] (r_scale=1.0)\n",
      "Base cov: gamma_Q=10.0, gamma_R=5.0\n",
      "diag(Q_base) = [0.09999999 0.09999999]\n",
      "diag(R_base) = [5.]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 655\u001b[39m\n\u001b[32m    651\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     \u001b[43mtrain_demo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 551\u001b[39m, in \u001b[36mtrain_demo\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    548\u001b[39m P0b = P0.expand(B, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    550\u001b[39m opt.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m xhat, P_hist, _, _ = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43myb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP0b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQ_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mR_base\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m loss_state = mse(xhat, xb)\n\u001b[32m    554\u001b[39m e = xb - xhat\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\minhy\\anaconda3\\envs\\ukn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\minhy\\anaconda3\\envs\\ukn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 327\u001b[39m, in \u001b[36mUKNet_ScalarQR.forward\u001b[39m\u001b[34m(self, y, f_fn, h_fn, x0, P0, Q_base, R_base)\u001b[39m\n\u001b[32m    325\u001b[39m x_pred0, P_pred0 = unscented_mean_cov(X_pred, Wm, Wc, noise=Q_feat)\n\u001b[32m    326\u001b[39m Y_pred0 = h_fn(X_pred)\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m y_pred0, S0 = \u001b[43munscented_mean_cov\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_pred0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m=\u001b[49m\u001b[43mR_feat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m e0 = y[:, t, :] - y_pred0\n\u001b[32m    330\u001b[39m de = e0 - e_prev\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 192\u001b[39m, in \u001b[36munscented_mean_cov\u001b[39m\u001b[34m(X, Wm, Wc, noise, eps)\u001b[39m\n\u001b[32m    189\u001b[39m     cov = cov + (noise.unsqueeze(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m noise.dim() == \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m noise)\n\u001b[32m    191\u001b[39m cov = symmetrize(cov)\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m cov = \u001b[43mensure_spd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mean, cov\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mensure_spd_batch\u001b[39m\u001b[34m(P, eps, max_shift)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03mPer-batch diagonal shift so min eigenvalue >= eps.\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03mAlso sanitizes NaN/Inf.\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     54\u001b[39m P = symmetrize(P)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m)\u001b[49m.all():\n\u001b[32m     57\u001b[39m     P = torch.nan_to_num(P, nan=\u001b[32m0.0\u001b[39m, posinf=\u001b[32m0.0\u001b[39m, neginf=\u001b[32m0.0\u001b[39m)\n\u001b[32m     58\u001b[39m     P = symmetrize(P)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "UKNet (UKF + learned scalar scales for Q/R) on 1D Constant-Velocity (CV) model.\n",
    "\n",
    "State: x_t = [position, velocity]^T (n=2)\n",
    "Meas : y_t = position              (m=1)\n",
    "\n",
    "Data gen uses Q_true, R_true (unknown to us).\n",
    "Filter/training uses base covariances Q_base, R_base (typically larger),\n",
    "and learns per-time-step scalar scales:\n",
    "    Q_t = exp(sq_t) * Q_base\n",
    "    R_t = exp(sr_t) * R_base\n",
    "\n",
    "Training loss:\n",
    "- state MSE\n",
    "- optional covariance calibration via state NLL from P (Mahalanobis + logdet)\n",
    "\n",
    "Final plots:\n",
    "- Loss history\n",
    "- RMSE over time (UKNet vs baseline UKF(Q_base,R_base))\n",
    "- Sample trajectory (pos/vel)\n",
    "- LAST PLOT: error per state with 99% CI band from P (Â± z * sqrt(P_ii)), z=2.5758\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Repro / device\n",
    "# -------------------------\n",
    "def set_seed(seed: int = 0):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# SPD / numerics (autograd-safe)\n",
    "# -------------------------\n",
    "def symmetrize(P: torch.Tensor) -> torch.Tensor:\n",
    "    return 0.5 * (P + P.transpose(-1, -2))\n",
    "\n",
    "\n",
    "def ensure_spd_batch(P: torch.Tensor, eps: float = 1e-6, max_shift: float = 1e6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Per-batch diagonal shift so min eigenvalue >= eps.\n",
    "    Also sanitizes NaN/Inf.\n",
    "    \"\"\"\n",
    "    P = symmetrize(P)\n",
    "\n",
    "    if not torch.isfinite(P).all():\n",
    "        P = torch.nan_to_num(P, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        P = symmetrize(P)\n",
    "\n",
    "    B, n, _ = P.shape\n",
    "    I = torch.eye(n, device=P.device, dtype=P.dtype).unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        eigmin = torch.linalg.eigvalsh(P).min(dim=-1).values  # [B]\n",
    "        shift = torch.clamp(eps - eigmin, min=0.0, max=max_shift)  # [B]\n",
    "\n",
    "    return P + shift.view(B, 1, 1) * I\n",
    "\n",
    "\n",
    "def robust_cholesky(P: torch.Tensor, eps: float = 1e-6, tries: int = 7) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Robust batch Cholesky: retries with increasing eps.\n",
    "    No cholesky_ex, no in-place patching -> safe for autograd.\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    for k in range(tries):\n",
    "        P2 = ensure_spd_batch(P, eps=eps * (10.0 ** k))\n",
    "        P2 = symmetrize(P2)\n",
    "        try:\n",
    "            return torch.linalg.cholesky(P2)\n",
    "        except RuntimeError as e:\n",
    "            last_err = e\n",
    "    raise last_err\n",
    "\n",
    "\n",
    "def l2_normalize(x: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    n = torch.norm(x, dim=-1, keepdim=True)\n",
    "    return x / (n + eps)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Data generation (linear CV)\n",
    "# -------------------------\n",
    "def sample_linear_sequences(\n",
    "    num_seq: int,\n",
    "    T: int,\n",
    "    F: torch.Tensor,\n",
    "    H: torch.Tensor,\n",
    "    Q: torch.Tensor,\n",
    "    R: torch.Tensor,\n",
    "    x0_cov: float = 1.0,\n",
    "    device: str = \"cpu\",\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    x_{t+1} = F x_t + w_t, w_t ~ N(0,Q)\n",
    "    y_t     = H x_t + v_t, v_t ~ N(0,R)\n",
    "    returns x:[N,T,n], y:[N,T,m]\n",
    "    \"\"\"\n",
    "    n = F.shape[0]\n",
    "    m = H.shape[0]\n",
    "\n",
    "    LQ = torch.linalg.cholesky(Q)\n",
    "    LR = torch.linalg.cholesky(R)\n",
    "\n",
    "    x = torch.zeros(num_seq, T, n, device=device)\n",
    "    y = torch.zeros(num_seq, T, m, device=device)\n",
    "\n",
    "    x[:, 0, :] = torch.randn(num_seq, n, device=device) * math.sqrt(x0_cov)\n",
    "\n",
    "    for t in range(T):\n",
    "        v = torch.randn(num_seq, m, device=device) @ LR.T\n",
    "        y[:, t, :] = x[:, t, :] @ H.T + v\n",
    "\n",
    "        if t < T - 1:\n",
    "            w = torch.randn(num_seq, n, device=device) @ LQ.T\n",
    "            x[:, t + 1, :] = x[:, t, :] @ F.T + w\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "class SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Unscented Transform\n",
    "# -------------------------\n",
    "def sigma_points(x, P, alpha=0.1, beta=2.0, kappa=0.0):\n",
    "    \"\"\"\n",
    "    x: [B,n], P:[B,n,n]\n",
    "    returns Xi:[B,2n+1,n], Wm,Wc:[2n+1]\n",
    "    \"\"\"\n",
    "    B, n = x.shape\n",
    "    lam = alpha**2 * (n + kappa) - n\n",
    "    c = n + lam\n",
    "    gamma = math.sqrt(c)\n",
    "\n",
    "    Wm = x.new_zeros(2 * n + 1)\n",
    "    Wc = x.new_zeros(2 * n + 1)\n",
    "    Wm[0] = lam / c\n",
    "    Wc[0] = lam / c + (1 - alpha**2 + beta)\n",
    "    Wm[1:] = 1.0 / (2 * c)\n",
    "    Wc[1:] = 1.0 / (2 * c)\n",
    "\n",
    "    P = ensure_spd_batch(P, eps=1e-6)\n",
    "    S = robust_cholesky(P, eps=1e-6)  # [B,n,n]\n",
    "\n",
    "    S_scaled = gamma * S\n",
    "    U = S_scaled.transpose(1, 2)  # [B,n,n]\n",
    "    x0 = x.unsqueeze(1)           # [B,1,n]\n",
    "    Xi = torch.cat([x0, x0 + U, x0 - U], dim=1)  # [B,2n+1,n]\n",
    "    return Xi, Wm, Wc\n",
    "\n",
    "\n",
    "def unscented_mean_cov(X, Wm, Wc, noise=None, eps=1e-6):\n",
    "    \"\"\"\n",
    "    X: [B,L,d]\n",
    "    return mean:[B,d], cov:[B,d,d]\n",
    "    \"\"\"\n",
    "    B, L, d = X.shape\n",
    "    mean = torch.sum(Wm.view(1, L, 1) * X, dim=1)\n",
    "\n",
    "    Xm = X - mean.unsqueeze(1)\n",
    "    cov = torch.zeros(B, d, d, device=X.device, dtype=X.dtype)\n",
    "    for i in range(L):\n",
    "        wi = Wc[i]\n",
    "        vi = Xm[:, i, :].unsqueeze(-1)\n",
    "        cov = cov + wi * (vi @ vi.transpose(-1, -2))\n",
    "\n",
    "    if noise is not None:\n",
    "        cov = cov + (noise.unsqueeze(0) if noise.dim() == 2 else noise)\n",
    "\n",
    "    cov = symmetrize(cov)\n",
    "    cov = ensure_spd_batch(cov, eps=eps)\n",
    "    return mean, cov\n",
    "\n",
    "\n",
    "def cross_cov(X, Y, x_mean, y_mean, Wc):\n",
    "    \"\"\"\n",
    "    X:[B,L,n], Y:[B,L,m] -> Pxy:[B,n,m]\n",
    "    \"\"\"\n",
    "    B, L, n = X.shape\n",
    "    m = Y.shape[-1]\n",
    "    Xc = X - x_mean.unsqueeze(1)\n",
    "    Yc = Y - y_mean.unsqueeze(1)\n",
    "    Pxy = torch.zeros(B, n, m, device=X.device, dtype=X.dtype)\n",
    "    for i in range(L):\n",
    "        wi = Wc[i]\n",
    "        xi = Xc[:, i, :].unsqueeze(-1)   # [B,n,1]\n",
    "        yi = Yc[:, i, :].unsqueeze(-1)   # [B,m,1]\n",
    "        Pxy = Pxy + wi * (xi @ yi.transpose(-1, -2))\n",
    "    return Pxy\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Baseline UKF (fixed Q_base, R_base)\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def batch_ukf_filter(\n",
    "    y: torch.Tensor,          # [B,T,m]\n",
    "    f_fn,\n",
    "    h_fn,\n",
    "    Q: torch.Tensor,          # [n,n]\n",
    "    R: torch.Tensor,          # [m,m]\n",
    "    x0: torch.Tensor,         # [B,n]\n",
    "    P0: torch.Tensor,         # [B,n,n]\n",
    "    ut_params=(0.1, 2.0, 0.0),\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    alpha, beta, kappa = ut_params\n",
    "    B, T, _ = y.shape\n",
    "\n",
    "    x = x0\n",
    "    P = ensure_spd_batch(P0, eps=1e-6)\n",
    "\n",
    "    x_list, P_list = [], []\n",
    "    for t in range(T):\n",
    "        P = ensure_spd_batch(P, eps=1e-6)\n",
    "        Xi, Wm, Wc = sigma_points(x, P, alpha, beta, kappa)\n",
    "\n",
    "        X_pred = f_fn(Xi)\n",
    "        x_pred, P_pred = unscented_mean_cov(X_pred, Wm, Wc, noise=Q)\n",
    "\n",
    "        Y_pred = h_fn(X_pred)\n",
    "        y_pred, S = unscented_mean_cov(Y_pred, Wm, Wc, noise=R)\n",
    "\n",
    "        Pxy = cross_cov(X_pred, Y_pred, x_pred, y_pred, Wc)\n",
    "\n",
    "        S = ensure_spd_batch(S, eps=1e-6)\n",
    "        Ls = robust_cholesky(S, eps=1e-6)\n",
    "        KT = torch.cholesky_solve(Pxy.transpose(1, 2), Ls)  # [B,m,n]\n",
    "        K = KT.transpose(1, 2)  # [B,n,m]\n",
    "\n",
    "        e = y[:, t, :] - y_pred\n",
    "        x = x_pred + torch.bmm(K, e.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        P = P_pred - torch.bmm(torch.bmm(K, S), K.transpose(-1, -2))\n",
    "        P = symmetrize(P)\n",
    "        P = ensure_spd_batch(P, eps=1e-6)\n",
    "\n",
    "        x_list.append(x)\n",
    "        P_list.append(P)\n",
    "\n",
    "    return torch.stack(x_list, dim=1), torch.stack(P_list, dim=1)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# UKNet: learn scalar scales for Q/R\n",
    "# -------------------------\n",
    "class UKNet_ScalarQR(nn.Module):\n",
    "    \"\"\"\n",
    "    UKF structure + learn per-step scalar scales sq_t, sr_t:\n",
    "        Q_t = exp(sq_t) * Q_base\n",
    "        R_t = exp(sr_t) * R_base\n",
    "    \"\"\"\n",
    "    def __init__(self, n: int, m: int, hidden_size: int = 64, ut_params=(0.1, 2.0, 0.0),\n",
    "                 log_scale_clip: float = 8.0):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.alpha, self.beta, self.kappa = ut_params\n",
    "        self.log_scale_clip = log_scale_clip\n",
    "\n",
    "        # features: e(m), de(m), dy(m), diagP(n), diagS(m) -> 4m + n\n",
    "        in_dim = 4 * m + n\n",
    "        self.gru = nn.GRUCell(in_dim, hidden_size)\n",
    "\n",
    "        self.fc_sq = nn.Linear(hidden_size, 1)  # log-scale for Q\n",
    "        self.fc_sr = nn.Linear(hidden_size, 1)  # log-scale for R\n",
    "\n",
    "        nn.init.zeros_(self.fc_sq.weight); nn.init.zeros_(self.fc_sq.bias)\n",
    "        nn.init.zeros_(self.fc_sr.weight); nn.init.zeros_(self.fc_sr.bias)\n",
    "\n",
    "    def forward(self, y, f_fn, h_fn, x0, P0, Q_base: torch.Tensor, R_base: torch.Tensor):\n",
    "        \"\"\"\n",
    "        y:[B,T,m], x0:[B,n], P0:[B,n,n]\n",
    "        Q_base:[n,n], R_base:[m,m]\n",
    "        returns xhat:[B,T,n], P_hist:[B,T,n,n], sq_hist:[B,T,1], sr_hist:[B,T,1]\n",
    "        \"\"\"\n",
    "        device = y.device\n",
    "        B, T, m = y.shape\n",
    "        n = self.n\n",
    "\n",
    "        x = x0\n",
    "        P = ensure_spd_batch(P0, eps=1e-6)\n",
    "        h = torch.zeros(B, self.gru.hidden_size, device=device, dtype=y.dtype)\n",
    "\n",
    "        e_prev = torch.zeros(B, m, device=device, dtype=y.dtype)\n",
    "        y_prev = torch.zeros(B, m, device=device, dtype=y.dtype)\n",
    "\n",
    "        x_list, P_list = [], []\n",
    "        sq_list, sr_list = [], []\n",
    "\n",
    "        # tiny covariances for feature extraction stability (optional)\n",
    "        Q_feat = torch.eye(n, device=device, dtype=y.dtype) * 1e-6\n",
    "        R_feat = torch.eye(m, device=device, dtype=y.dtype) * 1e-6\n",
    "\n",
    "        for t in range(T):\n",
    "            P = ensure_spd_batch(P, eps=1e-6)\n",
    "\n",
    "            # sigma points from posterior\n",
    "            Xi, Wm, Wc = sigma_points(x, P, self.alpha, self.beta, self.kappa)\n",
    "\n",
    "            # propagate through dynamics\n",
    "            X_pred = f_fn(Xi)\n",
    "\n",
    "            # provisional stats for features\n",
    "            x_pred0, P_pred0 = unscented_mean_cov(X_pred, Wm, Wc, noise=Q_feat)\n",
    "            Y_pred0 = h_fn(X_pred)\n",
    "            y_pred0, S0 = unscented_mean_cov(Y_pred0, Wm, Wc, noise=R_feat)\n",
    "\n",
    "            e0 = y[:, t, :] - y_pred0\n",
    "            de = e0 - e_prev\n",
    "            dy = y[:, t, :] - y_prev\n",
    "\n",
    "            diagP = torch.diagonal(P_pred0, dim1=-2, dim2=-1)  # [B,n]\n",
    "            diagS = torch.diagonal(S0, dim1=-2, dim2=-1)       # [B,m]\n",
    "\n",
    "            z = torch.cat([\n",
    "                l2_normalize(e0),\n",
    "                l2_normalize(de),\n",
    "                l2_normalize(dy),\n",
    "                l2_normalize(diagP.detach()),\n",
    "                l2_normalize(diagS.detach()),\n",
    "            ], dim=-1)\n",
    "\n",
    "            h = self.gru(z, h)\n",
    "\n",
    "            # log-scales (clipped)\n",
    "            sq = torch.clamp(self.fc_sq(h), -self.log_scale_clip, self.log_scale_clip)  # [B,1]\n",
    "            sr = torch.clamp(self.fc_sr(h), -self.log_scale_clip, self.log_scale_clip)  # [B,1]\n",
    "            q_scale = torch.exp(sq)  # [B,1]\n",
    "            r_scale = torch.exp(sr)  # [B,1]\n",
    "\n",
    "            # build Q_t, R_t (batch)\n",
    "            Q_t = q_scale.view(B, 1, 1) * Q_base.unsqueeze(0)  # [B,n,n]\n",
    "            R_t = r_scale.view(B, 1, 1) * R_base.unsqueeze(0)  # [B,m,m]\n",
    "            Q_t = ensure_spd_batch(Q_t, eps=1e-6)\n",
    "            R_t = ensure_spd_batch(R_t, eps=1e-6)\n",
    "\n",
    "            # --- UKF step with Q_t, R_t ---\n",
    "            x_pred, P_pred = unscented_mean_cov(X_pred, Wm, Wc, noise=Q_t)\n",
    "\n",
    "            Y_pred = h_fn(X_pred)\n",
    "            y_pred, S = unscented_mean_cov(Y_pred, Wm, Wc, noise=R_t)\n",
    "\n",
    "            Pxy = cross_cov(X_pred, Y_pred, x_pred, y_pred, Wc)\n",
    "\n",
    "            S = ensure_spd_batch(S, eps=1e-6)\n",
    "            Ls = robust_cholesky(S, eps=1e-6)\n",
    "            KT = torch.cholesky_solve(Pxy.transpose(1, 2), Ls)  # [B,m,n]\n",
    "            K = KT.transpose(1, 2)  # [B,n,m]\n",
    "\n",
    "            e = y[:, t, :] - y_pred\n",
    "            x = x_pred + torch.bmm(K, e.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "            P = P_pred - torch.bmm(torch.bmm(K, S), K.transpose(-1, -2))\n",
    "            P = symmetrize(P)\n",
    "            P = ensure_spd_batch(P, eps=1e-6)\n",
    "\n",
    "            x_list.append(x)\n",
    "            P_list.append(P)\n",
    "            sq_list.append(sq)\n",
    "            sr_list.append(sr)\n",
    "\n",
    "            e_prev = e.detach()\n",
    "            y_prev = y[:, t, :].detach()\n",
    "\n",
    "        return (\n",
    "            torch.stack(x_list, dim=1),\n",
    "            torch.stack(P_list, dim=1),\n",
    "            torch.stack(sq_list, dim=1),\n",
    "            torch.stack(sr_list, dim=1),\n",
    "        )\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Loss: state NLL from P\n",
    "# -------------------------\n",
    "def state_nll_from_P(e: torch.Tensor, P: torch.Tensor, eps: float = 1e-9) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    e:[B,T,n], P:[B,T,n,n]\n",
    "    NLL = 0.5*(e^T P^{-1} e + logdet(P))\n",
    "    \"\"\"\n",
    "    B, T, n = e.shape\n",
    "    e2 = e.reshape(B * T, n, 1)\n",
    "    P2 = P.reshape(B * T, n, n)\n",
    "\n",
    "    P2 = ensure_spd_batch(P2, eps=1e-6)\n",
    "    L = robust_cholesky(P2, eps=1e-6)\n",
    "\n",
    "    sol = torch.cholesky_solve(e2, L)  # [B*T,n,1]\n",
    "    maha = (e2.transpose(1, 2) @ sol).reshape(B, T)  # [B,T]\n",
    "\n",
    "    diag = torch.diagonal(L, dim1=-2, dim2=-1)\n",
    "    logdet = 2.0 * torch.sum(torch.log(diag + eps), dim=-1)  # [B*T]\n",
    "    logdet = logdet.reshape(B, T)\n",
    "\n",
    "    return 0.5 * (maha + logdet).mean()\n",
    "\n",
    "\n",
    "def rmse_over_time(xhat: torch.Tensor, x_true: torch.Tensor) -> torch.Tensor:\n",
    "    mse_t = ((xhat - x_true) ** 2).mean(dim=(0, 2))\n",
    "    return torch.sqrt(mse_t + 1e-12)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Plots\n",
    "# -------------------------\n",
    "def plot_loss_history(train_hist: List[float], test_hist: List[float], title: str):\n",
    "    epochs = np.arange(1, len(train_hist) + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_hist, label=\"Train (total)\")\n",
    "    plt.plot(epochs, test_hist, label=\"Test (state MSE)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss / MSE\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_error_with_ci_99(x_true_1, x_hat_1, P_1, title_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Plot per-state error with 99% CI band from covariance.\n",
    "    x_true_1: [T,n], x_hat_1: [T,n], P_1: [T,n,n]\n",
    "    \"\"\"\n",
    "    z99 = 2.5758293035489004  # two-sided 99% (0.995 quantile)\n",
    "    T, n = x_true_1.shape\n",
    "    t = np.arange(T)\n",
    "\n",
    "    err = (x_true_1 - x_hat_1)  # [T,n]\n",
    "    var = np.stack([np.diag(P_1[k]) for k in range(T)], axis=0)  # [T,n]\n",
    "    std = np.sqrt(np.maximum(var, 1e-12))\n",
    "    band = z99 * std\n",
    "\n",
    "    names = [\"position\", \"velocity\"]\n",
    "    for i in range(n):\n",
    "        plt.figure()\n",
    "        plt.plot(t, err[:, i], label=f\"error ({names[i]})\")\n",
    "        plt.fill_between(t, -band[:, i], band[:, i], alpha=0.2, label=\"99% CI from P\")\n",
    "        plt.axhline(0.0, linewidth=1)\n",
    "        plt.xlabel(\"Time step\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.title(f\"{title_prefix} Error + 99% CI ({names[i]})\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Train + compare\n",
    "# -------------------------\n",
    "def train_demo():\n",
    "    set_seed(0)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.float32\n",
    "\n",
    "    # ----- Problem setup -----\n",
    "    dt = 0.1\n",
    "    F = torch.tensor([[1.0, dt],\n",
    "                      [0.0, 1.0]], device=device, dtype=dtype)\n",
    "    H = torch.tensor([[1.0, 0.0]], device=device, dtype=dtype)\n",
    "\n",
    "    # Ground-truth noise (unknown to us)\n",
    "    q_scale = 0.1\n",
    "    r_scale = 1.0\n",
    "    Q_true = torch.eye(2, device=device, dtype=dtype) * (q_scale ** 2)\n",
    "    R_true = torch.eye(1, device=device, dtype=dtype) * (r_scale ** 2)\n",
    "\n",
    "    # Assumed base covariances (can be larger)\n",
    "    gamma_Q = 10.0\n",
    "    gamma_R = 5.0\n",
    "    Q_base = gamma_Q * Q_true\n",
    "    R_base = gamma_R * R_true\n",
    "\n",
    "    ut_params = (0.1, 2.0, 0.0)\n",
    "\n",
    "    print(\"=== Adopted Problem: 1D Constant-Velocity ===\")\n",
    "    print(f\"device={device}, dt={dt}, state=[pos, vel], measurement=pos only\")\n",
    "    print(\"F=\\n\", F.detach().cpu().numpy())\n",
    "    print(\"H=\\n\", H.detach().cpu().numpy())\n",
    "    print(\"diag(Q_true) =\", torch.diag(Q_true).detach().cpu().numpy(), f\"(q_scale={q_scale})\")\n",
    "    print(\"diag(R_true) =\", torch.diag(R_true).detach().cpu().numpy(), f\"(r_scale={r_scale})\")\n",
    "    print(f\"Base cov: gamma_Q={gamma_Q}, gamma_R={gamma_R}\")\n",
    "    print(\"diag(Q_base) =\", torch.diag(Q_base).detach().cpu().numpy())\n",
    "    print(\"diag(R_base) =\", torch.diag(R_base).detach().cpu().numpy())\n",
    "    print()\n",
    "\n",
    "    # f/h for sigma points\n",
    "    def f_fn(X):  # X:[B,L,2]\n",
    "        return X @ F.T\n",
    "\n",
    "    def h_fn(X):  # X:[B,L,2] -> [B,L,1]\n",
    "        return X @ H.T\n",
    "\n",
    "    # ----- Data -----\n",
    "    T = 60\n",
    "    N_train, N_test = 4000, 800\n",
    "    x_train, y_train = sample_linear_sequences(N_train, T, F, H, Q_true, R_true, device=device)\n",
    "    x_test, y_test = sample_linear_sequences(N_test, T, F, H, Q_true, R_true, device=device)\n",
    "\n",
    "    train_loader = DataLoader(SeqDataset(x_train, y_train), batch_size=128, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(SeqDataset(x_test, y_test), batch_size=256, shuffle=False, num_workers=0)\n",
    "\n",
    "    # ----- Init -----\n",
    "    x0 = torch.zeros(1, 2, device=device, dtype=dtype)\n",
    "    P0 = torch.eye(2, device=device, dtype=dtype).unsqueeze(0) * 1.0\n",
    "\n",
    "    # ----- Model -----\n",
    "    model = UKNet_ScalarQR(n=2, m=1, hidden_size=64, ut_params=ut_params, log_scale_clip=8.0).to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-6)\n",
    "    mse = nn.MSELoss()\n",
    "\n",
    "    beta_cov = 0.10  # weight for covariance NLL\n",
    "    num_epochs = 25\n",
    "\n",
    "    train_hist_total: List[float] = []\n",
    "    test_hist_mse: List[float] = []\n",
    "\n",
    "    for ep in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        tot_loss = 0.0\n",
    "        tot_mse = 0.0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            B = xb.shape[0]\n",
    "            x0b = x0.expand(B, -1).contiguous()\n",
    "            P0b = P0.expand(B, -1, -1).contiguous()\n",
    "\n",
    "            opt.zero_grad()\n",
    "            xhat, P_hist, _, _ = model(yb, f_fn, h_fn, x0b, P0b, Q_base=Q_base, R_base=R_base)\n",
    "\n",
    "            loss_state = mse(xhat, xb)\n",
    "            e = xb - xhat\n",
    "            loss_cov = state_nll_from_P(e, P_hist)\n",
    "\n",
    "            loss = (1.0 - beta_cov) * loss_state + beta_cov * loss_cov\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            tot_loss += loss.item() * B\n",
    "            tot_mse += loss_state.item() * B\n",
    "\n",
    "        train_total = tot_loss / len(train_loader.dataset)\n",
    "        train_mse = tot_mse / len(train_loader.dataset)\n",
    "\n",
    "        # test MSE\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot = 0.0\n",
    "            for xb, yb in test_loader:\n",
    "                B = xb.shape[0]\n",
    "                x0b = x0.expand(B, -1).contiguous()\n",
    "                P0b = P0.expand(B, -1, -1).contiguous()\n",
    "                xhat, _, _, _ = model(yb, f_fn, h_fn, x0b, P0b, Q_base=Q_base, R_base=R_base)\n",
    "                tot += mse(xhat, xb).item() * B\n",
    "            test_mse = tot / len(test_loader.dataset)\n",
    "\n",
    "        train_hist_total.append(train_total)\n",
    "        test_hist_mse.append(test_mse)\n",
    "\n",
    "        print(f\"Epoch {ep:02d} | Train total: {train_total:.6f} (MSE={train_mse:.6f}) | Test MSE: {test_mse:.6f}\")\n",
    "\n",
    "    # ----- Loss plot -----\n",
    "    plot_loss_history(train_hist_total, test_hist_mse, title=\"UKNet Scalar-Scale(Q/R) - Loss History\")\n",
    "\n",
    "    # ----- Final comparison (UKNet vs baseline UKF with Q_base/R_base) -----\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # UKNet on full test set\n",
    "        B = y_test.shape[0]\n",
    "        x0b = x0.expand(B, -1).contiguous()\n",
    "        P0b = P0.expand(B, -1, -1).contiguous()\n",
    "        xhat_kn, P_hist_kn, sq_hist, sr_hist = model(y_test, f_fn, h_fn, x0b, P0b, Q_base=Q_base, R_base=R_base)\n",
    "        kn_mse = torch.mean((xhat_kn - x_test) ** 2).item()\n",
    "\n",
    "        # baseline UKF\n",
    "        xhat_ukf, P_hist_ukf = batch_ukf_filter(\n",
    "            y=y_test, f_fn=f_fn, h_fn=h_fn, Q=Q_base, R=R_base, x0=x0b, P0=P0b, ut_params=ut_params\n",
    "        )\n",
    "        ukf_mse = torch.mean((xhat_ukf - x_test) ** 2).item()\n",
    "\n",
    "        print(\"\\n===== Final (Test Set) =====\")\n",
    "        print(f\"UKNet MSE: {kn_mse:.6e}\")\n",
    "        print(f\"UKF  MSE: {ukf_mse:.6e}\")\n",
    "\n",
    "        # RMSE(t)\n",
    "        rmse_kn = rmse_over_time(xhat_kn, x_test).detach().cpu().numpy()\n",
    "        rmse_ukf = rmse_over_time(xhat_ukf, x_test).detach().cpu().numpy()\n",
    "        t = np.arange(len(rmse_kn))\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(t, rmse_kn, label=\"UKNet RMSE(t)\")\n",
    "        plt.plot(t, rmse_ukf, label=\"UKF RMSE(t)\")\n",
    "        plt.xlabel(\"Time step\")\n",
    "        plt.ylabel(\"RMSE\")\n",
    "        plt.title(\"RMSE over time (test avg)\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "        # sample trajectory\n",
    "        sample_idx = 0\n",
    "        x_true_1 = x_test[sample_idx].detach().cpu().numpy()      # [T,2]\n",
    "        x_kn_1 = xhat_kn[sample_idx].detach().cpu().numpy()       # [T,2]\n",
    "        x_ukf_1 = xhat_ukf[sample_idx].detach().cpu().numpy()     # [T,2]\n",
    "\n",
    "        names = [\"position\", \"velocity\"]\n",
    "        for i in range(2):\n",
    "            plt.figure()\n",
    "            plt.plot(t, x_true_1[:, i], label=f\"True {names[i]}\")\n",
    "            plt.plot(t, x_kn_1[:, i], label=f\"UKNet {names[i]}\")\n",
    "            plt.plot(t, x_ukf_1[:, i], label=f\"UKF  {names[i]}\")\n",
    "            plt.xlabel(\"Time step\")\n",
    "            plt.ylabel(names[i])\n",
    "            plt.title(f\"Sample traj (idx={sample_idx}) - {names[i]}\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "            plt.show()\n",
    "\n",
    "        # ---- LAST PLOT: error + 99% CI from covariance ----\n",
    "        P_kn_1 = P_hist_kn[sample_idx].detach().cpu().numpy()  # [T,2,2]\n",
    "        plot_error_with_ci_99(\n",
    "            x_true_1=x_true_1,\n",
    "            x_hat_1=x_kn_1,\n",
    "            P_1=P_kn_1,\n",
    "            title_prefix=\"UKNet\"\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_demo()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
