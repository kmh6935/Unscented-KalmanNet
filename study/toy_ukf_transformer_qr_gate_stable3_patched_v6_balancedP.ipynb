{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b567c529",
      "metadata": {},
      "source": [
        "# Toy UKF + Causal Window Transformer NoiseNet (Full Cholesky Qx/R + pₖ→Qθ Gating)\n",
        "\n",
        "This notebook implements a **minimal end-to-end toy** for your setup:\n",
        "\n",
        "- **2-DOF** dynamics with state \\(x=[q_1,q_2,v_1,v_2]\\) (nx=4) and measurement \\(y=[q_1,q_2]\\) (ny=2)  \n",
        "- **Augmented state** \\(z=[x;\\theta]\\) with a **single parameter** \\(\\theta\\) scaling stiffness  \n",
        "- Sequence length **T=2000**, random change-point \\(k^*\\), and **+30% step jump** in \\(\\theta\\)  \n",
        "- A **causal Transformer** predicts **full SPD** \\(Q_{x,k}\\) and \\(R_k\\) via **Cholesky factors**, plus a change logit \\(p_k\\)  \n",
        "- \\(p_k\\) gates \\(Q_{\\theta,k}\\) inside the UKF predict covariance:\n",
        "  \\[\n",
        "  Q_{\\theta,k}=(1-\\sigma(p_k))Q_{\\theta}^{\\text{base}}+\\sigma(p_k)Q_{\\theta}^{\\text{jump}}\n",
        "  \\]\n",
        "- Training uses **sliding windows** with **burn-in**, **state supervision**, and **BCE supervision** for change detection.\n",
        "\n",
        "> Tip: Start with the default small training settings to sanity-check. Then increase epochs/steps to improve adaptation speed.\n",
        "\n",
        "\n",
        "**Training note:** During training we **do not supervise θ** (no θ loss). We only use state GT (and likelihood terms) for learning. In synthetic experiments, θ GT is used **only for evaluation / reporting**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "aa0c772c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch: 2.10.0+cu128\n",
            "cuda available: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Setup ---\n",
        "from __future__ import annotations\n",
        "import math, random\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7ccccf14",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CFG(device='cuda', dtype=torch.float32, nx=4, ny=2, nt=1, nz=5, dt=0.02, T=2000, jump_ratio=0.3, kstar_lo=0.3, kstar_hi=0.7, alpha=1.0, beta=2.0, kappa=0.0, jitter_P=0.001, jitter_S=0.0001, W=128, d_model=128, n_layers=2, n_heads=4, dropout=0.1, burn_in=128, L=256, batch_size=8, n_train_seq=128, n_val_seq=32, n_test_seq=32, lr=0.0003, epochs=3, steps_per_epoch=100, w_state=1.0, w_nll_y=0.05, w_nll_x=0.05, w_bce=0.2, w_smooth=0.0001, w_offdiag=0.0001, Qtheta_base=1e-08, Qtheta_jump=0.0001, p_label_width=25)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Config ---\n",
        "@dataclass\n",
        "class CFG:\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    dtype: torch.dtype = torch.float32\n",
        "\n",
        "    # problem dims\n",
        "    nx: int = 4\n",
        "    ny: int = 2\n",
        "    nt: int = 1\n",
        "    nz: int = 5\n",
        "\n",
        "    # simulation\n",
        "    dt: float = 0.02\n",
        "    T: int = 2000\n",
        "    jump_ratio: float = 0.30\n",
        "    kstar_lo: float = 0.3\n",
        "    kstar_hi: float = 0.7\n",
        "\n",
        "    # UKF params\n",
        "    alpha: float = 1.0\n",
        "    beta: float = 2.0\n",
        "    kappa: float = 0.0\n",
        "    jitter_P: float = 1e-3\n",
        "    jitter_S: float = 1e-4\n",
        "\n",
        "    # NoiseNet / Transformer\n",
        "    W: int = 128              # sliding window length for features (past-only)\n",
        "    d_model: int = 128\n",
        "    n_layers: int = 2\n",
        "    n_heads: int = 4\n",
        "    dropout: float = 0.1\n",
        "\n",
        "    # Training windows (burn-in + loss)\n",
        "    burn_in: int = 128\n",
        "    L: int = 256\n",
        "\n",
        "    # Dataset sizes\n",
        "    batch_size: int = 8\n",
        "\n",
        "    n_train_seq: int = 128\n",
        "    n_val_seq: int = 32\n",
        "    n_test_seq: int = 32\n",
        "\n",
        "    # Training (keep small for sanity-check; increase later)\n",
        "    lr: float = 3e-4\n",
        "    epochs: int = 3\n",
        "    steps_per_epoch: int = 100\n",
        "\n",
        "    # Loss weights\n",
        "    w_state: float = 1.0\n",
        "    w_nll_y: float = 0.05      # innovation NLL (calibrates S, hence intervals)\n",
        "    w_nll_x: float = 0.05      # state NLL using state GT only (no theta supervision)\n",
        "    w_bce: float = 0.2\n",
        "    w_smooth: float = 1e-4\n",
        "    w_offdiag: float = 1e-4\n",
        "\n",
        "    # Q_theta gating (variance)\n",
        "    Qtheta_base: float = 1e-8\n",
        "    Qtheta_jump: float = 1e-4   # increase (e.g., 1e-3) for faster adaptation if stable\n",
        "\n",
        "    # label width after change (p_k uses features up to k-1, so label starts at k*+1)\n",
        "    p_label_width: int = 25\n",
        "\n",
        "\n",
        "    # sampling: oversample windows where the change occurs inside the LOSS region\n",
        "    p_event_frac: float = 0.5\n",
        "    p_event_jitter: int = 64\n",
        "    p_event_target_in_loss: float = 0.25   # 0..1 location inside [burn_in, burn_in+L)\n",
        "\n",
        "    # BCE balancing for rare positives\n",
        "    bce_use_pos_weight: bool = True\n",
        "    bce_posw_max: float = 50.0\n",
        "\n",
        "    # safety: skip catastrophic batches\n",
        "    skip_loss_thresh: float = 1e5\n",
        "cfg = CFG()\n",
        "print(cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f1b4f0f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- Utilities: Cholesky parameterization for full SPD Qx and R ---\n",
        "\n",
        "def n_tril(n: int) -> int:\n",
        "    return n * (n + 1) // 2\n",
        "\n",
        "\n",
        "def vec_to_cholesky(v: torch.Tensor, n: int, eps: float = 1e-4,\n",
        "                    diag_min: float = -12.0, diag_max: float = 6.0,\n",
        "                    off_clip: float = 3.0) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    v: (..., n_tril(n)) lower-triangular params\n",
        "       convention: first n entries -> diag logits, remaining -> strict-lower row-wise\n",
        "    returns L: (..., n, n) with positive diag (via softplus)\n",
        "\n",
        "    Safety:\n",
        "      - nan/inf -> 0\n",
        "      - diag logits clamped to [diag_min, diag_max]\n",
        "      - off-diagonals clipped to [-off_clip, off_clip]\n",
        "    \"\"\"\n",
        "    v = torch.nan_to_num(v, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    diag_logits = torch.clamp(v[..., :n], min=diag_min, max=diag_max)\n",
        "    off = torch.clamp(v[..., n:], min=-off_clip, max=off_clip)\n",
        "\n",
        "    L = v.new_zeros(*v.shape[:-1], n, n)\n",
        "    diag = F.softplus(diag_logits) + eps\n",
        "    idx = torch.arange(n, device=v.device)\n",
        "    L[..., idx, idx] = diag\n",
        "\n",
        "    k = 0\n",
        "    for i in range(1, n):\n",
        "        for j in range(i):\n",
        "            L[..., i, j] = off[..., k]\n",
        "            k += 1\n",
        "    return L\n",
        "\n",
        "\n",
        "def chol_to_spd(L: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    n = L.shape[-1]\n",
        "    I = torch.eye(n, device=L.device, dtype=L.dtype)\n",
        "    return L @ L.transpose(-1, -2) + eps * I\n",
        "\n",
        "def blockdiag(Qx: torch.Tensor, Qt: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Qx: (B,nx,nx), Qt: (B,nt,nt) -> Qz: (B,nz,nz)\n",
        "    \"\"\"\n",
        "    B, nx, _ = Qx.shape\n",
        "    _, nt, _ = Qt.shape\n",
        "    Qz = Qx.new_zeros(B, nx + nt, nx + nt)\n",
        "    Qz[:, :nx, :nx] = Qx\n",
        "    Qz[:, nx:, nx:] = Qt\n",
        "    return Qz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bceaa514",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- Physics: 2-DOF model (theta scales the coupling stiffness) ---\n",
        "\n",
        "def build_mck(theta: torch.Tensor, device, dtype) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    theta: (B,1) stiffness scale for coupling spring\n",
        "    Returns M,C,K for each batch as (B,2,2)\n",
        "    \"\"\"\n",
        "    B = theta.shape[0]\n",
        "    m1, m2 = 1.0, 1.0\n",
        "    c1, c2, cc = 0.05, 0.05, 0.02\n",
        "    k1, k2 = 20.0, 20.0\n",
        "    kc0 = 15.0\n",
        "    kc = kc0 * theta.squeeze(-1)  # (B,)\n",
        "\n",
        "    M = torch.tensor([[m1, 0.0],[0.0, m2]], device=device, dtype=dtype).expand(B,2,2).clone()\n",
        "\n",
        "    # Damping: C = [[c1+cc, -cc],[-cc, c2+cc]]\n",
        "    C = torch.zeros(B,2,2, device=device, dtype=dtype)\n",
        "    C[:,0,0] = c1 + cc\n",
        "    C[:,1,1] = c2 + cc\n",
        "    C[:,0,1] = -cc\n",
        "    C[:,1,0] = -cc\n",
        "\n",
        "    # Stiffness: K = [[k1+kc, -kc],[-kc, k2+kc]]\n",
        "    K = torch.zeros(B,2,2, device=device, dtype=dtype)\n",
        "    K[:,0,0] = k1 + kc\n",
        "    K[:,1,1] = k2 + kc\n",
        "    K[:,0,1] = -kc\n",
        "    K[:,1,0] = -kc\n",
        "    return M, C, K\n",
        "\n",
        "def f_step(z: torch.Tensor, dt: float) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    z: (B, nz) = [q1,q2,v1,v2,theta]\n",
        "    deterministic transition (no noise added here)\n",
        "    \"\"\"\n",
        "    device, dtype = z.device, z.dtype\n",
        "    q = z[:, 0:2]            # (B,2)\n",
        "    v = z[:, 2:4]            # (B,2)\n",
        "    theta = z[:, 4:5]        # (B,1)\n",
        "\n",
        "    M, C, K = build_mck(theta, device, dtype)\n",
        "    Minv = torch.linalg.inv(M)\n",
        "\n",
        "    a = -(Minv @ (C @ v.unsqueeze(-1) + K @ q.unsqueeze(-1))).squeeze(-1)  # (B,2)\n",
        "\n",
        "    q_next = q + dt * v\n",
        "    v_next = v + dt * a\n",
        "    theta_next = theta  # random-walk handled via Q_theta in the filter\n",
        "\n",
        "    return torch.cat([q_next, v_next, theta_next], dim=-1)\n",
        "\n",
        "def h_meas(z: torch.Tensor) -> torch.Tensor:\n",
        "    # measurement: y = [q1,q2]\n",
        "    return z[:, 0:2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "28670f8f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGxCAYAAABBZ+3pAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAMnVJREFUeJzt3Xt8FOWh//HvImEDlKwEIRdJuFWlUUwDCEmoYKwGUKIebYnFxnhELD2oINpqALkc6UFb7VHEa0UjtQa0EtDKRVAgIkELsmjR4w+OERASuShZEspyyfP7w2YOS+4bsnkSP+/Xa16vzDPPzD7Pzi77ZWaeGZcxxggAAMBibZq7AQAAAHUhsAAAAOsRWAAAgPUILAAAwHoEFgAAYD0CCwAAsB6BBQAAWI/AAgAArEdgAQAA1iOwAE3E5XLVa1q7dm2ztvOpp55Sbm5ulfK1a9fK5XLpr3/96xl7rQ0bNmjmzJk6dOjQGdvmqRYsWKCuXbvq8OHDTpnL5dIdd9xR72307NnzjLervLxc06dP1/nnny+3260uXbooLS1N27dvr3Gd1atXO5+RAwcOVGljTZ+n8PBwp963336rs88+W0uWLDnjfQJCrW1zNwBorQoLCwPmH3zwQa1Zs0bvvvtuQHlCQkIom1XFU089pXPOOUe33HJLk7/Whg0bNGvWLN1yyy06++yzz+i2jxw5oilTpui+++5Tp06d6r3esWPH9NBDD2nChAnq0qWLU7579279+c9/1pQpUxrVrrKyMqWlpWnv3r26//77dfHFF6u0tFQbNmzQkSNHalxn3Lhxio2N1d69e6ssz8/Pl9/vDyjbtWuXMjMz9W//9m9OWefOnXX33XfrN7/5ja666iq1a9euUX0BmhOBBWgiycnJAfNdu3ZVmzZtqpSf7siRI+rQoUNTNq1Veumll3Tw4EHddtttDVqvTZs2io6O1hVXXKFf/vKXOnbsmKZOnap33nlHv/nNbxrdrmnTpumzzz7Txx9/rN69ezvl11xzTY3r3H///ercubOuvvpqzZ49u8rypKSkKmUrV66UpCr9Hz9+vGbPnq2//vWvGjNmTLDdAJodp4SAZnTZZZfpoosuUkFBgVJTU9WhQwfdeuutkr47lTFz5swq6/Ts2bPK0ZCSkhL96le/Uvfu3dWuXTv16tVLs2bN0okTJ2p9/Z49e2rbtm1at26dc0rh9FMix48f19SpUxUbG6uIiAhdccUV+vzzz6tsa/Xq1frpT3+qiIgIdejQQUOGDNE777zjLJ85c6YTAHr16lXllNiiRYuUnp6umJgYtW/fXj/60Y90//33q7y8vI538TtPP/20MjIy6jxyY4zRlClTFBYWpj/96U9q27atbr/9dr3//vtatmyZiouLtXPnTm3YsEE33HBDvV67JkeOHNHzzz+vn//85wFhpTbvvfeennvuOT3//PM666yz6rWOMUYvvviievfurcsvvzxgWVRUlK688ko988wzDW4/YBMCC9DMiouL9ctf/lJjxozRsmXL9B//8R8NWr+kpESDBg3SypUrNX36dC1fvlxjx47VnDlzNG7cuFrXzc/PV+/evZWUlKTCwkIVFhYqPz8/oM6UKVO0c+dOPf/883ruuee0fft2ZWRk6OTJk06dl19+Wenp6YqIiNBLL72kV199VZGRkRo+fLgTWm677TbdeeedkqTFixc7r9e/f39J0vbt23XVVVdp/vz5WrFihSZNmqRXX31VGRkZdb4HX331lT755BOlpaXVWs/v92vMmDGaN2+e3nzzTY0bN04nTpzQCy+8oEsvvVQjR45UTEyM4uPjlZqaqsWLFzvrVlRU6MSJE3VOp74vmzdvVnl5uc477zz9+te/VufOndWuXTsNHDhQb731VpX2/fOf/9TYsWM1adIk532pj9WrV2vnzp269dZb5XK5qiy/7LLL9P777zfZtUNASBgAIZGdnW06duwYUDZs2DAjybzzzjtV6ksyM2bMqFLeo0cPk52d7cz/6le/Mj/4wQ/Mzp07A+o98sgjRpLZtm1bre268MILzbBhw6qUr1mzxkgyV111VUD5q6++aiSZwsJCY4wx5eXlJjIy0mRkZATUO3nypElMTDSDBg1yyv7whz8YSaaoqKjWNlVUVJjjx4+bdevWGUlm69attdZftGiRkWQ2btxYZZkkM2HCBHPw4EHzk5/8xJx77rnG6/U6y/1+v5k5c6Y5cOCAMea799cYY3bu3Glmz57t1MvOzjaS6pxOfS/z8vKMJBMREWGGDBli3njjDfO3v/3NpKWlGZfLZVasWBHQ1nvuucf07t3bHDlyxBhjzIwZM4wks3///lr7n5mZac466yzz1VdfVbt81apVRpJZvnx5rdsBbMY1LEAz69y5c5XD+A3xt7/9TWlpaYqNjQ04BTRy5Ejde++9WrduXaMu7D39WouLL75YkrRz504lJydrw4YN+uabb5SdnV3lFNSIESP0+9//XuXl5erYsWOtr/PFF19o2rRpevfdd7Vv3z4ZY5xln332mfO61am8MLVbt27VLi8qKlJKSorCw8O1ceNGde/e3VnWrl07zZgxo8o68fHxmjp1qjM/c+bMeo02OvWC34qKCuc1li9f7ixLS0vTeeedpwcffFDDhw+XJH344Yd67LHHtGLFCrVv377O16n0zTffaMmSJRoxYoTOPffcautUvi979uyp93YB2xBYgGYWExPTqPW//vprvfnmmwoLC6t2+elDYhvq1JEzkuR2uyV9d/qi8vUl6Wc/+1mN2/jmm29qDSxlZWW69NJLFR4ertmzZ+v8889Xhw4dtHv3bl1//fXOa9WkcvmpQ3pP9eGHH+rAgQP63e9+FxBWqvPll19WWx4fH1/nupICTslUvnepqakBQaZDhw4aNmxYwHDjW2+9Vddff70GDhzonLo5evSoJMnn88ntdlc7+unll1+W3++v9WLjyvelrvcRsBmBBWhm1V1zIH0XDE4fuipJBw8eDJg/55xzdPHFF+t3v/tdtduJjY1tfCNrcc4550iSnnjiiRpHQEVFRdW6jXfffVd79+7V2rVrNWzYMKe8vtdcVLbhm2++qTYAZmZmKjo6WlOnTlVFRYWmTZtWr+2e6tZbb9VLL71UZ71hw4Y5FxLXdlTIGKM2bf7vMsJt27Zp27Zteu2116rU7dOnjxITE+X1eqssmz9/vqKiojRq1KgaX+ubb76R9H/vE9ASEVgAS/Xs2VMff/xxQNm7776rsrKygLJRo0Zp2bJl6tOnjzp37tzg13G73Y36n/eQIUN09tln69NPP63zlMnpR2cqVYa2yuWVnn322Xq1oW/fvpKk//3f/9WFF15YbZ1p06apU6dOuvvuu1VeXq45c+bUa9uVgjklFBMTo5SUFL3//vvy+XyKiIiQ9N3ooXXr1gUEvDVr1lTZVm5url566SUtWbKk2tM9mzZt0scff6zf/va3atu25n/Ov/jiC0nNf88foDEILIClsrKy9MADD2j69OkaNmyYPv30U82bN08ejyeg3n/+539q1apVSk1N1V133aULLrhAR48e1Zdffqlly5bpmWeeqfVURr9+/bRw4UItWrRIvXv3Vnh4uPr161fvdv7gBz/QE088oezsbH3zzTf62c9+pm7dumn//v3aunWr9u/fr6efftp5LUl6/PHHlZ2drbCwMF1wwQVKTU1V586dNX78eM2YMUNhYWH6y1/+oq1bt9arDYMHD1b79u21cePGWu9vMnHiRP3gBz/Q7bffrrKyMs2dO7fGI1yn69mzZ1B3wX3kkUeUlpam4cOH67777pPL5dKjjz6qAwcO6MEHH3TqXXbZZVXWrTxSM2TIkGqPjsyfP1+SNHbs2FrbsHHjRnXp0qVB+xWwTnNf9Qt8X9Q0SujCCy+str7f7ze//e1vTVxcnGnfvr0ZNmyY8Xq9VUYJGWPM/v37zV133WV69eplwsLCTGRkpBkwYICZOnWqKSsrq7VdX375pUlPTzedOnUykpxRMpWjhF577bWA+kVFRUaSefHFFwPK161bZ66++moTGRlpwsLCzLnnnmuuvvrqKuvn5OSY2NhY06ZNGyPJrFmzxhhjzIYNG0xKSorp0KGD6dq1q7ntttvMRx99VO1rVScrK8skJCRUKde/RgmdKi8vz7Rt29b8+7//uzl58mSd226s9957zwwbNsx06NDBdOjQwVx++eXm/fffr3O92kYJHTlyxHg8HjN06NBat1FRUWF69Ohh7rzzzqDbD9jAZcwpl+IDQAu1adMmXXLJJdq4caMGDx7c3M2xxjvvvKP09HRt27bNOXUGtEQEFgCtRmZmpsrLy/W3v/2tuZtijbS0NP3whz/Un/70p+ZuCtAo3OkWQKvx6KOP6pJLLgl4WvP32bfffqthw4bVOIIMaEk4wgIAAKzHERYAAGA9AgsAALAegQUAAFiv1dw4rqKiQnv37lWnTp3qfSMoAADQvIwxOnz4sGJjYwMeV3G6VhNY9u7dq7i4uOZuBgAACMLu3btrvSt3qwkslc/v2L17t/O8DgAAYDefz6e4uLhqn0Z+qlYTWCpPA0VERBBYAABoYeq6nIOLbgEAgPUILAAAwHoEFgAAYD0CCwAAsB6BBQAAWI/AAgAArEdgAQAA1iOwAAAA6xFYAACA9QgsAADAeg0OLAUFBcrIyFBsbKxcLpeWLFlSa/3169dryJAh6tKli9q3b6++ffvqv//7v6vUe/3115WQkCC3262EhATl5+c3tGkAAKCVanBgKS8vV2JioubNm1ev+h07dtQdd9yhgoICffbZZ5o2bZqmTZum5557zqlTWFiozMxMZWVlaevWrcrKytLo0aP1wQcfNLR5AACgFXIZY0zQK7tcys/P13XXXdeg9a6//np17NhRf/7znyVJmZmZ8vl8Wr58uVNnxIgR6ty5s/Ly8qrdht/vl9/vd+Yrn/ZYWlrKww9hHe/uQ3pz615VBP91A4Bmd+uQXoqL7HBGt+nz+eTxeOr8/Q7505q3bNmiDRs2aPbs2U5ZYWGh7r777oB6w4cP12OPPVbjdubMmaNZs2Y1VTOBM2rG0n9o61elzd0MAGiUjMTYMx5Y6itkgaV79+7av3+/Tpw4oZkzZ+q2225zlpWUlCgqKiqgflRUlEpKSmrcXk5OjiZPnuzMVx5hAWxUfuykJOm6H8fq3M7tm7k1ABCcqIjwZnvtkAWW9957T2VlZdq4caPuv/9+/fCHP9QvfvELZ7nL5Qqob4ypUnYqt9stt9vdZO0FmsKNg+KV3LtLczcDAFqckAWWXr16SZL69eunr7/+WjNnznQCS3R0dJWjKfv27aty1AUAAHw/Nct9WIwxARfMpqSkaNWqVQF13n77baWmpoa6aQAAwEINPsJSVlamHTt2OPNFRUXyer2KjIxUfHy8cnJytGfPHi1YsECS9OSTTyo+Pl59+/aV9N19WR555BHdeeedzjYmTpyooUOH6uGHH9a1116rpUuXavXq1Vq/fn1j+wcAAFqBBgeWTZs2KS0tzZmvvPA1Oztbubm5Ki4u1q5du5zlFRUVysnJUVFRkdq2bas+ffrooYce0q9+9SunTmpqqhYuXKhp06bpgQceUJ8+fbRo0SINHjy4MX0DrNGIuwcAANTI+7DYpL7juIHm8NNH1+p/95dr4e3JXHQLAKeo7+83zxICQqjmcW8AgNoQWAAAgPUILAAAwHoEFgAAYD0CCxACreLKdgBoRgQWAABgPQILEEK1PR8LAFAzAgsAALAegQUAAFiPwAKEAlfdAkCjEFgAAID1CCwAAMB6BBYghBgkBADBIbAAAADrEVgAAID1CCxACDBICAAah8ACAACsR2ABAADWI7AAIcQgIQAIDoEFAABYj8ACAACsR2ABQsAYxgkBQGMQWAAAgPUILEAIcWt+AAgOgQUAAFiPwAIAAKxHYAEAANYjsAAhwBghAGgcAgsAALAegQUIKYYJAUAwCCwAAMB6BBYAAGA9AgsQAtyZHwAah8ACAACsR2ABAADWI7AAIcSzhAAgOAQWAABgPQILAACwHoEFCAHDzfkBoFEILAAAwHoEFgAAYD0CCxBCDBICgOAQWAAAgPUILAAAwHoEFiAEeJYQADROgwNLQUGBMjIyFBsbK5fLpSVLltRaf/HixbryyivVtWtXRUREKCUlRStXrgyok5ubK5fLVWU6evRoQ5sHAABaoQYHlvLyciUmJmrevHn1ql9QUKArr7xSy5Yt0+bNm5WWlqaMjAxt2bIloF5ERISKi4sDpvDw8IY2DwAAtEJtG7rCyJEjNXLkyHrXf+yxxwLm/+u//ktLly7Vm2++qaSkJKfc5XIpOjq63tv1+/3y+/3OvM/nq/e6QHNx8TAhAAhKyK9hqaio0OHDhxUZGRlQXlZWph49eqh79+4aNWpUlSMwp5szZ448Ho8zxcXFNWWzAQBAMwp5YHn00UdVXl6u0aNHO2V9+/ZVbm6u3njjDeXl5Sk8PFxDhgzR9u3ba9xOTk6OSktLnWn37t2haD4AAGgGDT4l1Bh5eXmaOXOmli5dqm7dujnlycnJSk5OduaHDBmi/v3764knntDcuXOr3Zbb7Zbb7W7yNgNnAqOEAKBxQhZYFi1apLFjx+q1117TFVdcUWvdNm3a6JJLLqn1CAsAAPj+CMkpoby8PN1yyy165ZVXdPXVV9dZ3xgjr9ermJiYELQOAADYrsFHWMrKyrRjxw5nvqioSF6vV5GRkYqPj1dOTo727NmjBQsWSPourNx88816/PHHlZycrJKSEklS+/bt5fF4JEmzZs1ScnKyzjvvPPl8Ps2dO1der1dPPvnkmegjYA3GCAFAcBp8hGXTpk1KSkpyhiRPnjxZSUlJmj59uiSpuLhYu3btcuo/++yzOnHihCZMmKCYmBhnmjhxolPn0KFDuv322/WjH/1I6enp2rNnjwoKCjRo0KDG9g8AALQCLmNax+WAPp9PHo9HpaWlioiIaO7mAAGGPPSu9hz6p5ZOGKLEuLObuzkAYI36/n7zLCEAAGA9AgsAALAegQUAAFiPwAKEEI8SAoDgEFgAAID1CCxACLSSwXgA0GwILAAAwHoEFgAAYD0CCwAAsB6BBQghF08TAoCgEFgAAID1CCxACDBGCAAah8ACAACsR2ABAADWI7AAIcSt+QEgOAQWAABgPQILAACwHoEFCAEeJQQAjUNgAQAA1iOwAAAA6xFYAACA9QgsAADAegQWAABgPQILEAKGpwkBQKMQWAAAgPUILAAAwHoEFiCEeJYQAASHwAIAAKxHYAFCgFvzA0DjEFgAAID1CCwAAMB6BBYAAGA9AgsQQi4xTAgAgkFgAQAA1iOwACHAICEAaBwCCwAAsB6BBQAAWI/AAgAArEdgAUKIZwkBQHAILAAAwHoEFiAEeJYQADQOgQUAAFiPwAIAAKxHYAEAANYjsAAhxCghAAhOgwNLQUGBMjIyFBsbK5fLpSVLltRaf/HixbryyivVtWtXRUREKCUlRStXrqxS7/XXX1dCQoLcbrcSEhKUn5/f0KYBAIBWqsGBpby8XImJiZo3b1696hcUFOjKK6/UsmXLtHnzZqWlpSkjI0Nbtmxx6hQWFiozM1NZWVnaunWrsrKyNHr0aH3wwQcNbR5gKYYJAUBjuIwJfsCly+VSfn6+rrvuugatd+GFFyozM1PTp0+XJGVmZsrn82n58uVOnREjRqhz587Ky8urdht+v19+v9+Z9/l8iouLU2lpqSIiIhreGaAJDZy9SgfKjmnFpEvVN5rPJwBU8vl88ng8df5+h/waloqKCh0+fFiRkZFOWWFhodLT0wPqDR8+XBs2bKhxO3PmzJHH43GmuLi4JmszAABoXiEPLI8++qjKy8s1evRop6ykpERRUVEB9aKiolRSUlLjdnJyclRaWupMu3fvbrI2AwCA5tU2lC+Wl5enmTNnaunSperWrVvAMtdpwyeMMVXKTuV2u+V2u5uknUBTcYlhQgAQjJAFlkWLFmns2LF67bXXdMUVVwQsi46OrnI0Zd++fVWOugAtFbfmB4DGCckpoby8PN1yyy165ZVXdPXVV1dZnpKSolWrVgWUvf3220pNTQ1F8wAAgOUafISlrKxMO3bscOaLiork9XoVGRmp+Ph45eTkaM+ePVqwYIGk78LKzTffrMcff1zJycnOkZT27dvL4/FIkiZOnKihQ4fq4Ycf1rXXXqulS5dq9erVWr9+/ZnoIwAAaOEafIRl06ZNSkpKUlJSkiRp8uTJSkpKcoYoFxcXa9euXU79Z599VidOnNCECRMUExPjTBMnTnTqpKamauHChXrxxRd18cUXKzc3V4sWLdLgwYMb2z8AANAKNOo+LDap7zhuoDkMeHCVDpYf08pJQ3VBdKfmbg4AWMPa+7AA32c8SwgAgkNgAUKgVRzGBIBmRGABAADWI7AAAADrEVgAAID1CCxACHHNLQAEh8ACAACsR2ABQqCV3O4IAJoNgQUAAFiPwAIAAKxHYAEAANYjsAAhxK35ASA4BBYAAGA9AgsQAowRAoDGIbAAAADrEVgAAID1CCwAAMB6BBYgpBgmBADBILAAAADrEViAEOBRQgDQOAQWAABgPQILAACwHoEFAABYj8AChBDPEgKA4BBYgBAwXHULAI1CYAEAANYjsAAAAOsRWAAAgPUILAAAwHoEFiCEGCQEAMEhsAAhwBghAGgcAgsAALAegQUAAFiPwAIAAKxHYAEAANYjsAAh5OJhQgAQFAILEAoMEwKARiGwAAAA6xFYAACA9QgsAADAegQWAABgPQILEEKMEQKA4BBYgBBgkBAANA6BBQAAWI/AAgAArNfgwFJQUKCMjAzFxsbK5XJpyZIltdYvLi7WmDFjdMEFF6hNmzaaNGlSlTq5ublyuVxVpqNHjza0eQAAoBVqcGApLy9XYmKi5s2bV6/6fr9fXbt21dSpU5WYmFhjvYiICBUXFwdM4eHhDW0eAABohdo2dIWRI0dq5MiR9a7fs2dPPf7445KkF154ocZ6LpdL0dHR9d6u3++X3+935n0+X73XBULNmO8uu+VRQgAQHGuuYSkrK1OPHj3UvXt3jRo1Slu2bKm1/pw5c+TxeJwpLi4uRC0FAAChZkVg6du3r3Jzc/XGG28oLy9P4eHhGjJkiLZv317jOjk5OSotLXWm3bt3h7DFAAAglBp8SqgpJCcnKzk52ZkfMmSI+vfvryeeeEJz586tdh232y232x2qJgIAgGZkxRGW07Vp00aXXHJJrUdYAADA94eVgcUYI6/Xq5iYmOZuCnBGubg5PwAEpcGnhMrKyrRjxw5nvqioSF6vV5GRkYqPj1dOTo727NmjBQsWOHW8Xq+z7v79++X1etWuXTslJCRIkmbNmqXk5GSdd9558vl8mjt3rrxer5588slGdg+wA7fmB4DGaXBg2bRpk9LS0pz5yZMnS5Kys7OVm5ur4uJi7dq1K2CdpKQk5+/NmzfrlVdeUY8ePfTll19Kkg4dOqTbb79dJSUl8ng8SkpKUkFBgQYNGhRMnwAAQCvjMpU3iGjhfD6fPB6PSktLFRER0dzNAQIkTF+hI8dOquA3aYrv0qG5mwMA1qjv77eV17AAAACcisACAACsR2ABQohb8wNAcAgsQAi0jivFAKD5EFgAAID1CCwAAMB6BBYAAGA9AgsAALAegQUAAFiPwAKEgOFpQgDQKAQWAABgPQILAACwHoEFAABYj8ACAACsR2ABQohnCQFAcAgsQAjwLCEAaBwCCwAAsB6BBQAAWI/AAgAArEdgAQAA1iOwACFQec2ti2FCABAUAgsAALAegQUAAFiPwAIAAKxHYAEAANYjsAAAAOsRWIBQ+NcwIcYIAUBwCCwAAMB6BBYAAGA9AgsAALAegQUAAFiPwAIAAKxHYAFCwPxrmBCPEgKA4BBYAACA9QgsAADAegQWAABgPQILAACwHoEFAABYj8AChIBxniXEMCEACAaBBQAAWI/AAgAArEdgAQAA1iOwAAAA6xFYgBD41zW33JofAIJEYAEAANZrcGApKChQRkaGYmNj5XK5tGTJklrrFxcXa8yYMbrgggvUpk0bTZo0qdp6r7/+uhISEuR2u5WQkKD8/PyGNg0AALRSDQ4s5eXlSkxM1Lx58+pV3+/3q2vXrpo6daoSExOrrVNYWKjMzExlZWVp69atysrK0ujRo/XBBx80tHkAAKAVchlTeUurIFZ2uZSfn6/rrruuXvUvu+wy/fjHP9Zjjz0WUJ6ZmSmfz6fly5c7ZSNGjFDnzp2Vl5dX7bb8fr/8fr8z7/P5FBcXp9LSUkVERDS4L0BT6jNlmU5WGH0w5aeKighv7uYAgDV8Pp88Hk+dv99WXMNSWFio9PT0gLLhw4drw4YNNa4zZ84ceTweZ4qLi2vqZgIAgGZiRWApKSlRVFRUQFlUVJRKSkpqXCcnJ0elpaXOtHv37qZuJhC0ygOZDBICgOC0be4GVHKdNt7TGFOl7FRut1tut7upmwUAACxgxRGW6OjoKkdT9u3bV+WoCwAA+H6yIrCkpKRo1apVAWVvv/22UlNTm6lFAADAJg0+JVRWVqYdO3Y480VFRfJ6vYqMjFR8fLxycnK0Z88eLViwwKnj9Xqddffv3y+v16t27dopISFBkjRx4kQNHTpUDz/8sK699lotXbpUq1ev1vr16xvZPQAA0Bo0OLBs2rRJaWlpzvzkyZMlSdnZ2crNzVVxcbF27doVsE5SUpLz9+bNm/XKK6+oR48e+vLLLyVJqampWrhwoaZNm6YHHnhAffr00aJFizR48OBg+gQAAFqZRt2HxSb1HccNNIdeOW/JGOnDqT9Vt07chwUAKrWo+7AAAADUhsACAACsR2ABAADWI7AAAADrEVgAAID1CCxACFSOxXPxNCEACAqBBQAAWI/AAgAArEdgAQAA1iOwAAAA6xFYAACA9QgsQAi5GCQEAEEhsAAAAOsRWAAAgPUILAAAwHoEFgAAYD0CC9DETOV9+QEAQSOwACHEICEACA6BBQAAWI/AAgAArEdgAQAA1iOwAAAA6xFYgCbGICEAaDwCCxBCLh4mBABBIbAAAADrEVgAAID1CCwAAMB6BBYAAGA9AgvQxBgkBACNR2ABQogxQgAQHAILAACwHoEFAABYj8ACAACsR2ABAADWI7AATcyc8jAh7swPAMEhsAAAAOsRWAAAgPUILAAAwHoEFgAAYD0CC9DEuDU/ADQegQUIIRc35weAoBBYAACA9QgsAADAegQWAABgPQILAACwXoMDS0FBgTIyMhQbGyuXy6UlS5bUuc66des0YMAAhYeHq3fv3nrmmWcClufm5srlclWZjh492tDmAdYxDBMCgEZrcGApLy9XYmKi5s2bV6/6RUVFuuqqq3TppZdqy5YtmjJliu666y69/vrrAfUiIiJUXFwcMIWHhze0eYDdGCQEAEFp29AVRo4cqZEjR9a7/jPPPKP4+Hg99thjkqQf/ehH2rRpkx555BHdcMMNTj2Xy6Xo6Oh6b9fv98vv9zvzPp+v3usCAICWpcmvYSksLFR6enpA2fDhw7Vp0yYdP37cKSsrK1OPHj3UvXt3jRo1Slu2bKl1u3PmzJHH43GmuLi4Jmk/AABofk0eWEpKShQVFRVQFhUVpRMnTujAgQOSpL59+yo3N1dvvPGG8vLyFB4eriFDhmj79u01bjcnJ0elpaXOtHv37ibtBwAAaD4NPiUUDJcr8MS9+ddViJXlycnJSk5OdpYPGTJE/fv31xNPPKG5c+dWu0232y23291ELQYAADZp8iMs0dHRKikpCSjbt2+f2rZtqy5dulTfqDZtdMkll9R6hAVoKQxPEwKARmvywJKSkqJVq1YFlL399tsaOHCgwsLCql3HGCOv16uYmJimbh4QUi5GCQFAUBocWMrKyuT1euX1eiV9N2zZ6/Vq165dkr67tuTmm2926o8fP147d+7U5MmT9dlnn+mFF17Q/Pnzde+99zp1Zs2apZUrV+qLL76Q1+vV2LFj5fV6NX78+EZ2DwAAtAYNvoZl06ZNSktLc+YnT54sScrOzlZubq6Ki4ud8CJJvXr10rJly3T33XfrySefVGxsrObOnRswpPnQoUO6/fbbVVJSIo/Ho6SkJBUUFGjQoEGN6RsAAGglXMa0jvtw+nw+eTwelZaWKiIiormbAzj8J07qgmkrJEkfz0xXRHj1p0IB4Puovr/fPEsIAABYj8ACNLHWcQwTAJoXgQUIIQYJAUBwCCwAAMB6BBYAAGA9AgsAALAegQUAAFiPwAIAAKxHYAFC6PQnlwMA6ofAAgAArEdgAQAA1iOwAAAA6xFYgCbGrfkBoPEILAAAwHoEFiCEGCMEAMEhsAAAAOsRWAAAgPUILAAAwHoEFqCJGTFMCAAai8ACAACsR2ABQohHCQFAcAgsAADAegQWAABgPQILAACwHoEFaGI8SwgAGo/AAoSQi5vzA0BQCCwAAMB6BBYAAGA9AgsAALAegQUAAFiPwAI0MQYJAUDjEViAEOLW/AAQHAILAACwHoEFAABYj8ACAACsR2ABmpjh3vwA0GgEFgAAYD0CCwAAsB6BBQAAWI/AAgAArEdgAQAA1iOwAE2MMUIA0HgEFgAAYD0CCxBCPEsIAIJDYAEAANZrcGApKChQRkaGYmNj5XK5tGTJkjrXWbdunQYMGKDw8HD17t1bzzzzTJU6r7/+uhISEuR2u5WQkKD8/PyGNg0AALRSDQ4s5eXlSkxM1Lx58+pVv6ioSFdddZUuvfRSbdmyRVOmTNFdd92l119/3alTWFiozMxMZWVlaevWrcrKytLo0aP1wQcfNLR5AACgFXKZRjzoxOVyKT8/X9ddd12Nde677z698cYb+uyzz5yy8ePHa+vWrSosLJQkZWZmyufzafny5U6dESNGqHPnzsrLy6t2u36/X36/35n3+XyKi4tTaWmpIiIigu1SFfPXF+mrb4+cse3h++fYiQr95YNdkqTPZ4+Qu+1ZzdwiALCHz+eTx+Op8/e7bVM3pLCwUOnp6QFlw4cP1/z583X8+HGFhYWpsLBQd999d5U6jz32WI3bnTNnjmbNmtUUTQ7w1sd79dGuQ03+Omj92oedpbO46hYAgtLkgaWkpERRUVEBZVFRUTpx4oQOHDigmJiYGuuUlJTUuN2cnBxNnjzZma88wnKm3TCgu1L6dDnj28X3z+BeXdT2LK5zB4BgNHlgkb47dXSqyrNQp5ZXV+f0slO53W653e4z2Mrq3TS4R5O/BgAAqF2T/3cvOjq6ypGSffv2qW3bturSpUutdU4/6gIAAL6fmjywpKSkaNWqVQFlb7/9tgYOHKiwsLBa66SmpjZ18wAAQAvQ4FNCZWVl2rFjhzNfVFQkr9eryMhIxcfHKycnR3v27NGCBQskfTciaN68eZo8ebLGjRunwsJCzZ8/P2D0z8SJEzV06FA9/PDDuvbaa7V06VKtXr1a69evPwNdBAAALV2Dj7Bs2rRJSUlJSkpKkiRNnjxZSUlJmj59uiSpuLhYu3btcur36tVLy5Yt09q1a/XjH/9YDz74oObOnasbbrjBqZOamqqFCxfqxRdf1MUXX6zc3FwtWrRIgwcPbmz/AABAK9Co+7DYpL7juAEAgD3q+/vNGEsAAGA9AgsAALAegQUAAFiPwAIAAKxHYAEAANYjsAAAAOsRWAAAgPUILAAAwHoheVpzKFTe/87n8zVzSwAAQH1V/m7XdR/bVhNYDh8+LEmKi4tr5pYAAICGOnz4sDweT43LW82t+SsqKrR371516tRJLpfrjG3X5/MpLi5Ou3fvbrW3/G/tfaR/LV9r72Nr75/U+vtI/4JnjNHhw4cVGxurNm1qvlKl1RxhadOmjbp3795k24+IiGiVH8JTtfY+0r+Wr7X3sbX3T2r9faR/wantyEolLroFAADWI7AAAADrEVjq4Ha7NWPGDLnd7uZuSpNp7X2kfy1fa+9ja++f1Pr7SP+aXqu56BYAALReHGEBAADWI7AAAADrEVgAAID1CCwAAMB6BBYAAGA9AksdnnrqKfXq1Uvh4eEaMGCA3nvvveZuUp3mzJmjSy65RJ06dVK3bt103XXX6fPPPw+oc8stt8jlcgVMycnJAXX8fr/uvPNOnXPOOerYsaOuueYaffXVV6HsSo1mzpxZpf3R0dHOcmOMZs6cqdjYWLVv316XXXaZtm3bFrANm/vXs2fPKv1zuVyaMGGCpJa5/woKCpSRkaHY2Fi5XC4tWbIkYPmZ2mfffvutsrKy5PF45PF4lJWVpUOHDjVx72rv3/Hjx3XfffepX79+6tixo2JjY3XzzTdr7969Adu47LLLquzXG2+80fr+SWfuM9lc/ZPq7mN130mXy6U//OEPTh2b92F9fhts/h4SWGqxaNEiTZo0SVOnTtWWLVt06aWXauTIkdq1a1dzN61W69at04QJE7Rx40atWrVKJ06cUHp6usrLywPqjRgxQsXFxc60bNmygOWTJk1Sfn6+Fi5cqPXr16usrEyjRo3SyZMnQ9mdGl144YUB7f/kk0+cZb///e/1xz/+UfPmzdPf//53RUdH68orr3QekinZ3b+///3vAX1btWqVJOnnP/+5U6el7b/y8nIlJiZq3rx51S4/U/tszJgx8nq9WrFihVasWCGv16usrKxm7d+RI0f00Ucf6YEHHtBHH32kxYsX6//9v/+na665pkrdcePGBezXZ599NmC5jf2rdCY+k83VP6nuPp7at+LiYr3wwgtyuVy64YYbAurZug/r89tg9ffQoEaDBg0y48ePDyjr27evuf/++5upRcHZt2+fkWTWrVvnlGVnZ5trr722xnUOHTpkwsLCzMKFC52yPXv2mDZt2pgVK1Y0ZXPrZcaMGSYxMbHaZRUVFSY6Oto89NBDTtnRo0eNx+MxzzzzjDHG/v6dbuLEiaZPnz6moqLCGNPy958kk5+f78yfqX326aefGklm48aNTp3CwkIjyfzP//xPE/fq/5zev+p8+OGHRpLZuXOnUzZs2DAzceLEGtexuX9n4jNpS/+Mqd8+vPbaa83ll18eUNZS9qExVX8bbP8ecoSlBseOHdPmzZuVnp4eUJ6enq4NGzY0U6uCU1paKkmKjIwMKF+7dq26deum888/X+PGjdO+ffucZZs3b9bx48cD+h8bG6uLLrrImv5v375dsbGx6tWrl2688UZ98cUXkqSioiKVlJQEtN3tdmvYsGFO21tC/yodO3ZML7/8sm699daAJ5G39P13qjO1zwoLC+XxeDR48GCnTnJysjwej3X9Li0tlcvl0tlnnx1Q/pe//EXnnHOOLrzwQt17770B/7O1vX+N/Uza3r9Tff3113rrrbc0duzYKstayj48/bfB9u9hq3la85l24MABnTx5UlFRUQHlUVFRKikpaaZWNZwxRpMnT9ZPfvITXXTRRU75yJEj9fOf/1w9evRQUVGRHnjgAV1++eXavHmz3G63SkpK1K5dO3Xu3Dlge7b0f/DgwVqwYIHOP/98ff3115o9e7ZSU1O1bds2p33V7budO3dKkvX9O9WSJUt06NAh3XLLLU5ZS99/pztT+6ykpETdunWrsv1u3bpZ1e+jR4/q/vvv15gxYwKefHvTTTepV69eio6O1j/+8Q/l5ORo69atzilBm/t3Jj6TNvfvdC+99JI6deqk66+/PqC8pezD6n4bbP8eEljqcOr/aKXvdvLpZTa744479PHHH2v9+vUB5ZmZmc7fF110kQYOHKgePXrorbfeqvIFPJUt/R85cqTzd79+/ZSSkqI+ffropZdeci70C2bf2dK/U82fP18jR45UbGysU9bS919NzsQ+q66+Tf0+fvy4brzxRlVUVOipp54KWDZu3Djn74suukjnnXeeBg4cqI8++kj9+/eXZG//ztRn0tb+ne6FF17QTTfdpPDw8IDylrIPa/ptkOz9HnJKqAbnnHOOzjrrrCppcN++fVXSp63uvPNOvfHGG1qzZo26d+9ea92YmBj16NFD27dvlyRFR0fr2LFj+vbbbwPq2dr/jh07ql+/ftq+fbszWqi2fddS+rdz506tXr1at912W631Wvr+O1P7LDo6Wl9//XWV7e/fv9+Kfh8/flyjR49WUVGRVq1aFXB0pTr9+/dXWFhYwH61uX+nCuYz2VL699577+nzzz+v83sp2bkPa/ptsP17SGCpQbt27TRgwADnMF6lVatWKTU1tZlaVT/GGN1xxx1avHix3n33XfXq1avOdQ4ePKjdu3crJiZGkjRgwACFhYUF9L+4uFj/+Mc/rOy/3+/XZ599ppiYGOdw7KltP3bsmNatW+e0vaX078UXX1S3bt109dVX11qvpe+/M7XPUlJSVFpaqg8//NCp88EHH6i0tLTZ+10ZVrZv367Vq1erS5cuda6zbds2HT9+3NmvNvfvdMF8JltK/+bPn68BAwYoMTGxzro27cO6fhus/x4Gfbnu98DChQtNWFiYmT9/vvn000/NpEmTTMeOHc2XX37Z3E2r1a9//Wvj8XjM2rVrTXFxsTMdOXLEGGPM4cOHzT333GM2bNhgioqKzJo1a0xKSoo599xzjc/nc7Yzfvx40717d7N69Wrz0Ucfmcsvv9wkJiaaEydONFfXHPfcc49Zu3at+eKLL8zGjRvNqFGjTKdOnZx989BDDxmPx2MWL15sPvnkE/OLX/zCxMTEtJj+GWPMyZMnTXx8vLnvvvsCylvq/jt8+LDZsmWL2bJli5Fk/vjHP5otW7Y4o2TO1D4bMWKEufjii01hYaEpLCw0/fr1M6NGjWrW/h0/ftxcc801pnv37sbr9QZ8L/1+vzHGmB07dphZs2aZv//976aoqMi89dZbpm/fviYpKcn6/p3Jz2Rz9a+uPlYqLS01HTp0ME8//XSV9W3fh3X9Nhhj9/eQwFKHJ5980vTo0cO0a9fO9O/fP2BosK0kVTu9+OKLxhhjjhw5YtLT003Xrl1NWFiYiY+PN9nZ2WbXrl0B2/nnP/9p7rjjDhMZGWnat29vRo0aVaVOc8nMzDQxMTEmLCzMxMbGmuuvv95s27bNWV5RUWFmzJhhoqOjjdvtNkOHDjWffPJJwDZs7p8xxqxcudJIMp9//nlAeUvdf2vWrKn2c5mdnW2MOXP77ODBg+amm24ynTp1Mp06dTI33XST+fbbb5u1f0VFRTV+L9esWWOMMWbXrl1m6NChJjIy0rRr18706dPH3HXXXebgwYPW9+9Mfiabq3919bHSs88+a9q3b28OHTpUZX3b92Fdvw3G2P09dP2rEwAAANbiGhYAAGA9AgsAALAegQUAAFiPwAIAAKxHYAEAANYjsAAAAOsRWAAAgPUILAAAwHoEFgAAYD0CCwAAsB6BBQAAWO//A48u8gXGimC0AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "# --- Simulation: T=2000 with random k* and +30% step jump in theta ---\n",
        "\n",
        "@torch.no_grad()\n",
        "def simulate_sequence(T: int, dt: float, jump_ratio: float, kstar: int, device, dtype):\n",
        "    \"\"\"\n",
        "    Generates x_true (T,nx), theta_true (T,1), y (T,ny)\n",
        "    \"\"\"\n",
        "    theta0 = 1.0\n",
        "    theta1 = theta0 * (1.0 + jump_ratio)  # +30% at/after k*\n",
        "    theta_true = torch.full((T,1), theta0, device=device, dtype=dtype)\n",
        "    theta_true[kstar:, 0] = theta1\n",
        "\n",
        "    # initial state\n",
        "    q0 = torch.tensor([0.2, -0.1], device=device, dtype=dtype)\n",
        "    v0 = torch.tensor([0.0, 0.0], device=device, dtype=dtype)\n",
        "    x = torch.cat([q0, v0]).unsqueeze(0)  # (1,4)\n",
        "\n",
        "    # simulation noises\n",
        "    Qsim = torch.diag(torch.tensor([1e-6,1e-6,1e-5,1e-5], device=device, dtype=dtype))\n",
        "    Lsim = torch.linalg.cholesky(Qsim)\n",
        "\n",
        "    Rsim = torch.diag(torch.tensor([2e-4, 2e-4], device=device, dtype=dtype))\n",
        "    Lr = torch.linalg.cholesky(Rsim)\n",
        "\n",
        "    x_true = torch.zeros(T,4, device=device, dtype=dtype)\n",
        "    y = torch.zeros(T,2, device=device, dtype=dtype)\n",
        "\n",
        "    for k in range(T):\n",
        "        th = theta_true[k:k+1]  # (1,1)\n",
        "        z = torch.cat([x, th], dim=-1)  # (1,5)\n",
        "\n",
        "        z_next = f_step(z, dt=dt)\n",
        "        x_next = z_next[:, :4]\n",
        "        x_next = x_next + (torch.randn(1,4, device=device, dtype=dtype) @ Lsim.T)\n",
        "        x = x_next\n",
        "\n",
        "        x_true[k] = x.squeeze(0)\n",
        "        y[k] = x_true[k, :2] + (torch.randn(2, device=device, dtype=dtype) @ Lr.T)\n",
        "\n",
        "    return x_true, theta_true, y\n",
        "\n",
        "def make_p_label(T: int, kstar: int, width: int, device, dtype):\n",
        "    # label starts at kstar+1 because p_k is predicted from phi up to k-1\n",
        "    start = min(T-1, kstar + 1)\n",
        "    end = min(T, start + width)\n",
        "    p = torch.zeros(T,1, device=device, dtype=dtype)\n",
        "    p[start:end, 0] = 1.0\n",
        "    return p\n",
        "\n",
        "# quick sanity-check visualization of one simulated sequence\n",
        "device, dtype = cfg.device, cfg.dtype\n",
        "kstar = random.randint(int(cfg.kstar_lo*cfg.T), int(cfg.kstar_hi*cfg.T))\n",
        "x_true, theta_true, y = simulate_sequence(cfg.T, cfg.dt, cfg.jump_ratio, kstar, device, dtype)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(theta_true.cpu().numpy())\n",
        "plt.title(f\"True theta (k*={kstar})\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d157200c",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def safe_cholesky(A: torch.Tensor, jitter: float = 1e-6, max_tries: int = 12) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Very robust batched Cholesky.\n",
        "\n",
        "    Strategy:\n",
        "      1) sanitize + symmetrize\n",
        "      2) try cholesky_ex with *per-batch* diagonal loading scaled by matrix magnitude\n",
        "      3) if still failing, repair in float64 by shifting eigenvalues, then cholesky\n",
        "\n",
        "    Returns L such that (approximately) A ≈ L L^T.\n",
        "    \"\"\"\n",
        "    A = torch.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    A = 0.5 * (A + A.transpose(-1, -2))\n",
        "    B, n, _ = A.shape\n",
        "    I = torch.eye(n, device=A.device, dtype=A.dtype).unsqueeze(0)\n",
        "\n",
        "    # Scale-aware base jitter per batch (prevents \"too small jitter\" when A blows up)\n",
        "    diag = torch.diagonal(A, dim1=-2, dim2=-1)  # (B,n)\n",
        "    scale = diag.abs().mean(dim=-1)             # (B,)\n",
        "    base = jitter * (1.0 + scale)               # (B,)\n",
        "\n",
        "    # Try increasing diagonal loading\n",
        "    for i in range(max_tries):\n",
        "        j = (10.0 ** i) * base                  # (B,)\n",
        "        Aj = A + j.view(B, 1, 1) * I\n",
        "        L, info = torch.linalg.cholesky_ex(Aj)\n",
        "        if (info == 0).all():\n",
        "            return L\n",
        "\n",
        "    # Float64 repair path (rare): eigenvalue shift then cholesky\n",
        "    A64 = A.double()\n",
        "    I64 = torch.eye(n, device=A.device, dtype=torch.float64).unsqueeze(0)\n",
        "\n",
        "    w, V = torch.linalg.eigh(A64)\n",
        "    # clamp eigenvalues, but also apply a global shift if needed\n",
        "    min_eig = w.min(dim=-1).values             # (B,)\n",
        "    # Ensure min eigenvalue at least eps64\n",
        "    eps64 = float(max(jitter, 1e-12))\n",
        "    shift = torch.clamp(-min_eig + eps64, min=0.0)\n",
        "    A64 = (V @ torch.diag_embed(torch.clamp(w, min=eps64)) @ V.transpose(-1, -2)) + shift.view(B,1,1) * I64\n",
        "    A64 = 0.5 * (A64 + A64.transpose(-1, -2))\n",
        "\n",
        "    L64, info = torch.linalg.cholesky_ex(A64)\n",
        "    if (info != 0).any():\n",
        "        # last-resort huge diagonal loading\n",
        "        big = (10.0 ** max_tries) * base.double()\n",
        "        A64 = A64 + big.view(B,1,1) * I64\n",
        "        L64 = torch.linalg.cholesky(A64)\n",
        "\n",
        "    return L64.to(A.dtype)\n",
        "\n",
        "\n",
        "# --- Innovation features phi_k = [L^{-1}e, |L^{-1}e|, NIS, logdetS] ---\n",
        "\n",
        "def make_phi(e: torch.Tensor, S: torch.Tensor, jitter: float = 1e-6,\n",
        "             clip_e: float = 50.0, clip_phi: float = 50.0) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Innovation features phi_k = [e_white, |e_white|, log1p(NIS), logdetS]\n",
        "\n",
        "    Stabilization:\n",
        "      - sanitize NaN/Inf\n",
        "      - robust Cholesky with diagonal loading\n",
        "      - clip whitened innovation to prevent overflow in NIS\n",
        "      - use log1p(NIS) to compress heavy tails\n",
        "      - clip logdetS\n",
        "    \"\"\"\n",
        "    e = torch.nan_to_num(e, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    S = torch.nan_to_num(S, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    B, ny = e.shape\n",
        "    I = torch.eye(ny, device=e.device, dtype=e.dtype).unsqueeze(0)\n",
        "    S = 0.5 * (S + S.transpose(-1, -2)) + jitter * I\n",
        "\n",
        "    L = safe_cholesky(S, jitter=jitter)  # (B,ny,ny)\n",
        "\n",
        "    # e_white = L^{-1} e\n",
        "    e_white = torch.linalg.solve_triangular(L, e.unsqueeze(-1), upper=False).squeeze(-1)\n",
        "    e_white = torch.clamp(torch.nan_to_num(e_white, nan=0.0, posinf=0.0, neginf=0.0), -clip_e, clip_e)\n",
        "\n",
        "    nis = (e_white**2).sum(dim=-1, keepdim=True)\n",
        "    nis = torch.log1p(torch.clamp(torch.nan_to_num(nis, nan=0.0, posinf=0.0, neginf=0.0), 0.0, 1e6))\n",
        "\n",
        "    diagL = torch.diagonal(L, dim1=-2, dim2=-1)\n",
        "    diagL = torch.clamp(torch.nan_to_num(diagL, nan=jitter, posinf=jitter, neginf=jitter), min=jitter)\n",
        "    logdetS = 2.0 * torch.log(diagL).sum(dim=-1, keepdim=True)\n",
        "    logdetS = torch.clamp(torch.nan_to_num(logdetS, nan=0.0, posinf=0.0, neginf=0.0), -clip_phi, clip_phi)\n",
        "\n",
        "    phi = torch.cat([e_white, e_white.abs(), nis, logdetS], dim=-1)\n",
        "    phi = torch.clamp(torch.nan_to_num(phi, nan=0.0, posinf=0.0, neginf=0.0), -clip_phi, clip_phi)\n",
        "    return phi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "515f7b59",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Linear algebra helpers for UKF stability ---\n",
        "# NOTE: `safe_cholesky()` is defined above (with nan/inf sanitization + diagonal loading).\n",
        "# Here we keep a single `make_spd()` (sanitize + eigenvalue clamp) to avoid accidental overwrites.\n",
        "\n",
        "def sanitize_sym(A: torch.Tensor) -> torch.Tensor:\n",
        "    A = torch.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return 0.5 * (A + A.transpose(-1, -2))\n",
        "\n",
        "def make_spd(A: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    \"\"\"Project a symmetric matrix to SPD by clamping eigenvalues (batched).\"\"\"\n",
        "    A = sanitize_sym(A)\n",
        "        # For small matrices, float64 eig improves numerical robustness (prevents rare Cholesky failures)\n",
        "    if A.dtype == torch.float32 and A.shape[-1] <= 10:\n",
        "        A64 = A.double()\n",
        "        w, V = torch.linalg.eigh(A64)\n",
        "        w = torch.clamp(w, min=float(eps))\n",
        "        A_spd = V @ torch.diag_embed(w) @ V.transpose(-1, -2)\n",
        "        A_spd = A_spd.to(A.dtype)\n",
        "    else:\n",
        "        w, V = torch.linalg.eigh(A)\n",
        "        w = torch.clamp(w, min=eps)\n",
        "        A_spd = V @ torch.diag_embed(w) @ V.transpose(-1, -2)\n",
        "    n = A.shape[-1]\n",
        "    I = torch.eye(n, device=A.device, dtype=A.dtype)\n",
        "    return A_spd + eps * I\n",
        "\n",
        "def sqrtm_spd(A: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "    \"\"\"Symmetric sqrt for (approx) SPD matrices via eigendecomposition (robust fallback).\"\"\"\n",
        "    A = sanitize_sym(A)\n",
        "    w, V = torch.linalg.eigh(A)\n",
        "    w = torch.clamp(w, min=eps)\n",
        "    return V @ torch.diag_embed(torch.sqrt(w))\n",
        "\n",
        "\n",
        "# --- UKF step (batched) ---\n",
        "\n",
        "def ukf_step(z: torch.Tensor, P: torch.Tensor, y: torch.Tensor, Qz: torch.Tensor, R: torch.Tensor,\n",
        "            dt: float, alpha: float, beta: float, kappa: float,\n",
        "            jitter_P: float, jitter_S: float):\n",
        "    \"\"\"\n",
        "    z: (B,nz) mean\n",
        "    P: (B,nz,nz)\n",
        "    y: (B,ny)\n",
        "    Qz: (B,nz,nz)\n",
        "    R:  (B,ny,ny)\n",
        "    Returns z_upd, P_upd, innovation e, S\n",
        "    \"\"\"\n",
        "    device, dtype = z.device, z.dtype\n",
        "    B, nz = z.shape\n",
        "    ny = y.shape[-1]\n",
        "\n",
        "    lam = alpha**2 * (nz + kappa) - nz\n",
        "    c = nz + lam\n",
        "    Wm0 = lam / c\n",
        "    Wc0 = Wm0 + (1 - alpha**2 + beta)\n",
        "    W = 1.0 / (2.0 * c)\n",
        "\n",
        "    # robust sqrt of P for sigma points\n",
        "    Pj = make_spd(P, eps=jitter_P)\n",
        "    U = sqrtm_spd(Pj, eps=jitter_P) * math.sqrt(c)  # (B,nz,nz)\n",
        "\n",
        "    # sigma points\n",
        "    sigmas = z.unsqueeze(1).repeat(1, 2*nz+1, 1)\n",
        "    Ucols = U.transpose(-1, -2)  # each row = a column of U\n",
        "    sigmas[:, 1:nz+1, :] = z.unsqueeze(1) + Ucols\n",
        "    sigmas[:, nz+1:,  :] = z.unsqueeze(1) - Ucols\n",
        "\n",
        "    # propagate through dynamics\n",
        "    sig_flat = sigmas.reshape(B*(2*nz+1), nz)\n",
        "    zprop = f_step(sig_flat, dt=dt).reshape(B, 2*nz+1, nz)\n",
        "\n",
        "    # weights\n",
        "    wm = torch.full((2*nz+1,), W, device=device, dtype=dtype)\n",
        "    wc = torch.full((2*nz+1,), W, device=device, dtype=dtype)\n",
        "    wm[0] = Wm0\n",
        "    wc[0] = Wc0\n",
        "\n",
        "    # predicted mean/cov\n",
        "    z_pred = (zprop * wm.view(1,-1,1)).sum(dim=1)\n",
        "    dz = zprop - z_pred.unsqueeze(1)\n",
        "    P_pred = torch.einsum(\"i,bij,bik->bjk\", wc, dz, dz) + Qz\n",
        "\n",
        "    # predicted measurement\n",
        "    yprop = h_meas(zprop.reshape(B*(2*nz+1), nz)).reshape(B, 2*nz+1, ny)\n",
        "    y_pred = (yprop * wm.view(1,-1,1)).sum(dim=1)\n",
        "    dy = yprop - y_pred.unsqueeze(1)\n",
        "    S = torch.einsum(\"i,bij,bik->bjk\", wc, dy, dy) + R\n",
        "    Pzy = torch.einsum(\"i,bij,bik->bjk\", wc, dz, dy)\n",
        "\n",
        "    # stabilize S (and ensure SPD)\n",
        "    I_y = torch.eye(ny, device=device, dtype=dtype).unsqueeze(0)\n",
        "    S = make_spd(S + jitter_S * I_y, eps=jitter_S)\n",
        "\n",
        "    # gain and update: K = Pzy @ S^{-1} using Cholesky solves\n",
        "    Ls = safe_cholesky(S, jitter=jitter_S)\n",
        "    tmp = torch.linalg.solve_triangular(Ls, Pzy.transpose(-1,-2), upper=False)\n",
        "    tmp = torch.linalg.solve_triangular(Ls.transpose(-1,-2), tmp, upper=True)\n",
        "    K = tmp.transpose(-1,-2)\n",
        "\n",
        "    e = y - y_pred\n",
        "    z_upd = z_pred + torch.einsum(\"bij,bj->bi\", K, e)\n",
        "\n",
        "    P_upd = P_pred - K @ S @ K.transpose(-1,-2)\n",
        "    P_upd = make_spd(0.5 * (P_upd + P_upd.transpose(-1,-2)), eps=jitter_P)\n",
        "    return z_upd, P_upd, e, S"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "785a89bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- Causal window Transformer NoiseNet ---\n",
        "# It predicts Qx_k, R_k (full SPD via Cholesky factors) and a change logit p_k.\n",
        "# It only sees past features (phi up to k-1), so the whole pipeline is causal.\n",
        "\n",
        "class CausalWindowTransformerNoiseNet(nn.Module):\n",
        "    def __init__(self, d_in: int, d_model: int, nx: int, ny: int,\n",
        "                 W: int, n_layers: int, n_heads: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.nx, self.ny = nx, ny\n",
        "        self.nQ = n_tril(nx)\n",
        "        self.nR = n_tril(ny)\n",
        "        self.d_out = self.nQ + self.nR + 1\n",
        "\n",
        "\n",
        "        self.phi_norm = nn.LayerNorm(d_in)\n",
        "        self.in_proj = nn.Linear(d_in, d_model)\n",
        "        self.pos = nn.Parameter(torch.zeros(1, W, d_model))\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=n_heads, dim_feedforward=4*d_model,\n",
        "            dropout=dropout, batch_first=True, activation=\"gelu\", norm_first=True\n",
        "        )\n",
        "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
        "        self.head = nn.Linear(d_model, self.d_out)\n",
        "\n",
        "        # initialize to small-ish Q/R (stability)\n",
        "        with torch.no_grad():\n",
        "            self.head.weight.mul_(0.01)\n",
        "            self.head.bias.zero_()\n",
        "            self.head.bias[:nx] = -6.0               # Q diag logits\n",
        "            self.head.bias[self.nQ:self.nQ+ny] = -6.0 # R diag logits\n",
        "            self.head.bias[-1] = -6.0                 # p_logit bias -> p≈0 at init\n",
        "\n",
        "    def forward(self, phi_seq: torch.Tensor):\n",
        "        # phi_seq: (B,W,d_in)\n",
        "        phi_seq = torch.nan_to_num(phi_seq, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        phi_seq = torch.clamp(phi_seq, -50.0, 50.0)\n",
        "        phi_seq = self.phi_norm(phi_seq)\n",
        "\n",
        "        B, W, _ = phi_seq.shape\n",
        "        h = self.in_proj(phi_seq) + self.pos[:, :W, :]\n",
        "        h = self.enc(h)\n",
        "        out = self.head(h[:, -1, :])  # last token as \"current\" prediction\n",
        "        out = torch.nan_to_num(out, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        qv = out[:, :self.nQ]\n",
        "        rv = out[:, self.nQ:self.nQ+self.nR]\n",
        "        p_logit = out[:, -1:].clamp(-20.0, 20.0)\n",
        "        return qv, rv, p_logit\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "14f45d50",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating new datasets...\n",
            "  - Building Train (128 seq)...\n",
            "  - Building Val (32 seq)...\n",
            "  - Building Test (32 seq)...\n",
            "Saving datasets to ./data...\n",
            "Data generation and saving complete.\n",
            "Dataset Sizes -> Train: 128, Val: 32, Test: 32\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Dataset creation (in-memory): random k* per sequence ---\n",
        "# --- 데이터 저장 경로 설정 ---\n",
        "DATA_DIR = \"./data\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "TRAIN_FILE = os.path.join(DATA_DIR, \"train_data.pt\")\n",
        "VAL_FILE = os.path.join(DATA_DIR, \"val_data.pt\")\n",
        "TEST_FILE = os.path.join(DATA_DIR, \"test_data.pt\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def build_dataset(n_seq: int, cfg: CFG):\n",
        "    dev, dt = cfg.device, cfg.dtype\n",
        "    data = []\n",
        "    for _ in range(n_seq):\n",
        "        kstar = random.randint(int(cfg.kstar_lo * cfg.T), int(cfg.kstar_hi * cfg.T))\n",
        "        x_true, theta_true, y = simulate_sequence(cfg.T, cfg.dt, cfg.jump_ratio, kstar, dev, dt)\n",
        "        p = make_p_label(cfg.T, kstar, cfg.p_label_width, dev, dt)\n",
        "        data.append({\"x_true\": x_true, \"theta_true\": theta_true, \"y\": y, \"p\": p, \"kstar\": kstar})\n",
        "    return data\n",
        "\n",
        "def sample_batch(data, cfg: CFG, balanced: bool = True):\n",
        "    \"\"\"\n",
        "    Samples B segments of length seg=burn_in+L.\n",
        "\n",
        "    Why 'balanced':\n",
        "      p labels are sparse (only around k*). Pure random cropping often yields\n",
        "      batches with *no positives inside the LOSS window*, which makes BCE learning\n",
        "      unstable / degenerate.\n",
        "\n",
        "    Strategy:\n",
        "      With probability cfg.p_event_frac, choose the crop so that the label start\n",
        "      (k*+1) lands inside the LOSS region (after burn_in), near a target location.\n",
        "      Otherwise sample uniformly at random.\n",
        "    \"\"\"\n",
        "    B = cfg.batch_size\n",
        "    T = cfg.T\n",
        "    seg = cfg.burn_in + cfg.L\n",
        "    idx = random.sample(range(len(data)), B)\n",
        "    s_max = T - seg - 1\n",
        "\n",
        "    y, x, p = [], [], []\n",
        "    starts = []\n",
        "    for i in idx:\n",
        "        d = data[i]\n",
        "        if balanced and (random.random() < cfg.p_event_frac):\n",
        "            kstar = int(d[\"kstar\"])\n",
        "            # label starts at kstar+1; we want that to be inside the LOSS region [burn_in, burn_in+L)\n",
        "            target = cfg.burn_in + int(cfg.p_event_target_in_loss * cfg.L)\n",
        "            s = (kstar + 1) - target + random.randint(-cfg.p_event_jitter, cfg.p_event_jitter)\n",
        "            s = max(0, min(s_max, s))\n",
        "        else:\n",
        "            s = random.randint(0, s_max)\n",
        "        starts.append(s)\n",
        "\n",
        "        y.append(d[\"y\"][s:s+seg])\n",
        "        x.append(d[\"x_true\"][s:s+seg])\n",
        "        p.append(d[\"p\"][s:s+seg])\n",
        "\n",
        "    return torch.stack(y, 0), torch.stack(x, 0), torch.stack(p, 0)\n",
        "\n",
        "def prepare_datasets(cfg, force_generate=False):\n",
        "    \"\"\"\n",
        "    데이터 파일이 있으면 로드하고, 없으면 생성 후 저장합니다.\n",
        "    force_generate=True로 설정하면 무조건 새로 생성하고 덮어씁니다.\n",
        "    \"\"\"\n",
        "    # 1. 파일이 모두 존재하고, 강제 생성이 아닐 경우 -> 로드\n",
        "    if os.path.exists(TRAIN_FILE) and os.path.exists(VAL_FILE) and os.path.exists(TEST_FILE) and not force_generate:\n",
        "        print(f\"Loading datasets from {DATA_DIR}...\")\n",
        "        train_data = torch.load(TRAIN_FILE)\n",
        "        val_data = torch.load(VAL_FILE)\n",
        "        test_data = torch.load(TEST_FILE)\n",
        "        print(\"Data loaded successfully.\")\n",
        "    \n",
        "    # 2. 파일이 없거나 강제 생성일 경우 -> 생성 및 저장\n",
        "    else:\n",
        "        print(\"Generating new datasets...\")\n",
        "        # (1) Training Data\n",
        "        print(f\"  - Building Train ({cfg.n_train_seq} seq)...\")\n",
        "        train_data = build_dataset(cfg.n_train_seq, cfg)\n",
        "        \n",
        "        # (2) Validation Data\n",
        "        print(f\"  - Building Val ({cfg.n_val_seq} seq)...\")\n",
        "        val_data = build_dataset(cfg.n_val_seq, cfg)\n",
        "        \n",
        "        # (3) Testing Data (새로 추가됨)\n",
        "        print(f\"  - Building Test ({cfg.n_test_seq} seq)...\")\n",
        "        test_data = build_dataset(cfg.n_test_seq, cfg)\n",
        "        \n",
        "        # 저장\n",
        "        print(f\"Saving datasets to {DATA_DIR}...\")\n",
        "        torch.save(train_data, TRAIN_FILE)\n",
        "        torch.save(val_data, VAL_FILE)\n",
        "        torch.save(test_data, TEST_FILE)\n",
        "        print(\"Data generation and saving complete.\")\n",
        "        \n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "# --- 실행 ---\n",
        "# force_generate=True로 하면 기존 파일을 무시하고 새로 만듭니다.\n",
        "train_data, val_data, test_data = prepare_datasets(cfg, force_generate=True)\n",
        "\n",
        "print(f\"Dataset Sizes -> Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a10c3ee8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Differentiable rollout on one (burn-in + loss) window ---\n",
        "\n",
        "def _nll_gaussian_from_chol(resid: torch.Tensor, L: torch.Tensor, clip_quad: float = 1e6) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    resid: (B,d), L: (B,d,d) lower Cholesky of SPD cov\n",
        "    returns per-sample NLL up to an additive constant (drops d*log(2pi)/2).\n",
        "\n",
        "    Stabilization:\n",
        "      - sanitize NaN/Inf\n",
        "      - clamp quadratic form to avoid Inf -> NaN in later ops\n",
        "      - clamp logdet contribution\n",
        "    \"\"\"\n",
        "    resid = torch.nan_to_num(resid, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    L = torch.nan_to_num(L, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    sol = torch.cholesky_solve(resid.unsqueeze(-1), L)  # (B,d,1)\n",
        "    sol = torch.nan_to_num(sol, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    quad = (resid.unsqueeze(1) @ sol).squeeze(-1).squeeze(-1)  # (B,)\n",
        "    quad = torch.clamp(torch.nan_to_num(quad, nan=0.0, posinf=clip_quad, neginf=0.0), 0.0, clip_quad)\n",
        "\n",
        "    diagL = torch.diagonal(L, dim1=-2, dim2=-1)\n",
        "    diagL = torch.clamp(torch.nan_to_num(diagL, nan=1e-6, posinf=1e-6, neginf=1e-6), min=1e-6)\n",
        "    logdet = 2.0 * torch.log(diagL).sum(dim=-1)  # (B,)\n",
        "    logdet = torch.clamp(torch.nan_to_num(logdet, nan=0.0, posinf=0.0, neginf=0.0), -100.0, 100.0)\n",
        "\n",
        "    return 0.5 * (quad + logdet)\n",
        "def rollout_window(net: nn.Module, y_seg: torch.Tensor, x_true_seg: torch.Tensor, p_label_seg: torch.Tensor, cfg: CFG):\n",
        "    \"\"\"\n",
        "    y_seg: (B,Tseg,ny), x_true_seg: (B,Tseg,nx), p_label_seg: (B,Tseg,1)\n",
        "    Returns dict(losses, trajectories)\n",
        "    \"\"\"\n",
        "    device, dtype = y_seg.device, y_seg.dtype\n",
        "    B, Tseg, ny = y_seg.shape\n",
        "    nx, nz = cfg.nx, cfg.nz\n",
        "\n",
        "    d_in = 2*ny + 2\n",
        "\n",
        "    # init z and P\n",
        "    q0 = y_seg[:, 0, :]\n",
        "    v0 = torch.zeros(B,2, device=device, dtype=dtype)\n",
        "    theta0 = torch.ones(B,1, device=device, dtype=dtype)\n",
        "    z = torch.cat([q0, v0, theta0], dim=-1)\n",
        "\n",
        "    P = torch.diag(torch.tensor([1e-2,1e-2, 1e-1,1e-1, 1e-2], device=device, dtype=dtype)).unsqueeze(0).repeat(B,1,1)\n",
        "\n",
        "    # feature buffer: past-only features\n",
        "    phi_buf = torch.zeros(B, cfg.W, d_in, device=device, dtype=dtype)\n",
        "\n",
        "    xhat, thetahat, ppred = [], [], []\n",
        "    qv_hist, rv_hist = [], []\n",
        "    nll_y_hist, nll_x_hist = [], []\n",
        "\n",
        "    for t in range(Tseg):\n",
        "        # 1) predict Qx_k, R_k, p_k from past feature window\n",
        "        qv, rv, p_logit = net(phi_buf)\n",
        "        p = torch.sigmoid(p_logit)\n",
        "\n",
        "        # 2) build full SPD Qx and R via Cholesky\n",
        "        Lq = vec_to_cholesky(qv, nx)\n",
        "        Qx = chol_to_spd(Lq)\n",
        "        Qx = make_spd(torch.nan_to_num(Qx, nan=0.0, posinf=0.0, neginf=0.0), eps=cfg.jitter_P)\n",
        "        Lr = vec_to_cholesky(rv, ny)\n",
        "        R = chol_to_spd(Lr)\n",
        "        R = make_spd(torch.nan_to_num(R, nan=0.0, posinf=0.0, neginf=0.0), eps=cfg.jitter_S)\n",
        "\n",
        "        # 3) gate Q_theta,k using p_k (THIS is the key location)\n",
        "        qtheta = (1.0 - p) * cfg.Qtheta_base + p * cfg.Qtheta_jump\n",
        "        Qt = qtheta.view(B,1,1)\n",
        "        Qz = blockdiag(Qx, Qt)\n",
        "\n",
        "        # 4) UKF update\n",
        "        z, P, e, S = ukf_step(\n",
        "            z, P, y_seg[:, t, :], Qz, R,\n",
        "            dt=cfg.dt, alpha=cfg.alpha, beta=cfg.beta, kappa=cfg.kappa,\n",
        "            jitter_P=cfg.jitter_P, jitter_S=cfg.jitter_S\n",
        "        )\n",
        "\n",
        "        # 4b) probabilistic losses (no theta supervision; uses y and x GT only)\n",
        "        # innovation NLL calibrates S (and indirectly R, Q, P)\n",
        "        Ls = safe_cholesky(S, jitter=cfg.jitter_S)\n",
        "        nll_y = _nll_gaussian_from_chol(e, Ls)  # (B,)\n",
        "        nll_y_hist.append(nll_y)\n",
        "\n",
        "        # state NLL calibrates Px (optional but recommended)\n",
        "        Px = P[:, :nx, :nx]\n",
        "        dx = x_true_seg[:, t, :] - z[:, :nx]\n",
        "        Lx = safe_cholesky(Px, jitter=cfg.jitter_P)\n",
        "        nll_x = _nll_gaussian_from_chol(dx, Lx)\n",
        "        nll_x_hist.append(nll_x)\n",
        "\n",
        "        # 5) create phi_t and append to buffer (for next step)\n",
        "        phi_t = make_phi(e, S, jitter=cfg.jitter_S)\n",
        "        phi_buf = torch.cat([phi_buf[:, 1:, :], phi_t.unsqueeze(1)], dim=1)\n",
        "\n",
        "        xhat.append(z[:, :nx])\n",
        "        thetahat.append(z[:, nx:nx+1])\n",
        "        ppred.append(p_logit)\n",
        "        qv_hist.append(qv)\n",
        "        rv_hist.append(rv)\n",
        "\n",
        "    xhat = torch.stack(xhat, 1)\n",
        "    thetahat = torch.stack(thetahat, 1)\n",
        "    ppred = torch.stack(ppred, 1)\n",
        "    ppred = torch.nan_to_num(ppred, nan=0.0, posinf=0.0, neginf=0.0).clamp(-20.0, 20.0)\n",
        "\n",
        "    qv_hist = torch.stack(qv_hist, 1)\n",
        "    rv_hist = torch.stack(rv_hist, 1)\n",
        "    nll_y_hist = torch.stack(nll_y_hist, 1)  # (B,Tseg)\n",
        "    nll_x_hist = torch.stack(nll_x_hist, 1)\n",
        "\n",
        "    start = cfg.burn_in\n",
        "    end = cfg.burn_in + cfg.L\n",
        "\n",
        "    x_loss = F.smooth_l1_loss(xhat[:, start:end, :], x_true_seg[:, start:end, :])  # robust vs outliers\n",
        "    nll_y = nll_y_hist[:, start:end].mean()\n",
        "    nll_x = nll_x_hist[:, start:end].mean()\n",
        "    p_label = torch.nan_to_num(p_label_seg[:, start:end, :], nan=0.0, posinf=0.0, neginf=0.0).clamp(0.0, 1.0)\n",
        "\n",
        "    # ---- Balanced BCE (pos_weight) to handle sparse change labels ----\n",
        "    logits = ppred[:, start:end, :]\n",
        "    pos = p_label.sum()\n",
        "    neg = p_label.numel() - pos\n",
        "    pos_rate = (pos / (p_label.numel() + 1e-12)).detach()\n",
        "\n",
        "    if cfg.bce_use_pos_weight and (pos > 0) and (neg > 0):\n",
        "        pw = torch.clamp(neg / (pos + 1e-6), 1.0, cfg.bce_posw_max)\n",
        "        pw = torch.tensor([float(pw.detach())], device=device, dtype=dtype)\n",
        "        bce = F.binary_cross_entropy_with_logits(logits, p_label, pos_weight=pw)\n",
        "    else:\n",
        "        bce = F.binary_cross_entropy_with_logits(logits, p_label)\n",
        "\n",
        "    dq = (qv_hist[:, start+1:end, :] - qv_hist[:, start:end-1, :]).pow(2).mean()\n",
        "    dr = (rv_hist[:, start+1:end, :] - rv_hist[:, start:end-1, :]).pow(2).mean()\n",
        "    smooth = dq + dr\n",
        "\n",
        "    q_off = qv_hist[:, start:end, nx:]\n",
        "    r_off = rv_hist[:, start:end, ny:]\n",
        "    offdiag = (q_off.pow(2).mean() + r_off.pow(2).mean())\n",
        "\n",
        "    loss = (\n",
        "        cfg.w_state*x_loss\n",
        "        + cfg.w_nll_y*nll_y\n",
        "        + cfg.w_nll_x*nll_x\n",
        "        + cfg.w_bce*bce\n",
        "        + cfg.w_smooth*smooth\n",
        "        + cfg.w_offdiag*offdiag\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"loss\": loss,\n",
        "        \"x_loss\": x_loss.detach(),\n",
        "        \"nll_y\": nll_y.detach(),\n",
        "        \"nll_x\": nll_x.detach(),\n",
        "        \"bce\": bce.detach(),\n",
        "        \"smooth\": smooth.detach(),\n",
        "        \"offdiag\": offdiag.detach(),\n",
        "        \"xhat\": xhat.detach(),\n",
        "        \"thetahat\": thetahat.detach(),\n",
        "        \"ppred\": ppred.detach(),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# DEBUG CELL: Catch the first NaN/Inf during rollout and print\n",
        "#             the exact step + batch element + key diagnostics.\n",
        "# ------------------------------------------------------------\n",
        "# Usage (run this cell BEFORE training cell):\n",
        "#   enable_debug_rollout(stop_on_nan=True)\n",
        "#   # then run the training cell as-is\n",
        "# To disable:\n",
        "#   disable_debug_rollout()\n",
        "# ============================================================\n",
        "\n",
        "import math\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "def _tensor_stats(x: torch.Tensor) -> Dict[str, Any]:\n",
        "    x = x.detach()\n",
        "    finite = torch.isfinite(x)\n",
        "    frac = float(finite.float().mean().cpu())\n",
        "    stats = {\n",
        "        \"shape\": tuple(x.shape),\n",
        "        \"dtype\": str(x.dtype),\n",
        "        \"frac_finite\": frac,\n",
        "    }\n",
        "    if finite.any():\n",
        "        xf = x[finite]\n",
        "        stats.update({\n",
        "            \"min\": float(xf.min().cpu()),\n",
        "            \"max\": float(xf.max().cpu()),\n",
        "            \"mean\": float(xf.mean().cpu()),\n",
        "            \"std\": float(xf.std().cpu()) if xf.numel() > 1 else 0.0,\n",
        "        })\n",
        "    else:\n",
        "        stats.update({\"min\": float(\"nan\"), \"max\": float(\"nan\"), \"mean\": float(\"nan\"), \"std\": float(\"nan\")})\n",
        "    return stats\n",
        "\n",
        "def _first_bad_batch(x: torch.Tensor) -> Optional[int]:\n",
        "    \"\"\"Return first batch index whose entries contain non-finite values, else None.\"\"\"\n",
        "    if x.ndim == 0:\n",
        "        return 0 if (not torch.isfinite(x)) else None\n",
        "    if x.shape[0] == 0:\n",
        "        return None\n",
        "    finite = torch.isfinite(x)\n",
        "    per_b = finite.view(finite.shape[0], -1).all(dim=1)\n",
        "    bad = (~per_b).nonzero(as_tuple=False)\n",
        "    return int(bad[0].item()) if bad.numel() > 0 else None\n",
        "\n",
        "def _print_diag(name: str, x: torch.Tensor, b: int):\n",
        "    print(f\"{name} (batch {b}) diag:\", torch.diagonal(x[b], dim1=-2, dim2=-1).detach().cpu().numpy())\n",
        "\n",
        "def _print_eigs(name: str, A: torch.Tensor, b: int):\n",
        "    Ab = 0.5*(A[b] + A[b].transpose(-1,-2))\n",
        "    w = torch.linalg.eigvalsh(Ab).detach().cpu().numpy()\n",
        "    print(f\"{name} (batch {b}) eigs:\", w)\n",
        "\n",
        "def _debug_abort(msg: str):\n",
        "    print(\"========== DEBUG ABORT ==========\")\n",
        "    print(msg)\n",
        "    print(\"=================================\")\n",
        "    raise FloatingPointError(msg)\n",
        "\n",
        "def rollout_window_debug(net: nn.Module,\n",
        "                         y_seg: torch.Tensor,\n",
        "                         x_true_seg: torch.Tensor,\n",
        "                         p_label_seg: torch.Tensor,\n",
        "                         cfg: CFG,\n",
        "                         stop_on_nan: bool = True,\n",
        "                         plogit_clip: float = 20.0,\n",
        "                         ewhite_clip: float = 100.0,\n",
        "                         phi_clip: float = 1e4):\n",
        "    \"\"\"\n",
        "    Debug version of rollout_window: prints diagnostics at first sign of NaN/Inf.\n",
        "    Intended for short runs / one mini-batch to locate the failure source.\n",
        "    \"\"\"\n",
        "    device, dtype = y_seg.device, y_seg.dtype\n",
        "    B, Tseg, ny = y_seg.shape\n",
        "    nx, nz = cfg.nx, cfg.nz\n",
        "\n",
        "    d_in = 2*ny + 2\n",
        "\n",
        "    # init z and P\n",
        "    q0 = y_seg[:, 0, :]\n",
        "    v0 = torch.zeros(B,2, device=device, dtype=dtype)\n",
        "    theta0 = torch.ones(B,1, device=device, dtype=dtype)\n",
        "    z = torch.cat([q0, v0, theta0], dim=-1)\n",
        "\n",
        "    P = torch.diag(torch.tensor([1e-2,1e-2, 1e-1,1e-1, 1e-2], device=device,\n",
        "                                dtype=dtype)).unsqueeze(0).repeat(B,1,1)\n",
        "\n",
        "    phi_buf = torch.zeros(B, cfg.W, d_in, device=device, dtype=dtype)\n",
        "\n",
        "    xhat, thetahat, ppred = [], [], []\n",
        "    qv_hist, rv_hist = [], []\n",
        "    nll_y_hist, nll_x_hist = [], []\n",
        "\n",
        "    for t in range(Tseg):\n",
        "        # ---- Net forward ----\n",
        "        if not torch.isfinite(phi_buf).all():\n",
        "            b = _first_bad_batch(phi_buf)\n",
        "            print(f\"[DEBUG] non-finite phi_buf BEFORE net at t={t}, b={b}\")\n",
        "            print(\"phi_buf stats:\", _tensor_stats(phi_buf))\n",
        "            if b is not None:\n",
        "                # print a few entries\n",
        "                print(\"phi_buf[b, -1, :]:\", phi_buf[b, -1, :].detach().cpu().numpy())\n",
        "            if stop_on_nan:\n",
        "                _debug_abort(\"phi_buf became non-finite before net forward.\")\n",
        "\n",
        "        qv, rv, p_logit = net(phi_buf)\n",
        "\n",
        "        b_bad = _first_bad_batch(p_logit)\n",
        "        if (not torch.isfinite(qv).all()) or (not torch.isfinite(rv).all()) or (b_bad is not None):\n",
        "            b = b_bad if b_bad is not None else (_first_bad_batch(qv) or _first_bad_batch(rv) or 0)\n",
        "            print(f\"[DEBUG] non-finite net output at t={t}, b={b}\")\n",
        "            print(\"qv stats:\", _tensor_stats(qv))\n",
        "            print(\"rv stats:\", _tensor_stats(rv))\n",
        "            print(\"p_logit stats:\", _tensor_stats(p_logit))\n",
        "            print(\"phi_buf stats:\", _tensor_stats(phi_buf))\n",
        "            if stop_on_nan:\n",
        "                _debug_abort(\"net output became non-finite.\")\n",
        "\n",
        "        # sanitize p_logit to avoid BCE NaNs during debugging\n",
        "        p_logit = torch.nan_to_num(p_logit, nan=0.0, posinf=0.0, neginf=0.0).clamp(-plogit_clip, plogit_clip)\n",
        "        p = torch.sigmoid(p_logit)\n",
        "\n",
        "        # ---- Build Qx, R ----\n",
        "        Lq = vec_to_cholesky(qv, nx)\n",
        "        Qx = chol_to_spd(Lq)\n",
        "        Lr = vec_to_cholesky(rv, ny)\n",
        "        R = chol_to_spd(Lr)\n",
        "\n",
        "        if (not torch.isfinite(Qx).all()) or (not torch.isfinite(R).all()):\n",
        "            b = _first_bad_batch(Qx) or _first_bad_batch(R) or 0\n",
        "            print(f\"[DEBUG] non-finite Q/R at t={t}, b={b}\")\n",
        "            print(\"Qx stats:\", _tensor_stats(Qx))\n",
        "            print(\"R stats:\", _tensor_stats(R))\n",
        "            if stop_on_nan:\n",
        "                _debug_abort(\"Qx or R became non-finite.\")\n",
        "\n",
        "        # ---- Gate Q_theta ----\n",
        "        qtheta = (1.0 - p) * cfg.Qtheta_base + p * cfg.Qtheta_jump\n",
        "        Qt = qtheta.view(B,1,1)\n",
        "        Qz = blockdiag(Qx, Qt)\n",
        "\n",
        "        if not torch.isfinite(Qz).all():\n",
        "            b = _first_bad_batch(Qz) or 0\n",
        "            print(f\"[DEBUG] non-finite Qz at t={t}, b={b}\")\n",
        "            print(\"p stats:\", _tensor_stats(p))\n",
        "            print(\"qtheta stats:\", _tensor_stats(qtheta))\n",
        "            print(\"Qz stats:\", _tensor_stats(Qz))\n",
        "            if stop_on_nan:\n",
        "                _debug_abort(\"Qz became non-finite.\")\n",
        "\n",
        "        # ---- UKF step ----\n",
        "        z, P, e, S = ukf_step(\n",
        "            z, P, y_seg[:, t, :], Qz, R,\n",
        "            dt=cfg.dt, alpha=cfg.alpha, beta=cfg.beta, kappa=cfg.kappa,\n",
        "            jitter_P=cfg.jitter_P, jitter_S=cfg.jitter_S\n",
        "        )\n",
        "\n",
        "        if (not torch.isfinite(z).all()) or (not torch.isfinite(P).all()) or (not torch.isfinite(e).all()) or (not torch.isfinite(S).all()):\n",
        "            b = _first_bad_batch(z) or _first_bad_batch(P) or _first_bad_batch(e) or _first_bad_batch(S) or 0\n",
        "            print(f\"[DEBUG] non-finite UKF outputs at t={t}, b={b}\")\n",
        "            print(\"z stats:\", _tensor_stats(z))\n",
        "            print(\"P stats:\", _tensor_stats(P))\n",
        "            print(\"e stats:\", _tensor_stats(e))\n",
        "            print(\"S stats:\", _tensor_stats(S))\n",
        "            _print_diag(\"S\", S, b); _print_eigs(\"S\", S, b)\n",
        "            _print_diag(\"P\", P, b); _print_eigs(\"P\", P, b)\n",
        "            if stop_on_nan:\n",
        "                _debug_abort(\"UKF outputs became non-finite.\")\n",
        "\n",
        "        # ---- key diagnostics for stability ----\n",
        "        # min eigs\n",
        "        b = 0\n",
        "        try:\n",
        "            wS = torch.linalg.eigvalsh(0.5*(S + S.transpose(-1,-2)))\n",
        "            wP = torch.linalg.eigvalsh(0.5*(P + P.transpose(-1,-2)))\n",
        "            minS = float(wS.min().detach().cpu())\n",
        "            minP = float(wP.min().detach().cpu())\n",
        "        except Exception:\n",
        "            minS, minP = float(\"nan\"), float(\"nan\")\n",
        "\n",
        "        # e_white magnitude\n",
        "        Ls = safe_cholesky(S, jitter=cfg.jitter_S)\n",
        "        e_white = torch.linalg.solve_triangular(Ls, e.unsqueeze(-1), upper=False).squeeze(-1)\n",
        "        e_white = torch.nan_to_num(e_white, nan=0.0, posinf=0.0, neginf=0.0).clamp(-ewhite_clip, ewhite_clip)\n",
        "        max_ew = float(e_white.abs().max().detach().cpu())\n",
        "        nis = (e_white**2).sum(dim=-1)  # (B,)\n",
        "        max_nis = float(nis.max().detach().cpu())\n",
        "\n",
        "        # ---- probabilistic losses ----\n",
        "        nll_y = _nll_gaussian_from_chol(e, Ls)\n",
        "        Px = P[:, :nx, :nx]\n",
        "        dx = x_true_seg[:, t, :] - z[:, :nx]\n",
        "        Lx = safe_cholesky(Px, jitter=cfg.jitter_P)\n",
        "        nll_x = _nll_gaussian_from_chol(dx, Lx)\n",
        "\n",
        "        if (not torch.isfinite(nll_y).all()) or (not torch.isfinite(nll_x).all()):\n",
        "            b = _first_bad_batch(nll_y) or _first_bad_batch(nll_x) or 0\n",
        "            print(f\"[DEBUG] non-finite NLL at t={t}, b={b}\")\n",
        "            print(\"nll_y stats:\", _tensor_stats(nll_y))\n",
        "            print(\"nll_x stats:\", _tensor_stats(nll_x))\n",
        "            print(f\"min_eig(S)={minS:.3e}, min_eig(P)={minP:.3e}, max|e_white|={max_ew:.3e}, max NIS={max_nis:.3e}\")\n",
        "            _print_diag(\"S\", S, b); _print_eigs(\"S\", S, b)\n",
        "            if stop_on_nan:\n",
        "                _debug_abort(\"NLL became non-finite.\")\n",
        "\n",
        "        nll_y_hist.append(nll_y)\n",
        "        nll_x_hist.append(nll_x)\n",
        "\n",
        "        # ---- phi update ----\n",
        "        phi_t = make_phi(e, S, jitter=cfg.jitter_S)\n",
        "        if not torch.isfinite(phi_t).all():\n",
        "            b = _first_bad_batch(phi_t) or 0\n",
        "            print(f\"[DEBUG] non-finite phi_t at t={t}, b={b}\")\n",
        "            print(\"phi_t stats:\", _tensor_stats(phi_t))\n",
        "            print(f\"min_eig(S)={minS:.3e}, min_eig(P)={minP:.3e}, max|e_white|={max_ew:.3e}, max NIS={max_nis:.3e}\")\n",
        "            _print_diag(\"S\", S, b); _print_eigs(\"S\", S, b)\n",
        "            if stop_on_nan:\n",
        "                _debug_abort(\"phi_t became non-finite.\")\n",
        "\n",
        "        # optional: clamp phi_t to avoid propagating extreme values while debugging\n",
        "        phi_t = torch.nan_to_num(phi_t, nan=0.0, posinf=0.0, neginf=0.0).clamp(-phi_clip, phi_clip)\n",
        "\n",
        "        phi_buf = torch.cat([phi_buf[:, 1:, :], phi_t.unsqueeze(1)], dim=1)\n",
        "\n",
        "        # store\n",
        "        xhat.append(z[:, :nx])\n",
        "        thetahat.append(z[:, nx:nx+1])\n",
        "        ppred.append(p_logit)\n",
        "        qv_hist.append(qv)\n",
        "        rv_hist.append(rv)\n",
        "\n",
        "        # lightweight periodic info (optional)\n",
        "        if t == 0 or (t % 50 == 0):\n",
        "            print(f\"[DEBUG t={t}] min_eig(S)={minS:.3e}, min_eig(P)={minP:.3e}, max|e_white|={max_ew:.3e}, max NIS={max_nis:.3e}, \"\n",
        "                  f\"p in [{float(p.min()):.3e},{float(p.max()):.3e}]\")\n",
        "\n",
        "    # Finish as original\n",
        "    xhat = torch.stack(xhat, 1)\n",
        "    thetahat = torch.stack(thetahat, 1)\n",
        "    ppred = torch.stack(ppred, 1)\n",
        "\n",
        "    start = cfg.burn_in\n",
        "    end = cfg.burn_in + cfg.L\n",
        "\n",
        "    x_loss = F.mse_loss(xhat[:, start:end, :], x_true_seg[:, start:end, :])\n",
        "    nll_y = torch.stack(nll_y_hist, 1)[:, start:end].mean()\n",
        "    nll_x = torch.stack(nll_x_hist, 1)[:, start:end].mean()\n",
        "    bce = F.binary_cross_entropy_with_logits(ppred[:, start:end, :], p_label_seg[:, start:end, :])\n",
        "\n",
        "    loss = cfg.w_state*x_loss + cfg.w_nll_y*nll_y + cfg.w_nll_x*nll_x + cfg.w_bce*bce\n",
        "    return {\"loss\": loss, \"x_loss\": x_loss, \"nll_y\": nll_y, \"nll_x\": nll_x, \"bce\": bce,\n",
        "            \"xhat\": xhat, \"thetahat\": thetahat, \"ppred\": ppred}\n",
        "\n",
        "# --- Monkey-patch helpers (optional) ---\n",
        "_rollout_window_original = None\n",
        "\n",
        "def enable_debug_rollout(stop_on_nan: bool = True):\n",
        "    \"\"\"\n",
        "    After calling this, any call to rollout_window(...) will use rollout_window_debug(...).\n",
        "    \"\"\"\n",
        "    global rollout_window, _rollout_window_original\n",
        "    if _rollout_window_original is None:\n",
        "        _rollout_window_original = rollout_window\n",
        "    def _patched(net, y_seg, x_true_seg, p_label_seg, cfg):\n",
        "        return rollout_window_debug(net, y_seg, x_true_seg, p_label_seg, cfg, stop_on_nan=stop_on_nan)\n",
        "    rollout_window = _patched\n",
        "    print(\"✅ Debug rollout enabled. (rollout_window has been patched)\")\n",
        "\n",
        "def disable_debug_rollout():\n",
        "    global rollout_window, _rollout_window_original\n",
        "    if _rollout_window_original is not None:\n",
        "        rollout_window = _rollout_window_original\n",
        "        _rollout_window_original = None\n",
        "        print(\"✅ Debug rollout disabled. (rollout_window restored)\")\n",
        "    else:\n",
        "        print(\"Debug rollout was not enabled.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4605cc45",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\minhy\\AppData\\Local\\Temp\\ipykernel_9740\\1310868581.py:21: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  self.enc = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Train ---\n",
        "\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "d_in = 2*cfg.ny + 2\n",
        "net = CausalWindowTransformerNoiseNet(\n",
        "    d_in=d_in, d_model=cfg.d_model, nx=cfg.nx, ny=cfg.ny,\n",
        "    W=cfg.W, n_layers=cfg.n_layers, n_heads=cfg.n_heads, dropout=cfg.dropout\n",
        ").to(cfg.device)\n",
        "\n",
        "# Sanity-check: p_logit bias and initial p\n",
        "with torch.no_grad():\n",
        "    print('init head.bias[-1] (p_logit):', float(net.head.bias[-1].detach().cpu()))\n",
        "    dummy_phi = torch.zeros(1, cfg.W, d_in, device=cfg.device, dtype=cfg.dtype)\n",
        "    _, _, p0 = net(dummy_phi)\n",
        "    print('init p (sigmoid):', float(torch.sigmoid(p0)[0,0].detach().cpu()))\n",
        "\n",
        "opt = torch.optim.AdamW(net.parameters(), lr=cfg.lr, weight_decay=1e-4)\n",
        "\n",
        "for ep in range(cfg.epochs):\n",
        "    net.train()\n",
        "    loss_sum = x_sum = bce_sum = nly_sum = nlx_sum = 0.0\n",
        "    posrate_sum = 0.0\n",
        "    pmean_sum = 0.0\n",
        "\n",
        "    for it in range(cfg.steps_per_epoch):\n",
        "        y_seg, x_seg, p_seg = sample_batch(train_data, cfg, balanced=True)\n",
        "        pos_rate = float(p_seg[:, cfg.burn_in:cfg.burn_in+cfg.L, :].mean().detach().cpu())\n",
        "        out = rollout_window(net, y_seg, x_seg, p_seg, cfg)\n",
        "\n",
        "        # Skip catastrophic batches (often from a bad crop right on the jump + unstable covariances)\n",
        "        if float(out['loss'].detach()) > cfg.skip_loss_thresh:\n",
        "            print(f\"[warn] huge loss at ep {ep+1} it {it+1}: {float(out['loss']):.3e} (pos_rate={pos_rate:.3f})\")\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            continue\n",
        "\n",
        "        # Skip non-finite batches to avoid optimizer poisoning\n",
        "        if (not torch.isfinite(out['loss']).all()) or (not torch.isfinite(out['bce']).all()):\n",
        "            print(f\"[warn] non-finite loss at ep {ep+1} it {it+1}: loss={out['loss'].detach().cpu().numpy()}, bce={out['bce'].detach().cpu().numpy()}\")\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            continue\n",
        "\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        out[\"loss\"].backward()\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        loss_sum += float(out[\"loss\"].detach())\n",
        "        x_sum += float(out[\"x_loss\"])\n",
        "        bce_sum += float(out[\"bce\"])\n",
        "        nly_sum += float(out[\"nll_y\"])\n",
        "        nlx_sum += float(out[\"nll_x\"])\n",
        "        posrate_sum += pos_rate\n",
        "        # optional: monitor mean predicted p on this batch window\n",
        "        if 'ppred' in out:\n",
        "            pmean_sum += float(torch.sigmoid(out['ppred'][:, cfg.burn_in:cfg.burn_in+cfg.L, :]).mean().detach())\n",
        "\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        yv, xv, pv = sample_batch(val_data, cfg, balanced=False)\n",
        "        outv = rollout_window(net, yv, xv, pv, cfg)\n",
        "\n",
        "    print(f\"[ep {ep+1}/{cfg.epochs}] \"\n",
        "          f\"train loss {loss_sum/cfg.steps_per_epoch:.4f} (x {x_sum/cfg.steps_per_epoch:.4f}, bce {bce_sum/cfg.steps_per_epoch:.4f}) | \"\n",
        "          f\"val loss {float(outv['loss']):.4f} (x {float(outv['x_loss']):.4f}, bce {float(outv['bce']):.4f}) | train pos_rate {posrate_sum/cfg.steps_per_epoch:.3f} | train p_mean {pmean_sum/cfg.steps_per_epoch:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e85c5db6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- Full 2000-step rollout + probabilistic evaluation (theta GT used ONLY here) ---\n",
        "\n",
        "@torch.no_grad()\n",
        "def rollout_full(net: nn.Module, y: torch.Tensor, cfg: CFG):\n",
        "    \"\"\"\n",
        "    y: (T,ny)\n",
        "    returns:\n",
        "      zhat: (T,nz)\n",
        "      P:    (T,nz,nz)\n",
        "      p:    (T,1)  change probability\n",
        "      nis:  (T,1)  normalized innovation squared (should average ~ ny)\n",
        "    \"\"\"\n",
        "    device, dtype = cfg.device, cfg.dtype\n",
        "    T = y.shape[0]\n",
        "    ny, nx, nz = cfg.ny, cfg.nx, cfg.nz\n",
        "    d_in = 2*ny + 2\n",
        "\n",
        "    q0 = y[0:1, :]\n",
        "    v0 = torch.zeros(1,2, device=device, dtype=dtype)\n",
        "    theta0 = torch.ones(1,1, device=device, dtype=dtype)\n",
        "    z = torch.cat([q0, v0, theta0], dim=-1)\n",
        "\n",
        "    P = torch.diag(torch.tensor([1e-2,1e-2, 1e-1,1e-1, 1e-2], device=device, dtype=dtype)).unsqueeze(0)\n",
        "    phi_buf = torch.zeros(1, cfg.W, d_in, device=device, dtype=dtype)\n",
        "\n",
        "    zhist = torch.zeros(T, nz, device=device, dtype=dtype)\n",
        "    Phist = torch.zeros(T, nz, nz, device=device, dtype=dtype)\n",
        "    p = torch.zeros(T, 1, device=device, dtype=dtype)\n",
        "    nis = torch.zeros(T, 1, device=device, dtype=dtype)\n",
        "\n",
        "    for t in range(T):\n",
        "        qv, rv, p_logit = net(phi_buf)\n",
        "        p_logit = torch.nan_to_num(p_logit, nan=0.0, posinf=0.0, neginf=0.0).clamp(-20.0, 20.0)\n",
        "        p_t = torch.sigmoid(p_logit)\n",
        "\n",
        "        Lq = vec_to_cholesky(qv, nx)\n",
        "        Qx = chol_to_spd(Lq)\n",
        "        Lr = vec_to_cholesky(rv, ny)\n",
        "        R = chol_to_spd(Lr)\n",
        "\n",
        "        qtheta = (1.0 - p_t) * cfg.Qtheta_base + p_t * cfg.Qtheta_jump\n",
        "        Qt = qtheta.view(1,1,1)\n",
        "        Qz = blockdiag(Qx, Qt)\n",
        "\n",
        "        z, P, e, S = ukf_step(\n",
        "            z, P, y[t:t+1, :], Qz, R,\n",
        "            dt=cfg.dt, alpha=cfg.alpha, beta=cfg.beta, kappa=cfg.kappa,\n",
        "            jitter_P=cfg.jitter_P, jitter_S=cfg.jitter_S\n",
        "        )\n",
        "\n",
        "        # NIS = e^T S^{-1} e\n",
        "        Ls = safe_cholesky(S, jitter=cfg.jitter_S)\n",
        "        sol = torch.cholesky_solve(e.unsqueeze(-1), Ls)\n",
        "        quad = (e.unsqueeze(1) @ sol).squeeze(-1).squeeze(-1)  # (1,)\n",
        "        nis[t, 0] = quad[0]\n",
        "\n",
        "        phi_t = make_phi(e, S, jitter=cfg.jitter_S)\n",
        "        phi_buf = torch.cat([phi_buf[:, 1:, :], phi_t.unsqueeze(1)], dim=1)\n",
        "\n",
        "        zhist[t] = z[0]\n",
        "        Phist[t] = P[0]\n",
        "        p[t] = p_t[0]\n",
        "\n",
        "    return zhist, Phist, p, nis\n",
        "\n",
        "\n",
        "def _z_for_level(level: float) -> float:\n",
        "    # two-sided central interval\n",
        "    if level == 0.50: return 0.67448975\n",
        "    if level == 0.90: return 1.64485363\n",
        "    if level == 0.95: return 1.95996398\n",
        "    if level == 0.99: return 2.57582930\n",
        "    raise ValueError(\"Unsupported level\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_one(net: nn.Module, d: dict, cfg: CFG, levels=(0.50, 0.90, 0.99)):\n",
        "    y = d[\"y\"]\n",
        "    x_true = d[\"x_true\"]\n",
        "    theta_true = d[\"theta_true\"]\n",
        "    p_label = d[\"p\"]\n",
        "    kstar = d[\"kstar\"]\n",
        "\n",
        "    zhat, Phist, p, nis = rollout_full(net, y, cfg)\n",
        "    nx = cfg.nx\n",
        "\n",
        "    xhat = zhat[:, :nx]\n",
        "    thetahat = zhat[:, nx:nx+1]\n",
        "\n",
        "    # diag variances\n",
        "    Pxx = torch.diagonal(Phist[:, :nx, :nx], dim1=-2, dim2=-1)  # (T,nx)\n",
        "    Ptt = Phist[:, nx, nx].unsqueeze(-1)                        # (T,1)\n",
        "\n",
        "    # point errors\n",
        "    ex = xhat - x_true\n",
        "    et = thetahat - theta_true\n",
        "\n",
        "    rmse_x = torch.sqrt((ex**2).mean()).item()\n",
        "    rmse_theta = torch.sqrt((et**2).mean()).item()\n",
        "\n",
        "    # coverage (componentwise marginal)\n",
        "    cov = {}\n",
        "    for lv in levels:\n",
        "        zc = _z_for_level(lv)\n",
        "        cov[f\"x@{int(lv*100)}\"] = (ex.abs() <= zc*torch.sqrt(Pxx)).float().mean().item()\n",
        "        cov[f\"theta@{int(lv*100)}\"] = (et.abs() <= zc*torch.sqrt(Ptt)).float().mean().item()\n",
        "\n",
        "    # NEES (z) and NEES_theta (consistency checks for P)\n",
        "    dz = torch.cat([ex, et], dim=-1)  # (T,nz)\n",
        "    Lz = safe_cholesky(Phist, jitter=cfg.jitter_P)  # (T,nz,nz)\n",
        "    solz = torch.cholesky_solve(dz.unsqueeze(-1), Lz)  # (T,nz,1)\n",
        "    quadz = (dz.unsqueeze(1) @ solz).squeeze(-1).squeeze(-1)  # (T,)\n",
        "    nees = quadz.mean().item()\n",
        "    nees_theta = float(((et[:,0]**2) / (Ptt[:,0] + 1e-12)).mean().item())\n",
        "\n",
        "    # NIS mean\n",
        "    nis_mean = nis.mean().item()\n",
        "\n",
        "    # change detection metrics (AUC + first-detect delay)\n",
        "    # AUC via Mann-Whitney (no sklearn dependency)\n",
        "    y_true = p_label[:,0].detach().cpu().numpy().astype(np.int32)\n",
        "    y_score = p[:,0].detach().cpu().numpy()\n",
        "    n_pos = int(y_true.sum())\n",
        "    n_neg = int((1 - y_true).sum())\n",
        "    if n_pos == 0 or n_neg == 0:\n",
        "        auc = float(\"nan\")\n",
        "    else:\n",
        "        order = np.argsort(y_score)\n",
        "        ranks = np.empty_like(order, dtype=np.float64)\n",
        "        ranks[order] = np.arange(len(y_score), dtype=np.float64) + 1.0\n",
        "        sum_ranks_pos = ranks[y_true == 1].sum()\n",
        "        auc = (sum_ranks_pos - n_pos*(n_pos+1)/2.0) / (n_pos*n_neg)\n",
        "\n",
        "    # detection delay after k* (thresholded)\n",
        "    thr = 0.5\n",
        "    after = np.where(np.arange(cfg.T) >= kstar)[0]\n",
        "    det_idx = after[np.where(y_score[after] >= thr)[0]]\n",
        "    det_delay = int(det_idx[0] - kstar) if det_idx.size > 0 else None\n",
        "\n",
        "    return {\n",
        "        \"kstar\": kstar,\n",
        "        \"rmse_x\": rmse_x,\n",
        "        \"rmse_theta\": rmse_theta,\n",
        "        \"nees_mean\": nees,\n",
        "        \"nees_theta_mean\": nees_theta,\n",
        "        \"nis_mean\": nis_mean,\n",
        "        \"auc_p\": float(auc),\n",
        "        \"det_delay\": det_delay,\n",
        "        **cov\n",
        "    }, (zhat, Phist, p, nis)\n",
        "\n",
        "\n",
        "# --- Example: visualize one test sequence ---\n",
        "d = random.choice(test_data)\n",
        "metrics, (zhat, Phist, p, nis) = evaluate_one(net, d, cfg)\n",
        "\n",
        "kstar = d[\"kstar\"]\n",
        "x_true = d[\"x_true\"]\n",
        "theta_true = d[\"theta_true\"]\n",
        "nx = cfg.nx\n",
        "\n",
        "xhat = zhat[:, :nx]\n",
        "thetahat = zhat[:, nx:nx+1]\n",
        "Ptt = Phist[:, nx, nx].unsqueeze(-1)\n",
        "\n",
        "print(\"One-sequence metrics:\")\n",
        "for k,v in metrics.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# plot theta with 99% band around k*\n",
        "z99 = _z_for_level(0.99)\n",
        "theta_std = torch.sqrt(Ptt + 1e-12)\n",
        "\n",
        "w = 250\n",
        "a = max(0, kstar - w)\n",
        "b = min(cfg.T, kstar + w)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(theta_true[a:b,0].cpu().numpy(), label=\"theta true\")\n",
        "plt.plot(thetahat[a:b,0].cpu().numpy(), label=\"theta hat\")\n",
        "plt.fill_between(np.arange(a,b)-a,\n",
        "                 (thetahat[a:b,0] - z99*theta_std[a:b,0]).cpu().numpy(),\n",
        "                 (thetahat[a:b,0] + z99*theta_std[a:b,0]).cpu().numpy(),\n",
        "                 alpha=0.2, label=\"99% band\")\n",
        "plt.axvline(x=kstar-a)\n",
        "plt.title(\"Theta jump and estimate (zoomed) + 99% interval\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(p[a:b,0].cpu().numpy())\n",
        "plt.axvline(x=kstar-a)\n",
        "plt.title(\"p_k (change probability) (zoomed)\")\n",
        "plt.show()\n",
        "\n",
        "# --- Aggregate evaluation over multiple test sequences ---\n",
        "rows = []\n",
        "for d in test_data:\n",
        "    m, _ = evaluate_one(net, d, cfg)\n",
        "    rows.append(m)\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print(\"\\nTest summary (mean ± std):\")\n",
        "cols = [\"rmse_x\",\"rmse_theta\",\"nis_mean\",\"nees_mean\",\"nees_theta_mean\",\"auc_p\",\"x@50\",\"x@90\",\"x@99\",\"theta@50\",\"theta@90\",\"theta@99\"]\n",
        "for c in cols:\n",
        "    if c in df.columns:\n",
        "        mu = df[c].mean()\n",
        "        sd = df[c].std()\n",
        "        print(f\"  {c:12s}: {mu:.4f} ± {sd:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ukn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}