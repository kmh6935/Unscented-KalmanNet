{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b567c529",
   "metadata": {},
   "source": [
    "# Toy UKF + Causal Window Transformer NoiseNet (Full Cholesky Qx/R + pₖ→Qθ Gating)\n",
    "\n",
    "This notebook implements a **minimal end-to-end toy** for your setup:\n",
    "\n",
    "- **2-DOF** dynamics with state \\(x=[q_1,q_2,v_1,v_2]\\) (nx=4) and measurement \\(y=[q_1,q_2]\\) (ny=2)  \n",
    "- **Augmented state** \\(z=[x;\\theta]\\) with a **single parameter** \\(\\theta\\) scaling stiffness  \n",
    "- Sequence length **T=2000**, random change-point \\(k^*\\), and **+30% step jump** in \\(\\theta\\)  \n",
    "- A **causal Transformer** predicts **full SPD** \\(Q_{x,k}\\) and \\(R_k\\) via **Cholesky factors**, plus a change logit \\(p_k\\)  \n",
    "- \\(p_k\\) gates \\(Q_{\\theta,k}\\) inside the UKF predict covariance:\n",
    "  \\[\n",
    "  Q_{\\theta,k}=(1-\\sigma(p_k))Q_{\\theta}^{\\text{base}}+\\sigma(p_k)Q_{\\theta}^{\\text{jump}}\n",
    "  \\]\n",
    "- Training uses **sliding windows** with **burn-in**, **state supervision**, and **BCE supervision** for change detection.\n",
    "\n",
    "> Tip: Start with the default small training settings to sanity-check. Then increase epochs/steps to improve adaptation speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa0c772c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.10.0+cu128\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Setup ---\n",
    "from __future__ import annotations\n",
    "import math, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccccf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG(device='cuda', dtype=torch.float32, nx=4, ny=2, nt=1, nz=5, dt=0.02, T=2000, jump_ratio=0.3, kstar_lo=0.3, kstar_hi=0.7, alpha=1.0, beta=2.0, kappa=0.0, jitter_P=0.001, jitter_S=1e-06, W=128, d_model=128, n_layers=2, n_heads=4, dropout=0.1, burn_in=128, L=256, batch_size=16, n_train_seq=256, n_val_seq=64, n_test_seq=64, lr=0.0003, epochs=3, steps_per_epoch=100, w_state=1.0, w_bce=0.2, w_smooth=0.0001, w_offdiag=0.0001, Qtheta_base=1e-08, Qtheta_jump=0.0001, p_label_width=25)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Config ---\n",
    "@dataclass\n",
    "class CFG:\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype: torch.dtype = torch.float32\n",
    "\n",
    "    # problem dims\n",
    "    nx: int = 4\n",
    "    ny: int = 2\n",
    "    nt: int = 1\n",
    "    nz: int = 5\n",
    "\n",
    "    # simulation\n",
    "    dt: float = 0.02\n",
    "    T: int = 2000\n",
    "    jump_ratio: float = 0.30\n",
    "    kstar_lo: float = 0.3\n",
    "    kstar_hi: float = 0.7\n",
    "\n",
    "    # UKF params\n",
    "    alpha: float = 1.0\n",
    "    beta: float = 2.0\n",
    "    kappa: float = 0.0\n",
    "    jitter_P: float = 1e-3\n",
    "    jitter_S: float = 1e-6\n",
    "\n",
    "    # NoiseNet / Transformer\n",
    "    W: int = 128              # sliding window length for features (past-only)\n",
    "    d_model: int = 128\n",
    "    n_layers: int = 2\n",
    "    n_heads: int = 4\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # Training windows (burn-in + loss)\n",
    "    burn_in: int = 128\n",
    "    L: int = 256\n",
    "\n",
    "    # Dataset sizes\n",
    "    batch_size: int = 32\n",
    "\n",
    "    n_train_seq: int = 128*2\n",
    "    n_val_seq: int = 32*2\n",
    "    n_test_seq: int = 32*2\n",
    "\n",
    "    # Training (keep small for sanity-check; increase later)\n",
    "    lr: float = 3e-4\n",
    "    epochs: int = 3\n",
    "    steps_per_epoch: int = 100\n",
    "\n",
    "    # Loss weights\n",
    "    w_state: float = 1.0\n",
    "    w_bce: float = 0.2\n",
    "    w_smooth: float = 1e-4\n",
    "    w_offdiag: float = 1e-4\n",
    "\n",
    "    # Q_theta gating (variance)\n",
    "    Qtheta_base: float = 1e-8\n",
    "    Qtheta_jump: float = 1e-4   # increase (e.g., 1e-3) for faster adaptation if stable\n",
    "\n",
    "    # label width after change (p_k uses features up to k-1, so label starts at k*+1)\n",
    "    p_label_width: int = 25\n",
    "\n",
    "cfg = CFG()\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b4f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Utilities: Cholesky parameterization for full SPD Qx and R ---\n",
    "\n",
    "def n_tril(n: int) -> int:\n",
    "    return n * (n + 1) // 2\n",
    "\n",
    "\n",
    "def vec_to_cholesky(v: torch.Tensor, n: int, eps: float = 1e-4,\n",
    "                    diag_min: float = -12.0, diag_max: float = 6.0,\n",
    "                    off_clip: float = 3.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    v: (..., n_tril(n)) lower-triangular params\n",
    "       convention: first n entries -> diag logits, remaining -> strict-lower row-wise\n",
    "    returns L: (..., n, n) with positive diag (via softplus)\n",
    "\n",
    "    Safety:\n",
    "      - nan/inf -> 0\n",
    "      - diag logits clamped to [diag_min, diag_max]\n",
    "      - off-diagonals clipped to [-off_clip, off_clip]\n",
    "    \"\"\"\n",
    "    v = torch.nan_to_num(v, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    diag_logits = torch.clamp(v[..., :n], min=diag_min, max=diag_max)\n",
    "    off = torch.clamp(v[..., n:], min=-off_clip, max=off_clip)\n",
    "\n",
    "    L = v.new_zeros(*v.shape[:-1], n, n)\n",
    "    diag = F.softplus(diag_logits) + eps\n",
    "    idx = torch.arange(n, device=v.device)\n",
    "    L[..., idx, idx] = diag\n",
    "\n",
    "    k = 0\n",
    "    for i in range(1, n):\n",
    "        for j in range(i):\n",
    "            L[..., i, j] = off[..., k]\n",
    "            k += 1\n",
    "    return L\n",
    "\n",
    "\n",
    "def chol_to_spd(L: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    n = L.shape[-1]\n",
    "    I = torch.eye(n, device=L.device, dtype=L.dtype)\n",
    "    return L @ L.transpose(-1, -2) + eps * I\n",
    "\n",
    "def blockdiag(Qx: torch.Tensor, Qt: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Qx: (B,nx,nx), Qt: (B,nt,nt) -> Qz: (B,nz,nz)\n",
    "    \"\"\"\n",
    "    B, nx, _ = Qx.shape\n",
    "    _, nt, _ = Qt.shape\n",
    "    Qz = Qx.new_zeros(B, nx + nt, nx + nt)\n",
    "    Qz[:, :nx, :nx] = Qx\n",
    "    Qz[:, nx:, nx:] = Qt\n",
    "    return Qz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bceaa514",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Physics: 2-DOF model (theta scales the coupling stiffness) ---\n",
    "\n",
    "def build_mck(theta: torch.Tensor, device, dtype) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    theta: (B,1) stiffness scale for coupling spring\n",
    "    Returns M,C,K for each batch as (B,2,2)\n",
    "    \"\"\"\n",
    "    B = theta.shape[0]\n",
    "    m1, m2 = 1.0, 1.0\n",
    "    c1, c2, cc = 0.05, 0.05, 0.02\n",
    "    k1, k2 = 20.0, 20.0\n",
    "    kc0 = 15.0\n",
    "    kc = kc0 * theta.squeeze(-1)  # (B,)\n",
    "\n",
    "    M = torch.tensor([[m1, 0.0],[0.0, m2]], device=device, dtype=dtype).expand(B,2,2).clone()\n",
    "\n",
    "    # Damping: C = [[c1+cc, -cc],[-cc, c2+cc]]\n",
    "    C = torch.zeros(B,2,2, device=device, dtype=dtype)\n",
    "    C[:,0,0] = c1 + cc\n",
    "    C[:,1,1] = c2 + cc\n",
    "    C[:,0,1] = -cc\n",
    "    C[:,1,0] = -cc\n",
    "\n",
    "    # Stiffness: K = [[k1+kc, -kc],[-kc, k2+kc]]\n",
    "    K = torch.zeros(B,2,2, device=device, dtype=dtype)\n",
    "    K[:,0,0] = k1 + kc\n",
    "    K[:,1,1] = k2 + kc\n",
    "    K[:,0,1] = -kc\n",
    "    K[:,1,0] = -kc\n",
    "    return M, C, K\n",
    "\n",
    "def f_step(z: torch.Tensor, dt: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    z: (B, nz) = [q1,q2,v1,v2,theta]\n",
    "    deterministic transition (no noise added here)\n",
    "    \"\"\"\n",
    "    device, dtype = z.device, z.dtype\n",
    "    q = z[:, 0:2]            # (B,2)\n",
    "    v = z[:, 2:4]            # (B,2)\n",
    "    theta = z[:, 4:5]        # (B,1)\n",
    "\n",
    "    M, C, K = build_mck(theta, device, dtype)\n",
    "    Minv = torch.linalg.inv(M)\n",
    "\n",
    "    a = -(Minv @ (C @ v.unsqueeze(-1) + K @ q.unsqueeze(-1))).squeeze(-1)  # (B,2)\n",
    "\n",
    "    q_next = q + dt * v\n",
    "    v_next = v + dt * a\n",
    "    theta_next = theta  # random-walk handled via Q_theta in the filter\n",
    "\n",
    "    return torch.cat([q_next, v_next, theta_next], dim=-1)\n",
    "\n",
    "def h_meas(z: torch.Tensor) -> torch.Tensor:\n",
    "    # measurement: y = [q1,q2]\n",
    "    return z[:, 0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28670f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# --- Simulation: T=2000 with random k* and +30% step jump in theta ---\n",
    "\n",
    "@torch.no_grad()\n",
    "def simulate_sequence(T: int, dt: float, jump_ratio: float, kstar: int, device, dtype):\n",
    "    \"\"\"\n",
    "    Generates x_true (T,nx), theta_true (T,1), y (T,ny)\n",
    "    \"\"\"\n",
    "    theta0 = 1.0\n",
    "    theta1 = theta0 * (1.0 + jump_ratio)  # +30% at/after k*\n",
    "    theta_true = torch.full((T,1), theta0, device=device, dtype=dtype)\n",
    "    theta_true[kstar:, 0] = theta1\n",
    "\n",
    "    # initial state\n",
    "    q0 = torch.tensor([0.2, -0.1], device=device, dtype=dtype)\n",
    "    v0 = torch.tensor([0.0, 0.0], device=device, dtype=dtype)\n",
    "    x = torch.cat([q0, v0]).unsqueeze(0)  # (1,4)\n",
    "\n",
    "    # simulation noises\n",
    "    Qsim = torch.diag(torch.tensor([1e-6,1e-6,1e-5,1e-5], device=device, dtype=dtype))\n",
    "    Lsim = torch.linalg.cholesky(Qsim)\n",
    "\n",
    "    Rsim = torch.diag(torch.tensor([2e-4, 2e-4], device=device, dtype=dtype))\n",
    "    Lr = torch.linalg.cholesky(Rsim)\n",
    "\n",
    "    x_true = torch.zeros(T,4, device=device, dtype=dtype)\n",
    "    y = torch.zeros(T,2, device=device, dtype=dtype)\n",
    "\n",
    "    for k in range(T):\n",
    "        th = theta_true[k:k+1]  # (1,1)\n",
    "        z = torch.cat([x, th], dim=-1)  # (1,5)\n",
    "\n",
    "        z_next = f_step(z, dt=dt)\n",
    "        x_next = z_next[:, :4]\n",
    "        x_next = x_next + (torch.randn(1,4, device=device, dtype=dtype) @ Lsim.T)\n",
    "        x = x_next\n",
    "\n",
    "        x_true[k] = x.squeeze(0)\n",
    "        y[k] = x_true[k, :2] + (torch.randn(2, device=device, dtype=dtype) @ Lr.T)\n",
    "\n",
    "    return x_true, theta_true, y\n",
    "\n",
    "def make_p_label(T: int, kstar: int, width: int, device, dtype):\n",
    "    # label starts at kstar+1 because p_k is predicted from phi up to k-1\n",
    "    start = min(T-1, kstar + 1)\n",
    "    end = min(T, start + width)\n",
    "    p = torch.zeros(T,1, device=device, dtype=dtype)\n",
    "    p[start:end, 0] = 1.0\n",
    "    return p\n",
    "\n",
    "# quick sanity-check visualization of one simulated sequence\n",
    "device, dtype = cfg.device, cfg.dtype\n",
    "kstar = random.randint(int(cfg.kstar_lo*cfg.T), int(cfg.kstar_hi*cfg.T))\n",
    "x_true, theta_true, y = simulate_sequence(cfg.T, cfg.dt, cfg.jump_ratio, kstar, device, dtype)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(theta_true.cpu().numpy())\n",
    "plt.title(f\"True theta (k*={kstar})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d157200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def safe_cholesky(A: torch.Tensor, jitter: float = 1e-6, max_tries: int = 8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Robust Cholesky with increasing diagonal loading + nan/inf sanitization.\n",
    "    \"\"\"\n",
    "    A = torch.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    A = 0.5 * (A + A.transpose(-1,-2))\n",
    "    n = A.shape[-1]\n",
    "    I = torch.eye(n, device=A.device, dtype=A.dtype).unsqueeze(0)\n",
    "    for i in range(max_tries):\n",
    "        try:\n",
    "            return torch.linalg.cholesky(A + (jitter * (10.0**i)) * I)\n",
    "        except Exception:\n",
    "            continue\n",
    "    # fallback: eigen projection\n",
    "    w, V = torch.linalg.eigh(A)\n",
    "    w = torch.clamp(w, min=jitter)\n",
    "    A_spd = V @ torch.diag_embed(w) @ V.transpose(-1,-2)\n",
    "    return torch.linalg.cholesky(A_spd + jitter*I)\n",
    "\n",
    "\n",
    "\n",
    "# --- Innovation features phi_k = [L^{-1}e, |L^{-1}e|, NIS, logdetS] ---\n",
    "\n",
    "def make_phi(e: torch.Tensor, S: torch.Tensor, jitter: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    e: (B,ny)\n",
    "    S: (B,ny,ny) SPD\n",
    "    returns phi: (B, d_in) = [e_white, |e_white|, nis, logdetS]\n",
    "    \"\"\"\n",
    "    B, ny = e.shape\n",
    "    I = torch.eye(ny, device=e.device, dtype=e.dtype).unsqueeze(0)\n",
    "    S = S + jitter * I\n",
    "    L = safe_cholesky(S, jitter=jitter)  # (B,ny,ny)\n",
    "    # e_white = L^{-1} e\n",
    "    e_white = torch.linalg.solve_triangular(L, e.unsqueeze(-1), upper=False).squeeze(-1)\n",
    "    nis = (e_white**2).sum(dim=-1, keepdim=True)\n",
    "    logdetS = 2.0 * torch.log(torch.diagonal(L, dim1=-2, dim2=-1)).sum(dim=-1, keepdim=True)\n",
    "    phi = torch.cat([e_white, e_white.abs(), nis, logdetS], dim=-1)\n",
    "    return phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515f7b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sanitize_sym(A: torch.Tensor) -> torch.Tensor:\n",
    "    A = torch.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return 0.5 * (A + A.transpose(-1,-2))\n",
    "\n",
    "def make_spd(A: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Project a symmetric matrix to SPD by clamping eigenvalues.\n",
    "    \"\"\"\n",
    "    A = sanitize_sym(A)\n",
    "    w, V = torch.linalg.eigh(A)\n",
    "    w = torch.clamp(w, min=eps)\n",
    "    A_spd = V @ torch.diag_embed(w) @ V.transpose(-1,-2)\n",
    "    n = A.shape[-1]\n",
    "    I = torch.eye(n, device=A.device, dtype=A.dtype)\n",
    "    return A_spd + eps * I\n",
    "\n",
    "\n",
    "def sqrtm_spd(A: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Symmetric matrix square root for (approx) SPD matrices using eigendecomposition.\n",
    "    Returns U such that U U^T ≈ A. Works even when Cholesky fails.\n",
    "    \"\"\"\n",
    "    A = sanitize_sym(A)\n",
    "    w, V = torch.linalg.eigh(A)\n",
    "    w = torch.clamp(w, min=eps)\n",
    "    return V @ torch.diag_embed(torch.sqrt(w))\n",
    "\n",
    "\n",
    "def safe_cholesky(A: torch.Tensor, jitter: float = 1e-6, max_tries: int = 8) -> torch.Tensor:\n",
    "    A = sanitize_sym(A)\n",
    "    n = A.shape[-1]\n",
    "    I = torch.eye(n, device=A.device, dtype=A.dtype).unsqueeze(0)\n",
    "    for i in range(max_tries):\n",
    "        try:\n",
    "            return torch.linalg.cholesky(A + (jitter * (10.0**i)) * I)\n",
    "        except Exception:\n",
    "            continue\n",
    "    # fallback: eigen projection then cholesky\n",
    "    A = make_spd(A, eps=jitter)\n",
    "    return torch.linalg.cholesky(A + jitter * I)\n",
    "\n",
    "\n",
    "\n",
    "def make_spd(A: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Project a symmetric matrix to SPD by clamping eigenvalues.\n",
    "    A: (B,n,n)\n",
    "    \"\"\"\n",
    "    A = 0.5 * (A + A.transpose(-1, -2))\n",
    "    w, V = torch.linalg.eigh(A)\n",
    "    w = torch.clamp(w, min=eps)\n",
    "    return V @ torch.diag_embed(w) @ V.transpose(-1, -2)\n",
    "\n",
    "\n",
    "\n",
    "# --- UKF step (batched) ---\n",
    "\n",
    "def ukf_step(z: torch.Tensor, P: torch.Tensor, y: torch.Tensor, Qz: torch.Tensor, R: torch.Tensor,\n",
    "            dt: float, alpha: float, beta: float, kappa: float,\n",
    "            jitter_P: float, jitter_S: float):\n",
    "    \"\"\"\n",
    "    z: (B,nz) mean\n",
    "    P: (B,nz,nz)\n",
    "    y: (B,ny)\n",
    "    Qz: (B,nz,nz)\n",
    "    R:  (B,ny,ny)\n",
    "    Returns z_upd, P_upd, innovation e, S\n",
    "    \"\"\"\n",
    "    device, dtype = z.device, z.dtype\n",
    "    B, nz = z.shape\n",
    "    ny = y.shape[-1]\n",
    "\n",
    "    lam = alpha**2 * (nz + kappa) - nz\n",
    "    c = nz + lam\n",
    "    Wm0 = lam / c\n",
    "    Wc0 = Wm0 + (1 - alpha**2 + beta)\n",
    "    W = 1.0 / (2.0 * c)\n",
    "\n",
    "    # cholesky of P\n",
    "    I = torch.eye(nz, device=device, dtype=dtype).unsqueeze(0)\n",
    "    Pj = make_spd(P, eps=jitter_P)\n",
    "    U = sqrtm_spd(Pj, eps=jitter_P)  # robust matrix sqrt (no Cholesky)\n",
    "    U = U * math.sqrt(c)\n",
    "\n",
    "    # sigma points\n",
    "    sigmas = z.unsqueeze(1).repeat(1, 2*nz+1, 1)\n",
    "    Ucols = U.transpose(-1, -2)  # each row = a column of U\n",
    "    sigmas[:, 1:nz+1, :] = z.unsqueeze(1) + Ucols\n",
    "    sigmas[:, nz+1:,  :] = z.unsqueeze(1) - Ucols\n",
    "\n",
    "    # propagate through dynamics\n",
    "    sig_flat = sigmas.reshape(B*(2*nz+1), nz)\n",
    "    zprop = f_step(sig_flat, dt=dt).reshape(B, 2*nz+1, nz)\n",
    "\n",
    "    # weights\n",
    "    wm = torch.full((2*nz+1,), W, device=device, dtype=dtype)\n",
    "    wc = torch.full((2*nz+1,), W, device=device, dtype=dtype)\n",
    "    wm[0] = Wm0\n",
    "    wc[0] = Wc0\n",
    "\n",
    "    # predicted mean/cov\n",
    "    z_pred = (zprop * wm.view(1,-1,1)).sum(dim=1)\n",
    "    dz = zprop - z_pred.unsqueeze(1)\n",
    "    P_pred = torch.einsum(\"i,bij,bik->bjk\", wc, dz, dz) + Qz\n",
    "\n",
    "    # predicted measurement\n",
    "    yprop = h_meas(zprop.reshape(B*(2*nz+1), nz)).reshape(B, 2*nz+1, ny)\n",
    "    y_pred = (yprop * wm.view(1,-1,1)).sum(dim=1)\n",
    "    dy = yprop - y_pred.unsqueeze(1)\n",
    "    S = torch.einsum(\"i,bij,bik->bjk\", wc, dy, dy) + R\n",
    "    Pzy = torch.einsum(\"i,bij,bik->bjk\", wc, dz, dy)\n",
    "\n",
    "    # stabilize S\n",
    "    I_y = torch.eye(ny, device=device, dtype=dtype).unsqueeze(0)\n",
    "    S = S + jitter_S * I_y\n",
    "    S = make_spd(S, eps=jitter_S)\n",
    "\n",
    "    # gain and update\n",
    "    Ls = safe_cholesky(S, jitter=jitter_S)\n",
    "    # K = Pzy @ S^{-1} using Cholesky solves\n",
    "    tmp = torch.linalg.solve_triangular(Ls, Pzy.transpose(-1,-2), upper=False)\n",
    "    tmp = torch.linalg.solve_triangular(Ls.transpose(-1,-2), tmp, upper=True)\n",
    "    K = tmp.transpose(-1,-2)\n",
    "    e = y - y_pred\n",
    "    z_upd = z_pred + torch.einsum(\"bij,bj->bi\", K, e)\n",
    "    P_upd = P_pred - K @ S @ K.transpose(-1,-2)\n",
    "    P_upd = 0.5 * (P_upd + P_upd.transpose(-1,-2))\n",
    "    P_upd = make_spd(P_upd, eps=jitter_P)\n",
    "    return z_upd, P_upd, e, S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "785a89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Causal window Transformer NoiseNet ---\n",
    "# It predicts Qx_k, R_k (full SPD via Cholesky factors) and a change logit p_k.\n",
    "# It only sees past features (phi up to k-1), so the whole pipeline is causal.\n",
    "\n",
    "class CausalWindowTransformerNoiseNet(nn.Module):\n",
    "    def __init__(self, d_in: int, d_model: int, nx: int, ny: int,\n",
    "                 W: int, n_layers: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.nx, self.ny = nx, ny\n",
    "        self.nQ = n_tril(nx)\n",
    "        self.nR = n_tril(ny)\n",
    "        self.d_out = self.nQ + self.nR + 1\n",
    "\n",
    "        self.in_proj = nn.Linear(d_in, d_model)\n",
    "        self.pos = nn.Parameter(torch.zeros(1, W, d_model))\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=4*d_model,\n",
    "            dropout=dropout, batch_first=True, activation=\"gelu\", norm_first=True\n",
    "        )\n",
    "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.head = nn.Linear(d_model, self.d_out)\n",
    "\n",
    "        # initialize to small-ish Q/R (stability)\n",
    "        with torch.no_grad():\n",
    "            self.head.weight.mul_(0.01)\n",
    "            self.head.bias.zero_()\n",
    "            self.head.bias[:nx] = -6.0               # Q diag logits\n",
    "            self.head.bias[self.nQ:self.nQ+ny] = -6.0 # R diag logits\n",
    "\n",
    "    def forward(self, phi_seq: torch.Tensor):\n",
    "        # phi_seq: (B,W,d_in)\n",
    "        B, W, _ = phi_seq.shape\n",
    "        h = self.in_proj(phi_seq) + self.pos[:, :W, :]\n",
    "        h = self.enc(h)\n",
    "        out = self.head(h[:, -1, :])  # last token as \"current\" prediction\n",
    "\n",
    "        qv = out[:, :self.nQ]\n",
    "        rv = out[:, self.nQ:self.nQ+self.nR]\n",
    "        p_logit = out[:, -1:]\n",
    "        return qv, rv, p_logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14f45d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets from ./data...\n",
      "Data loaded successfully.\n",
      "Dataset Sizes -> Train: 256, Val: 64, Test: 64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Dataset creation (in-memory): random k* per sequence ---\n",
    "# --- 데이터 저장 경로 설정 ---\n",
    "DATA_DIR = \"./data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"train_data.pt\")\n",
    "VAL_FILE = os.path.join(DATA_DIR, \"val_data.pt\")\n",
    "TEST_FILE = os.path.join(DATA_DIR, \"test_data.pt\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_dataset(n_seq: int, cfg: CFG):\n",
    "    dev, dt = cfg.device, cfg.dtype\n",
    "    data = []\n",
    "    for _ in range(n_seq):\n",
    "        kstar = random.randint(int(cfg.kstar_lo * cfg.T), int(cfg.kstar_hi * cfg.T))\n",
    "        x_true, theta_true, y = simulate_sequence(cfg.T, cfg.dt, cfg.jump_ratio, kstar, dev, dt)\n",
    "        p = make_p_label(cfg.T, kstar, cfg.p_label_width, dev, dt)\n",
    "        data.append({\"x_true\": x_true, \"theta_true\": theta_true, \"y\": y, \"p\": p, \"kstar\": kstar})\n",
    "    return data\n",
    "\n",
    "def sample_batch(data, cfg: CFG):\n",
    "    B = cfg.batch_size\n",
    "    T = cfg.T\n",
    "    seg = cfg.burn_in + cfg.L\n",
    "    idx = random.sample(range(len(data)), B)\n",
    "    s_max = T - seg - 1\n",
    "    starts = [random.randint(0, s_max) for _ in range(B)]\n",
    "\n",
    "    y = []\n",
    "    x = []\n",
    "    p = []\n",
    "    for i, s in zip(idx, starts):\n",
    "        d = data[i]\n",
    "        y.append(d[\"y\"][s:s+seg])\n",
    "        x.append(d[\"x_true\"][s:s+seg])\n",
    "        p.append(d[\"p\"][s:s+seg])\n",
    "\n",
    "    return torch.stack(y, 0), torch.stack(x, 0), torch.stack(p, 0)\n",
    "\n",
    "def prepare_datasets(cfg, force_generate=False):\n",
    "    \"\"\"\n",
    "    데이터 파일이 있으면 로드하고, 없으면 생성 후 저장합니다.\n",
    "    force_generate=True로 설정하면 무조건 새로 생성하고 덮어씁니다.\n",
    "    \"\"\"\n",
    "    # 1. 파일이 모두 존재하고, 강제 생성이 아닐 경우 -> 로드\n",
    "    if os.path.exists(TRAIN_FILE) and os.path.exists(VAL_FILE) and os.path.exists(TEST_FILE) and not force_generate:\n",
    "        print(f\"Loading datasets from {DATA_DIR}...\")\n",
    "        train_data = torch.load(TRAIN_FILE)\n",
    "        val_data = torch.load(VAL_FILE)\n",
    "        test_data = torch.load(TEST_FILE)\n",
    "        print(\"Data loaded successfully.\")\n",
    "    \n",
    "    # 2. 파일이 없거나 강제 생성일 경우 -> 생성 및 저장\n",
    "    else:\n",
    "        print(\"Generating new datasets...\")\n",
    "        # (1) Training Data\n",
    "        print(f\"  - Building Train ({cfg.n_train_seq} seq)...\")\n",
    "        train_data = build_dataset(cfg.n_train_seq, cfg)\n",
    "        \n",
    "        # (2) Validation Data\n",
    "        print(f\"  - Building Val ({cfg.n_val_seq} seq)...\")\n",
    "        val_data = build_dataset(cfg.n_val_seq, cfg)\n",
    "        \n",
    "        # (3) Testing Data (새로 추가됨)\n",
    "        print(f\"  - Building Test ({cfg.n_test_seq} seq)...\")\n",
    "        test_data = build_dataset(cfg.n_test_seq, cfg)\n",
    "        \n",
    "        # 저장\n",
    "        print(f\"Saving datasets to {DATA_DIR}...\")\n",
    "        torch.save(train_data, TRAIN_FILE)\n",
    "        torch.save(val_data, VAL_FILE)\n",
    "        torch.save(test_data, TEST_FILE)\n",
    "        print(\"Data generation and saving complete.\")\n",
    "        \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# --- 실행 ---\n",
    "# force_generate=True로 하면 기존 파일을 무시하고 새로 만듭니다.\n",
    "train_data, val_data, test_data = prepare_datasets(cfg, force_generate=False)\n",
    "\n",
    "print(f\"Dataset Sizes -> Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a10c3ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Differentiable rollout on one (burn-in + loss) window ---\n",
    "\n",
    "def rollout_window(net: nn.Module, y_seg: torch.Tensor, x_true_seg: torch.Tensor, p_label_seg: torch.Tensor, cfg: CFG):\n",
    "    \"\"\"\n",
    "    y_seg: (B,Tseg,ny), x_true_seg: (B,Tseg,nx), p_label_seg: (B,Tseg,1)\n",
    "    Returns dict(losses, trajectories)\n",
    "    \"\"\"\n",
    "    device, dtype = y_seg.device, y_seg.dtype\n",
    "    B, Tseg, ny = y_seg.shape\n",
    "    nx, nz = cfg.nx, cfg.nz\n",
    "\n",
    "    d_in = 2*ny + 2\n",
    "\n",
    "    # init z and P\n",
    "    q0 = y_seg[:, 0, :]\n",
    "    v0 = torch.zeros(B,2, device=device, dtype=dtype)\n",
    "    theta0 = torch.ones(B,1, device=device, dtype=dtype)\n",
    "    z = torch.cat([q0, v0, theta0], dim=-1)\n",
    "\n",
    "    P = torch.diag(torch.tensor([1e-2,1e-2, 1e-1,1e-1, 1e-2], device=device, dtype=dtype)).unsqueeze(0).repeat(B,1,1)\n",
    "\n",
    "    # feature buffer: past-only features\n",
    "    phi_buf = torch.zeros(B, cfg.W, d_in, device=device, dtype=dtype)\n",
    "\n",
    "    xhat, thetahat, ppred = [], [], []\n",
    "    qv_hist, rv_hist = [], []\n",
    "\n",
    "    for t in range(Tseg):\n",
    "        # 1) predict Qx_k, R_k, p_k from past feature window\n",
    "        qv, rv, p_logit = net(phi_buf)\n",
    "        p = torch.sigmoid(p_logit)\n",
    "\n",
    "        # 2) build full SPD Qx and R via Cholesky\n",
    "        Lq = vec_to_cholesky(qv, nx)\n",
    "        Qx = chol_to_spd(Lq)\n",
    "        Lr = vec_to_cholesky(rv, ny)\n",
    "        R = chol_to_spd(Lr)\n",
    "\n",
    "        # 3) gate Q_theta,k using p_k (THIS is the key location)\n",
    "        qtheta = (1.0 - p) * cfg.Qtheta_base + p * cfg.Qtheta_jump\n",
    "        Qt = qtheta.view(B,1,1)\n",
    "        Qz = blockdiag(Qx, Qt)\n",
    "\n",
    "        # 4) UKF update\n",
    "        z, P, e, S = ukf_step(\n",
    "            z, P, y_seg[:, t, :], Qz, R,\n",
    "            dt=cfg.dt, alpha=cfg.alpha, beta=cfg.beta, kappa=cfg.kappa,\n",
    "            jitter_P=cfg.jitter_P, jitter_S=cfg.jitter_S\n",
    "        )\n",
    "\n",
    "        # 5) create phi_t and append to buffer (for next step)\n",
    "        phi_t = make_phi(e, S, jitter=cfg.jitter_S)\n",
    "        phi_buf = torch.cat([phi_buf[:, 1:, :], phi_t.unsqueeze(1)], dim=1)\n",
    "\n",
    "        xhat.append(z[:, :nx])\n",
    "        thetahat.append(z[:, nx:nx+1])\n",
    "        ppred.append(p_logit)\n",
    "        qv_hist.append(qv)\n",
    "        rv_hist.append(rv)\n",
    "\n",
    "    xhat = torch.stack(xhat, 1)\n",
    "    thetahat = torch.stack(thetahat, 1)\n",
    "    ppred = torch.stack(ppred, 1)\n",
    "    qv_hist = torch.stack(qv_hist, 1)\n",
    "    rv_hist = torch.stack(rv_hist, 1)\n",
    "\n",
    "    start = cfg.burn_in\n",
    "    end = cfg.burn_in + cfg.L\n",
    "\n",
    "    x_loss = F.mse_loss(xhat[:, start:end, :], x_true_seg[:, start:end, :])\n",
    "    bce = F.binary_cross_entropy_with_logits(ppred[:, start:end, :], p_label_seg[:, start:end, :])\n",
    "\n",
    "    dq = (qv_hist[:, start+1:end, :] - qv_hist[:, start:end-1, :]).pow(2).mean()\n",
    "    dr = (rv_hist[:, start+1:end, :] - rv_hist[:, start:end-1, :]).pow(2).mean()\n",
    "    smooth = dq + dr\n",
    "\n",
    "    q_off = qv_hist[:, start:end, nx:]\n",
    "    r_off = rv_hist[:, start:end, ny:]\n",
    "    offdiag = (q_off.pow(2).mean() + r_off.pow(2).mean())\n",
    "\n",
    "    loss = cfg.w_state*x_loss + cfg.w_bce*bce + cfg.w_smooth*smooth + cfg.w_offdiag*offdiag\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"x_loss\": x_loss.detach(),\n",
    "        \"bce\": bce.detach(),\n",
    "        \"smooth\": smooth.detach(),\n",
    "        \"offdiag\": offdiag.detach(),\n",
    "        \"xhat\": xhat.detach(),\n",
    "        \"thetahat\": thetahat.detach(),\n",
    "        \"ppred\": ppred.detach(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4605cc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minhy\\AppData\\Local\\Temp\\ipykernel_11812\\4182933182.py:21: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  self.enc = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n"
     ]
    },
    {
     "ename": "_LinAlgError",
     "evalue": "linalg.cholesky: (Batch element 1): The factorization could not be completed because the input is not positive-definite (the leading minor of order 2 is not positive-definite).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_LinAlgError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg.steps_per_epoch):\n\u001b[32m     19\u001b[39m     y_seg, x_seg, p_seg = sample_batch(train_data, cfg)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     out = \u001b[43mrollout_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_seg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_seg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_seg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     opt.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     23\u001b[39m     out[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m].backward()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mrollout_window\u001b[39m\u001b[34m(net, y_seg, x_true_seg, p_label_seg, cfg)\u001b[39m\n\u001b[32m     42\u001b[39m Qz = blockdiag(Qx, Qt)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# 4) UKF update\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m z, P, e, S = \u001b[43mukf_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_seg\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkappa\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkappa\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjitter_P\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjitter_P\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjitter_S\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjitter_S\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# 5) create phi_t and append to buffer (for next step)\u001b[39;00m\n\u001b[32m     52\u001b[39m phi_t = make_phi(e, S, jitter=cfg.jitter_S)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 119\u001b[39m, in \u001b[36mukf_step\u001b[39m\u001b[34m(z, P, y, Qz, R, dt, alpha, beta, kappa, jitter_P, jitter_S)\u001b[39m\n\u001b[32m    116\u001b[39m S = make_spd(S, eps=jitter_S)\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# gain and update\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m Ls = \u001b[43msafe_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjitter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjitter_S\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# K = Pzy @ S^{-1} using Cholesky solves\u001b[39;00m\n\u001b[32m    121\u001b[39m tmp = torch.linalg.solve_triangular(Ls, Pzy.transpose(-\u001b[32m1\u001b[39m,-\u001b[32m2\u001b[39m), upper=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36msafe_cholesky\u001b[39m\u001b[34m(A, jitter, max_tries)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# fallback: eigen projection then cholesky\u001b[39;00m\n\u001b[32m     39\u001b[39m A = make_spd(A, eps=jitter)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mjitter\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mI\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31m_LinAlgError\u001b[39m: linalg.cholesky: (Batch element 1): The factorization could not be completed because the input is not positive-definite (the leading minor of order 2 is not positive-definite)."
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Train ---\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "d_in = 2*cfg.ny + 2\n",
    "net = CausalWindowTransformerNoiseNet(\n",
    "    d_in=d_in, d_model=cfg.d_model, nx=cfg.nx, ny=cfg.ny,\n",
    "    W=cfg.W, n_layers=cfg.n_layers, n_heads=cfg.n_heads, dropout=cfg.dropout\n",
    ").to(cfg.device)\n",
    "\n",
    "opt = torch.optim.AdamW(net.parameters(), lr=cfg.lr, weight_decay=1e-4)\n",
    "\n",
    "for ep in range(cfg.epochs):\n",
    "    net.train()\n",
    "    loss_sum = x_sum = bce_sum = 0.0\n",
    "\n",
    "    for it in range(cfg.steps_per_epoch):\n",
    "        y_seg, x_seg, p_seg = sample_batch(train_data, cfg)\n",
    "        out = rollout_window(net, y_seg, x_seg, p_seg, cfg)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        out[\"loss\"].backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        loss_sum += float(out[\"loss\"].detach())\n",
    "        x_sum += float(out[\"x_loss\"])\n",
    "        bce_sum += float(out[\"bce\"])\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        yv, xv, pv = sample_batch(val_data, cfg)\n",
    "        outv = rollout_window(net, yv, xv, pv, cfg)\n",
    "\n",
    "    print(f\"[ep {ep+1}/{cfg.epochs}] \"\n",
    "          f\"train loss {loss_sum/cfg.steps_per_epoch:.4f} (x {x_sum/cfg.steps_per_epoch:.4f}, bce {bce_sum/cfg.steps_per_epoch:.4f}) | \"\n",
    "          f\"val loss {float(outv['loss']):.4f} (x {float(outv['x_loss']):.4f}, bce {float(outv['bce']):.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c5db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Full 2000-step rollout + plots around k* ---\n",
    "\n",
    "@torch.no_grad()\n",
    "def rollout_full(net: nn.Module, y: torch.Tensor, cfg: CFG):\n",
    "    \"\"\"\n",
    "    y: (T,ny)\n",
    "    returns xhat (T,nx), thetahat (T,1), p (T,1)\n",
    "    \"\"\"\n",
    "    device, dtype = cfg.device, cfg.dtype\n",
    "    ny, nx = cfg.ny, cfg.nx\n",
    "    d_in = 2*ny + 2\n",
    "\n",
    "    q0 = y[0:1, :]\n",
    "    v0 = torch.zeros(1,2, device=device, dtype=dtype)\n",
    "    theta0 = torch.ones(1,1, device=device, dtype=dtype)\n",
    "    z = torch.cat([q0, v0, theta0], dim=-1)\n",
    "\n",
    "    P = torch.diag(torch.tensor([1e-2,1e-2, 1e-1,1e-1, 1e-2], device=device, dtype=dtype)).unsqueeze(0)\n",
    "    phi_buf = torch.zeros(1, cfg.W, d_in, device=device, dtype=dtype)\n",
    "\n",
    "    xhat = torch.zeros(cfg.T, nx, device=device, dtype=dtype)\n",
    "    thetahat = torch.zeros(cfg.T, 1, device=device, dtype=dtype)\n",
    "    p = torch.zeros(cfg.T, 1, device=device, dtype=dtype)\n",
    "\n",
    "    for t in range(cfg.T):\n",
    "        qv, rv, p_logit = net(phi_buf)\n",
    "        p_t = torch.sigmoid(p_logit)\n",
    "\n",
    "        Lq = vec_to_cholesky(qv, nx)\n",
    "        Qx = chol_to_spd(Lq)\n",
    "        Lr = vec_to_cholesky(rv, ny)\n",
    "        R = chol_to_spd(Lr)\n",
    "\n",
    "        qtheta = (1.0 - p_t) * cfg.Qtheta_base + p_t * cfg.Qtheta_jump\n",
    "        Qt = qtheta.view(1,1,1)\n",
    "        Qz = blockdiag(Qx, Qt)\n",
    "\n",
    "        z, P, e, S = ukf_step(\n",
    "            z, P, y[t:t+1, :], Qz, R,\n",
    "            dt=cfg.dt, alpha=cfg.alpha, beta=cfg.beta, kappa=cfg.kappa,\n",
    "            jitter_P=cfg.jitter_P, jitter_S=cfg.jitter_S\n",
    "        )\n",
    "\n",
    "        phi_t = make_phi(e, S, jitter=cfg.jitter_S)\n",
    "        phi_buf = torch.cat([phi_buf[:, 1:, :], phi_t.unsqueeze(1)], dim=1)\n",
    "\n",
    "        xhat[t] = z[0, :nx]\n",
    "        thetahat[t] = z[0, nx:nx+1]\n",
    "        p[t] = p_t[0]\n",
    "\n",
    "    return xhat, thetahat, p\n",
    "\n",
    "# pick one validation sequence\n",
    "d = random.choice(val_data)\n",
    "kstar = d[\"kstar\"]\n",
    "y = d[\"y\"]\n",
    "theta_true = d[\"theta_true\"]\n",
    "\n",
    "xhat, thetahat, p = rollout_full(net, y, cfg)\n",
    "\n",
    "# compute \"adaptation time\" to be within 5% of new theta\n",
    "theta_new = theta_true[kstar, 0].item()\n",
    "tol = 0.05 * theta_new\n",
    "err = (thetahat[:,0] - theta_true[:,0]).abs()\n",
    "after = err[kstar:]\n",
    "idx = (after < tol).nonzero(as_tuple=False)\n",
    "t_adapt = int(idx[0].item()) if idx.numel() > 0 else None\n",
    "\n",
    "print(\"k* =\", kstar)\n",
    "print(\"theta_new =\", theta_new, \"| tol(5%) =\", tol)\n",
    "print(\"first time within tol after k*:\", t_adapt, \"steps (target <= 100)\")\n",
    "\n",
    "# plot theta and p around k*\n",
    "w = 250\n",
    "a = max(0, kstar - w)\n",
    "b = min(cfg.T, kstar + w)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(theta_true[a:b,0].cpu().numpy(), label=\"theta true\")\n",
    "plt.plot(thetahat[a:b,0].cpu().numpy(), label=\"theta hat\")\n",
    "plt.axvline(x=kstar-a)\n",
    "plt.title(\"Theta jump and estimate (zoomed)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(p[a:b,0].cpu().numpy())\n",
    "plt.axvline(x=kstar-a)\n",
    "plt.title(\"p_k (change probability) (zoomed)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
