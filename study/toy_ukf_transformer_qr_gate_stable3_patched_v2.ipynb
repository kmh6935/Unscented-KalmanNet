{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b567c529",
   "metadata": {},
   "source": [
    "# Toy UKF + Causal Window Transformer NoiseNet (Full Cholesky Qx/R + pₖ→Qθ Gating)\n",
    "\n",
    "This notebook implements a **minimal end-to-end toy** for your setup:\n",
    "\n",
    "- **2-DOF** dynamics with state \\(x=[q_1,q_2,v_1,v_2]\\) (nx=4) and measurement \\(y=[q_1,q_2]\\) (ny=2)  \n",
    "- **Augmented state** \\(z=[x;\\theta]\\) with a **single parameter** \\(\\theta\\) scaling stiffness  \n",
    "- Sequence length **T=2000**, random change-point \\(k^*\\), and **+30% step jump** in \\(\\theta\\)  \n",
    "- A **causal Transformer** predicts **full SPD** \\(Q_{x,k}\\) and \\(R_k\\) via **Cholesky factors**, plus a change logit \\(p_k\\)  \n",
    "- \\(p_k\\) gates \\(Q_{\\theta,k}\\) inside the UKF predict covariance:\n",
    "  \\[\n",
    "  Q_{\\theta,k}=(1-\\sigma(p_k))Q_{\\theta}^{\\text{base}}+\\sigma(p_k)Q_{\\theta}^{\\text{jump}}\n",
    "  \\]\n",
    "- Training uses **sliding windows** with **burn-in**, **state supervision**, and **BCE supervision** for change detection.\n",
    "\n",
    "> Tip: Start with the default small training settings to sanity-check. Then increase epochs/steps to improve adaptation speed.\n",
    "\n",
    "\n",
    "**Training note:** During training we **do not supervise θ** (no θ loss). We only use state GT (and likelihood terms) for learning. In synthetic experiments, θ GT is used **only for evaluation / reporting**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa0c772c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.10.0+cu128\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Setup ---\n",
    "from __future__ import annotations\n",
    "import math, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ccccf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG(device='cuda', dtype=torch.float32, nx=4, ny=2, nt=1, nz=5, dt=0.02, T=2000, jump_ratio=0.3, kstar_lo=0.3, kstar_hi=0.7, alpha=1.0, beta=2.0, kappa=0.0, jitter_P=0.001, jitter_S=1e-06, W=128, d_model=128, n_layers=2, n_heads=4, dropout=0.1, burn_in=128, L=256, batch_size=32, n_train_seq=256, n_val_seq=64, n_test_seq=64, lr=0.0003, epochs=3, steps_per_epoch=100, w_state=1.0, w_bce=0.2, w_smooth=0.0001, w_offdiag=0.0001, Qtheta_base=1e-08, Qtheta_jump=0.0001, p_label_width=25)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Config ---\n",
    "@dataclass\n",
    "class CFG:\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype: torch.dtype = torch.float32\n",
    "\n",
    "    # problem dims\n",
    "    nx: int = 4\n",
    "    ny: int = 2\n",
    "    nt: int = 1\n",
    "    nz: int = 5\n",
    "\n",
    "    # simulation\n",
    "    dt: float = 0.02\n",
    "    T: int = 2000\n",
    "    jump_ratio: float = 0.30\n",
    "    kstar_lo: float = 0.3\n",
    "    kstar_hi: float = 0.7\n",
    "\n",
    "    # UKF params\n",
    "    alpha: float = 1.0\n",
    "    beta: float = 2.0\n",
    "    kappa: float = 0.0\n",
    "    jitter_P: float = 1e-3\n",
    "    jitter_S: float = 1e-6\n",
    "\n",
    "    # NoiseNet / Transformer\n",
    "    W: int = 128              # sliding window length for features (past-only)\n",
    "    d_model: int = 128\n",
    "    n_layers: int = 2\n",
    "    n_heads: int = 4\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # Training windows (burn-in + loss)\n",
    "    burn_in: int = 128\n",
    "    L: int = 256\n",
    "\n",
    "    # Dataset sizes\n",
    "    batch_size: int = 32\n",
    "\n",
    "    n_train_seq: int = 128*2\n",
    "    n_val_seq: int = 32*2\n",
    "    n_test_seq: int = 32*2\n",
    "\n",
    "    # Training (keep small for sanity-check; increase later)\n",
    "    lr: float = 3e-4\n",
    "    epochs: int = 3\n",
    "    steps_per_epoch: int = 100\n",
    "\n",
    "    # Loss weights\n",
    "    w_state: float = 1.0\n",
    "    w_bce: float = 0.2\n",
    "    w_smooth: float = 1e-4\n",
    "    w_offdiag: float = 1e-4\n",
    "\n",
    "    # Q_theta gating (variance)\n",
    "    Qtheta_base: float = 1e-8\n",
    "    Qtheta_jump: float = 1e-4   # increase (e.g., 1e-3) for faster adaptation if stable\n",
    "\n",
    "    # label width after change (p_k uses features up to k-1, so label starts at k*+1)\n",
    "    p_label_width: int = 25\n",
    "\n",
    "cfg = CFG()\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d6c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b4f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Utilities: Cholesky parameterization for full SPD Qx and R ---\n",
    "\n",
    "def n_tril(n: int) -> int:\n",
    "    return n * (n + 1) // 2\n",
    "\n",
    "\n",
    "def vec_to_cholesky(v: torch.Tensor, n: int, eps: float = 1e-4,\n",
    "                    diag_min: float = -12.0, diag_max: float = 6.0,\n",
    "                    off_clip: float = 3.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    v: (..., n_tril(n)) lower-triangular params\n",
    "       convention: first n entries -> diag logits, remaining -> strict-lower row-wise\n",
    "    returns L: (..., n, n) with positive diag (via softplus)\n",
    "\n",
    "    Safety:\n",
    "      - nan/inf -> 0\n",
    "      - diag logits clamped to [diag_min, diag_max]\n",
    "      - off-diagonals clipped to [-off_clip, off_clip]\n",
    "    \"\"\"\n",
    "    v = torch.nan_to_num(v, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    diag_logits = torch.clamp(v[..., :n], min=diag_min, max=diag_max)\n",
    "    off = torch.clamp(v[..., n:], min=-off_clip, max=off_clip)\n",
    "\n",
    "    L = v.new_zeros(*v.shape[:-1], n, n)\n",
    "    diag = F.softplus(diag_logits) + eps\n",
    "    idx = torch.arange(n, device=v.device)\n",
    "    L[..., idx, idx] = diag\n",
    "\n",
    "    k = 0\n",
    "    for i in range(1, n):\n",
    "        for j in range(i):\n",
    "            L[..., i, j] = off[..., k]\n",
    "            k += 1\n",
    "    return L\n",
    "\n",
    "\n",
    "def chol_to_spd(L: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    n = L.shape[-1]\n",
    "    I = torch.eye(n, device=L.device, dtype=L.dtype)\n",
    "    return L @ L.transpose(-1, -2) + eps * I\n",
    "\n",
    "def blockdiag(Qx: torch.Tensor, Qt: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Qx: (B,nx,nx), Qt: (B,nt,nt) -> Qz: (B,nz,nz)\n",
    "    \"\"\"\n",
    "    B, nx, _ = Qx.shape\n",
    "    _, nt, _ = Qt.shape\n",
    "    Qz = Qx.new_zeros(B, nx + nt, nx + nt)\n",
    "    Qz[:, :nx, :nx] = Qx\n",
    "    Qz[:, nx:, nx:] = Qt\n",
    "    return Qz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bceaa514",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Physics: 2-DOF model (theta scales the coupling stiffness) ---\n",
    "\n",
    "def build_mck(theta: torch.Tensor, device, dtype) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    theta: (B,1) stiffness scale for coupling spring\n",
    "    Returns M,C,K for each batch as (B,2,2)\n",
    "    \"\"\"\n",
    "    B = theta.shape[0]\n",
    "    m1, m2 = 1.0, 1.0\n",
    "    c1, c2, cc = 0.05, 0.05, 0.02\n",
    "    k1, k2 = 20.0, 20.0\n",
    "    kc0 = 15.0\n",
    "    kc = kc0 * theta.squeeze(-1)  # (B,)\n",
    "\n",
    "    M = torch.tensor([[m1, 0.0],[0.0, m2]], device=device, dtype=dtype).expand(B,2,2).clone()\n",
    "\n",
    "    # Damping: C = [[c1+cc, -cc],[-cc, c2+cc]]\n",
    "    C = torch.zeros(B,2,2, device=device, dtype=dtype)\n",
    "    C[:,0,0] = c1 + cc\n",
    "    C[:,1,1] = c2 + cc\n",
    "    C[:,0,1] = -cc\n",
    "    C[:,1,0] = -cc\n",
    "\n",
    "    # Stiffness: K = [[k1+kc, -kc],[-kc, k2+kc]]\n",
    "    K = torch.zeros(B,2,2, device=device, dtype=dtype)\n",
    "    K[:,0,0] = k1 + kc\n",
    "    K[:,1,1] = k2 + kc\n",
    "    K[:,0,1] = -kc\n",
    "    K[:,1,0] = -kc\n",
    "    return M, C, K\n",
    "\n",
    "def f_step(z: torch.Tensor, dt: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    z: (B, nz) = [q1,q2,v1,v2,theta]\n",
    "    deterministic transition (no noise added here)\n",
    "    \"\"\"\n",
    "    device, dtype = z.device, z.dtype\n",
    "    q = z[:, 0:2]            # (B,2)\n",
    "    v = z[:, 2:4]            # (B,2)\n",
    "    theta = z[:, 4:5]        # (B,1)\n",
    "\n",
    "    M, C, K = build_mck(theta, device, dtype)\n",
    "    Minv = torch.linalg.inv(M)\n",
    "\n",
    "    a = -(Minv @ (C @ v.unsqueeze(-1) + K @ q.unsqueeze(-1))).squeeze(-1)  # (B,2)\n",
    "\n",
    "    q_next = q + dt * v\n",
    "    v_next = v + dt * a\n",
    "    theta_next = theta  # random-walk handled via Q_theta in the filter\n",
    "\n",
    "    return torch.cat([q_next, v_next, theta_next], dim=-1)\n",
    "\n",
    "def h_meas(z: torch.Tensor) -> torch.Tensor:\n",
    "    # measurement: y = [q1,q2]\n",
    "    return z[:, 0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28670f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# --- Simulation: T=2000 with random k* and +30% step jump in theta ---\n",
    "\n",
    "@torch.no_grad()\n",
    "def simulate_sequence(T: int, dt: float, jump_ratio: float, kstar: int, device, dtype):\n",
    "    \"\"\"\n",
    "    Generates x_true (T,nx), theta_true (T,1), y (T,ny)\n",
    "    \"\"\"\n",
    "    theta0 = 1.0\n",
    "    theta1 = theta0 * (1.0 + jump_ratio)  # +30% at/after k*\n",
    "    theta_true = torch.full((T,1), theta0, device=device, dtype=dtype)\n",
    "    theta_true[kstar:, 0] = theta1\n",
    "\n",
    "    # initial state\n",
    "    q0 = torch.tensor([0.2, -0.1], device=device, dtype=dtype)\n",
    "    v0 = torch.tensor([0.0, 0.0], device=device, dtype=dtype)\n",
    "    x = torch.cat([q0, v0]).unsqueeze(0)  # (1,4)\n",
    "\n",
    "    # simulation noises\n",
    "    Qsim = torch.diag(torch.tensor([1e-6,1e-6,1e-5,1e-5], device=device, dtype=dtype))\n",
    "    Lsim = torch.linalg.cholesky(Qsim)\n",
    "\n",
    "    Rsim = torch.diag(torch.tensor([2e-4, 2e-4], device=device, dtype=dtype))\n",
    "    Lr = torch.linalg.cholesky(Rsim)\n",
    "\n",
    "    x_true = torch.zeros(T,4, device=device, dtype=dtype)\n",
    "    y = torch.zeros(T,2, device=device, dtype=dtype)\n",
    "\n",
    "    for k in range(T):\n",
    "        th = theta_true[k:k+1]  # (1,1)\n",
    "        z = torch.cat([x, th], dim=-1)  # (1,5)\n",
    "\n",
    "        z_next = f_step(z, dt=dt)\n",
    "        x_next = z_next[:, :4]\n",
    "        x_next = x_next + (torch.randn(1,4, device=device, dtype=dtype) @ Lsim.T)\n",
    "        x = x_next\n",
    "\n",
    "        x_true[k] = x.squeeze(0)\n",
    "        y[k] = x_true[k, :2] + (torch.randn(2, device=device, dtype=dtype) @ Lr.T)\n",
    "\n",
    "    return x_true, theta_true, y\n",
    "\n",
    "def make_p_label(T: int, kstar: int, width: int, device, dtype):\n",
    "    # label starts at kstar+1 because p_k is predicted from phi up to k-1\n",
    "    start = min(T-1, kstar + 1)\n",
    "    end = min(T, start + width)\n",
    "    p = torch.zeros(T,1, device=device, dtype=dtype)\n",
    "    p[start:end, 0] = 1.0\n",
    "    return p\n",
    "\n",
    "# quick sanity-check visualization of one simulated sequence\n",
    "device, dtype = cfg.device, cfg.dtype\n",
    "kstar = random.randint(int(cfg.kstar_lo*cfg.T), int(cfg.kstar_hi*cfg.T))\n",
    "x_true, theta_true, y = simulate_sequence(cfg.T, cfg.dt, cfg.jump_ratio, kstar, device, dtype)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(theta_true.cpu().numpy())\n",
    "plt.title(f\"True theta (k*={kstar})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d157200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def safe_cholesky(A: torch.Tensor, jitter: float = 1e-6, max_tries: int = 8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Robust Cholesky with increasing diagonal loading + nan/inf sanitization.\n",
    "    \"\"\"\n",
    "    A = torch.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    A = 0.5 * (A + A.transpose(-1,-2))\n",
    "    n = A.shape[-1]\n",
    "    I = torch.eye(n, device=A.device, dtype=A.dtype).unsqueeze(0)\n",
    "    for i in range(max_tries):\n",
    "        try:\n",
    "            return torch.linalg.cholesky(A + (jitter * (10.0**i)) * I)\n",
    "        except Exception:\n",
    "            continue\n",
    "    # fallback: eigen projection\n",
    "    w, V = torch.linalg.eigh(A)\n",
    "    w = torch.clamp(w, min=jitter)\n",
    "    A_spd = V @ torch.diag_embed(w) @ V.transpose(-1,-2)\n",
    "    return torch.linalg.cholesky(A_spd + jitter*I)\n",
    "\n",
    "\n",
    "\n",
    "# --- Innovation features phi_k = [L^{-1}e, |L^{-1}e|, NIS, logdetS] ---\n",
    "\n",
    "def make_phi(e: torch.Tensor, S: torch.Tensor, jitter: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    e: (B,ny)\n",
    "    S: (B,ny,ny) SPD\n",
    "    returns phi: (B, d_in) = [e_white, |e_white|, nis, logdetS]\n",
    "    \"\"\"\n",
    "    B, ny = e.shape\n",
    "    I = torch.eye(ny, device=e.device, dtype=e.dtype).unsqueeze(0)\n",
    "    S = S + jitter * I\n",
    "    L = safe_cholesky(S, jitter=jitter)  # (B,ny,ny)\n",
    "    # e_white = L^{-1} e\n",
    "    e_white = torch.linalg.solve_triangular(L, e.unsqueeze(-1), upper=False).squeeze(-1)\n",
    "    nis = (e_white**2).sum(dim=-1, keepdim=True)\n",
    "    logdetS = 2.0 * torch.log(torch.diagonal(L, dim1=-2, dim2=-1)).sum(dim=-1, keepdim=True)\n",
    "    phi = torch.cat([e_white, e_white.abs(), nis, logdetS], dim=-1)\n",
    "    return phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515f7b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Linear algebra helpers for UKF stability ---\n",
    "# NOTE: `safe_cholesky()` is defined above (with nan/inf sanitization + diagonal loading).\n",
    "# Here we keep a single `make_spd()` (sanitize + eigenvalue clamp) to avoid accidental overwrites.\n",
    "\n",
    "def sanitize_sym(A: torch.Tensor) -> torch.Tensor:\n",
    "    A = torch.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return 0.5 * (A + A.transpose(-1, -2))\n",
    "\n",
    "def make_spd(A: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"Project a symmetric matrix to SPD by clamping eigenvalues (batched).\"\"\"\n",
    "    A = sanitize_sym(A)\n",
    "    w, V = torch.linalg.eigh(A)\n",
    "    w = torch.clamp(w, min=eps)\n",
    "    A_spd = V @ torch.diag_embed(w) @ V.transpose(-1, -2)\n",
    "    n = A.shape[-1]\n",
    "    I = torch.eye(n, device=A.device, dtype=A.dtype)\n",
    "    return A_spd + eps * I\n",
    "\n",
    "def sqrtm_spd(A: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"Symmetric sqrt for (approx) SPD matrices via eigendecomposition (robust fallback).\"\"\"\n",
    "    A = sanitize_sym(A)\n",
    "    w, V = torch.linalg.eigh(A)\n",
    "    w = torch.clamp(w, min=eps)\n",
    "    return V @ torch.diag_embed(torch.sqrt(w))\n",
    "\n",
    "\n",
    "# --- UKF step (batched) ---\n",
    "\n",
    "def ukf_step(z: torch.Tensor, P: torch.Tensor, y: torch.Tensor, Qz: torch.Tensor, R: torch.Tensor,\n",
    "            dt: float, alpha: float, beta: float, kappa: float,\n",
    "            jitter_P: float, jitter_S: float):\n",
    "    \"\"\"\n",
    "    z: (B,nz) mean\n",
    "    P: (B,nz,nz)\n",
    "    y: (B,ny)\n",
    "    Qz: (B,nz,nz)\n",
    "    R:  (B,ny,ny)\n",
    "    Returns z_upd, P_upd, innovation e, S\n",
    "    \"\"\"\n",
    "    device, dtype = z.device, z.dtype\n",
    "    B, nz = z.shape\n",
    "    ny = y.shape[-1]\n",
    "\n",
    "    lam = alpha**2 * (nz + kappa) - nz\n",
    "    c = nz + lam\n",
    "    Wm0 = lam / c\n",
    "    Wc0 = Wm0 + (1 - alpha**2 + beta)\n",
    "    W = 1.0 / (2.0 * c)\n",
    "\n",
    "    # robust sqrt of P for sigma points\n",
    "    Pj = make_spd(P, eps=jitter_P)\n",
    "    U = sqrtm_spd(Pj, eps=jitter_P) * math.sqrt(c)  # (B,nz,nz)\n",
    "\n",
    "    # sigma points\n",
    "    sigmas = z.unsqueeze(1).repeat(1, 2*nz+1, 1)\n",
    "    Ucols = U.transpose(-1, -2)  # each row = a column of U\n",
    "    sigmas[:, 1:nz+1, :] = z.unsqueeze(1) + Ucols\n",
    "    sigmas[:, nz+1:,  :] = z.unsqueeze(1) - Ucols\n",
    "\n",
    "    # propagate through dynamics\n",
    "    sig_flat = sigmas.reshape(B*(2*nz+1), nz)\n",
    "    zprop = f_step(sig_flat, dt=dt).reshape(B, 2*nz+1, nz)\n",
    "\n",
    "    # weights\n",
    "    wm = torch.full((2*nz+1,), W, device=device, dtype=dtype)\n",
    "    wc = torch.full((2*nz+1,), W, device=device, dtype=dtype)\n",
    "    wm[0] = Wm0\n",
    "    wc[0] = Wc0\n",
    "\n",
    "    # predicted mean/cov\n",
    "    z_pred = (zprop * wm.view(1,-1,1)).sum(dim=1)\n",
    "    dz = zprop - z_pred.unsqueeze(1)\n",
    "    P_pred = torch.einsum(\"i,bij,bik->bjk\", wc, dz, dz) + Qz\n",
    "\n",
    "    # predicted measurement\n",
    "    yprop = h_meas(zprop.reshape(B*(2*nz+1), nz)).reshape(B, 2*nz+1, ny)\n",
    "    y_pred = (yprop * wm.view(1,-1,1)).sum(dim=1)\n",
    "    dy = yprop - y_pred.unsqueeze(1)\n",
    "    S = torch.einsum(\"i,bij,bik->bjk\", wc, dy, dy) + R\n",
    "    Pzy = torch.einsum(\"i,bij,bik->bjk\", wc, dz, dy)\n",
    "\n",
    "    # stabilize S (and ensure SPD)\n",
    "    I_y = torch.eye(ny, device=device, dtype=dtype).unsqueeze(0)\n",
    "    S = make_spd(S + jitter_S * I_y, eps=jitter_S)\n",
    "\n",
    "    # gain and update: K = Pzy @ S^{-1} using Cholesky solves\n",
    "    Ls = safe_cholesky(S, jitter=jitter_S)\n",
    "    tmp = torch.linalg.solve_triangular(Ls, Pzy.transpose(-1,-2), upper=False)\n",
    "    tmp = torch.linalg.solve_triangular(Ls.transpose(-1,-2), tmp, upper=True)\n",
    "    K = tmp.transpose(-1,-2)\n",
    "\n",
    "    e = y - y_pred\n",
    "    z_upd = z_pred + torch.einsum(\"bij,bj->bi\", K, e)\n",
    "\n",
    "    P_upd = P_pred - K @ S @ K.transpose(-1,-2)\n",
    "    P_upd = make_spd(0.5 * (P_upd + P_upd.transpose(-1,-2)), eps=jitter_P)\n",
    "    return z_upd, P_upd, e, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "785a89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Causal window Transformer NoiseNet ---\n",
    "# It predicts Qx_k, R_k (full SPD via Cholesky factors) and a change logit p_k.\n",
    "# It only sees past features (phi up to k-1), so the whole pipeline is causal.\n",
    "\n",
    "class CausalWindowTransformerNoiseNet(nn.Module):\n",
    "    def __init__(self, d_in: int, d_model: int, nx: int, ny: int,\n",
    "                 W: int, n_layers: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.nx, self.ny = nx, ny\n",
    "        self.nQ = n_tril(nx)\n",
    "        self.nR = n_tril(ny)\n",
    "        self.d_out = self.nQ + self.nR + 1\n",
    "\n",
    "        self.in_proj = nn.Linear(d_in, d_model)\n",
    "        self.pos = nn.Parameter(torch.zeros(1, W, d_model))\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=4*d_model,\n",
    "            dropout=dropout, batch_first=True, activation=\"gelu\", norm_first=True\n",
    "        )\n",
    "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.head = nn.Linear(d_model, self.d_out)\n",
    "\n",
    "        # initialize to small-ish Q/R (stability)\n",
    "        with torch.no_grad():\n",
    "            self.head.weight.mul_(0.01)\n",
    "            self.head.bias.zero_()\n",
    "            self.head.bias[:nx] = -6.0               # Q diag logits\n",
    "            self.head.bias[self.nQ:self.nQ+ny] = -6.0 # R diag logits\n",
    "            self.head.bias[-1] = -6.0                 # p_logit bias -> p≈0 at init\n",
    "\n",
    "    def forward(self, phi_seq: torch.Tensor):\n",
    "        # phi_seq: (B,W,d_in)\n",
    "        B, W, _ = phi_seq.shape\n",
    "        h = self.in_proj(phi_seq) + self.pos[:, :W, :]\n",
    "        h = self.enc(h)\n",
    "        out = self.head(h[:, -1, :])  # last token as \"current\" prediction\n",
    "\n",
    "        qv = out[:, :self.nQ]\n",
    "        rv = out[:, self.nQ:self.nQ+self.nR]\n",
    "        p_logit = out[:, -1:]\n",
    "        return qv, rv, p_logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14f45d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets from ./data...\n",
      "Data loaded successfully.\n",
      "Dataset Sizes -> Train: 256, Val: 64, Test: 64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Dataset creation (in-memory): random k* per sequence ---\n",
    "# --- 데이터 저장 경로 설정 ---\n",
    "DATA_DIR = \"./data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"train_data.pt\")\n",
    "VAL_FILE = os.path.join(DATA_DIR, \"val_data.pt\")\n",
    "TEST_FILE = os.path.join(DATA_DIR, \"test_data.pt\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_dataset(n_seq: int, cfg: CFG):\n",
    "    dev, dt = cfg.device, cfg.dtype\n",
    "    data = []\n",
    "    for _ in range(n_seq):\n",
    "        kstar = random.randint(int(cfg.kstar_lo * cfg.T), int(cfg.kstar_hi * cfg.T))\n",
    "        x_true, theta_true, y = simulate_sequence(cfg.T, cfg.dt, cfg.jump_ratio, kstar, dev, dt)\n",
    "        p = make_p_label(cfg.T, kstar, cfg.p_label_width, dev, dt)\n",
    "        data.append({\"x_true\": x_true, \"theta_true\": theta_true, \"y\": y, \"p\": p, \"kstar\": kstar})\n",
    "    return data\n",
    "\n",
    "def sample_batch(data, cfg: CFG):\n",
    "    B = cfg.batch_size\n",
    "    T = cfg.T\n",
    "    seg = cfg.burn_in + cfg.L\n",
    "    idx = random.sample(range(len(data)), B)\n",
    "    s_max = T - seg - 1\n",
    "    starts = [random.randint(0, s_max) for _ in range(B)]\n",
    "\n",
    "    y = []\n",
    "    x = []\n",
    "    p = []\n",
    "    for i, s in zip(idx, starts):\n",
    "        d = data[i]\n",
    "        y.append(d[\"y\"][s:s+seg])\n",
    "        x.append(d[\"x_true\"][s:s+seg])\n",
    "        p.append(d[\"p\"][s:s+seg])\n",
    "\n",
    "    return torch.stack(y, 0), torch.stack(x, 0), torch.stack(p, 0)\n",
    "\n",
    "def prepare_datasets(cfg, force_generate=False):\n",
    "    \"\"\"\n",
    "    데이터 파일이 있으면 로드하고, 없으면 생성 후 저장합니다.\n",
    "    force_generate=True로 설정하면 무조건 새로 생성하고 덮어씁니다.\n",
    "    \"\"\"\n",
    "    # 1. 파일이 모두 존재하고, 강제 생성이 아닐 경우 -> 로드\n",
    "    if os.path.exists(TRAIN_FILE) and os.path.exists(VAL_FILE) and os.path.exists(TEST_FILE) and not force_generate:\n",
    "        print(f\"Loading datasets from {DATA_DIR}...\")\n",
    "        train_data = torch.load(TRAIN_FILE)\n",
    "        val_data = torch.load(VAL_FILE)\n",
    "        test_data = torch.load(TEST_FILE)\n",
    "        print(\"Data loaded successfully.\")\n",
    "    \n",
    "    # 2. 파일이 없거나 강제 생성일 경우 -> 생성 및 저장\n",
    "    else:\n",
    "        print(\"Generating new datasets...\")\n",
    "        # (1) Training Data\n",
    "        print(f\"  - Building Train ({cfg.n_train_seq} seq)...\")\n",
    "        train_data = build_dataset(cfg.n_train_seq, cfg)\n",
    "        \n",
    "        # (2) Validation Data\n",
    "        print(f\"  - Building Val ({cfg.n_val_seq} seq)...\")\n",
    "        val_data = build_dataset(cfg.n_val_seq, cfg)\n",
    "        \n",
    "        # (3) Testing Data (새로 추가됨)\n",
    "        print(f\"  - Building Test ({cfg.n_test_seq} seq)...\")\n",
    "        test_data = build_dataset(cfg.n_test_seq, cfg)\n",
    "        \n",
    "        # 저장\n",
    "        print(f\"Saving datasets to {DATA_DIR}...\")\n",
    "        torch.save(train_data, TRAIN_FILE)\n",
    "        torch.save(val_data, VAL_FILE)\n",
    "        torch.save(test_data, TEST_FILE)\n",
    "        print(\"Data generation and saving complete.\")\n",
    "        \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# --- 실행 ---\n",
    "# force_generate=True로 하면 기존 파일을 무시하고 새로 만듭니다.\n",
    "train_data, val_data, test_data = prepare_datasets(cfg, force_generate=False)\n",
    "\n",
    "print(f\"Dataset Sizes -> Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a10c3ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Differentiable rollout on one (burn-in + loss) window ---\n",
    "\n",
    "def _nll_gaussian_from_chol(resid: torch.Tensor, L: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    resid: (B,d), L: (B,d,d) lower Cholesky of SPD cov\n",
    "    returns per-sample NLL up to an additive constant (drops d*log(2pi)/2).\n",
    "    \"\"\"\n",
    "    sol = torch.cholesky_solve(resid.unsqueeze(-1), L)  # (B,d,1)\n",
    "    quad = (resid.unsqueeze(1) @ sol).squeeze(-1).squeeze(-1)  # (B,)\n",
    "    logdet = 2.0 * torch.log(torch.diagonal(L, dim1=-2, dim2=-1)).sum(dim=-1)  # (B,)\n",
    "    return 0.5 * (quad + logdet)\n",
    "\n",
    "def rollout_window(net: nn.Module, y_seg: torch.Tensor, x_true_seg: torch.Tensor, p_label_seg: torch.Tensor, cfg: CFG):\n",
    "    \"\"\"\n",
    "    y_seg: (B,Tseg,ny), x_true_seg: (B,Tseg,nx), p_label_seg: (B,Tseg,1)\n",
    "    Returns dict(losses, trajectories)\n",
    "    \"\"\"\n",
    "    device, dtype = y_seg.device, y_seg.dtype\n",
    "    B, Tseg, ny = y_seg.shape\n",
    "    nx, nz = cfg.nx, cfg.nz\n",
    "\n",
    "    d_in = 2*ny + 2\n",
    "\n",
    "    # init z and P\n",
    "    q0 = y_seg[:, 0, :]\n",
    "    v0 = torch.zeros(B,2, device=device, dtype=dtype)\n",
    "    theta0 = torch.ones(B,1, device=device, dtype=dtype)\n",
    "    z = torch.cat([q0, v0, theta0], dim=-1)\n",
    "\n",
    "    P = torch.diag(torch.tensor([1e-2,1e-2, 1e-1,1e-1, 1e-2], device=device, dtype=dtype)).unsqueeze(0).repeat(B,1,1)\n",
    "\n",
    "    # feature buffer: past-only features\n",
    "    phi_buf = torch.zeros(B, cfg.W, d_in, device=device, dtype=dtype)\n",
    "\n",
    "    xhat, thetahat, ppred = [], [], []\n",
    "    qv_hist, rv_hist = [], []\n",
    "    nll_y_hist, nll_x_hist = [], []\n",
    "\n",
    "    for t in range(Tseg):\n",
    "        # 1) predict Qx_k, R_k, p_k from past feature window\n",
    "        qv, rv, p_logit = net(phi_buf)\n",
    "        p = torch.sigmoid(p_logit)\n",
    "\n",
    "        # 2) build full SPD Qx and R via Cholesky\n",
    "        Lq = vec_to_cholesky(qv, nx)\n",
    "        Qx = chol_to_spd(Lq)\n",
    "        Lr = vec_to_cholesky(rv, ny)\n",
    "        R = chol_to_spd(Lr)\n",
    "\n",
    "        # 3) gate Q_theta,k using p_k (THIS is the key location)\n",
    "        qtheta = (1.0 - p) * cfg.Qtheta_base + p * cfg.Qtheta_jump\n",
    "        Qt = qtheta.view(B,1,1)\n",
    "        Qz = blockdiag(Qx, Qt)\n",
    "\n",
    "        # 4) UKF update\n",
    "        z, P, e, S = ukf_step(\n",
    "            z, P, y_seg[:, t, :], Qz, R,\n",
    "            dt=cfg.dt, alpha=cfg.alpha, beta=cfg.beta, kappa=cfg.kappa,\n",
    "            jitter_P=cfg.jitter_P, jitter_S=cfg.jitter_S\n",
    "        )\n",
    "\n",
    "        # 4b) probabilistic losses (no theta supervision; uses y and x GT only)\n",
    "        # innovation NLL calibrates S (and indirectly R, Q, P)\n",
    "        Ls = safe_cholesky(S, jitter=cfg.jitter_S)\n",
    "        nll_y = _nll_gaussian_from_chol(e, Ls)  # (B,)\n",
    "        nll_y_hist.append(nll_y)\n",
    "\n",
    "        # state NLL calibrates Px (optional but recommended)\n",
    "        Px = P[:, :nx, :nx]\n",
    "        dx = x_true_seg[:, t, :] - z[:, :nx]\n",
    "        Lx = safe_cholesky(Px, jitter=cfg.jitter_P)\n",
    "        nll_x = _nll_gaussian_from_chol(dx, Lx)\n",
    "        nll_x_hist.append(nll_x)\n",
    "\n",
    "        # 5) create phi_t and append to buffer (for next step)\n",
    "        phi_t = make_phi(e, S, jitter=cfg.jitter_S)\n",
    "        phi_buf = torch.cat([phi_buf[:, 1:, :], phi_t.unsqueeze(1)], dim=1)\n",
    "\n",
    "        xhat.append(z[:, :nx])\n",
    "        thetahat.append(z[:, nx:nx+1])\n",
    "        ppred.append(p_logit)\n",
    "        qv_hist.append(qv)\n",
    "        rv_hist.append(rv)\n",
    "\n",
    "    xhat = torch.stack(xhat, 1)\n",
    "    thetahat = torch.stack(thetahat, 1)\n",
    "    ppred = torch.stack(ppred, 1)\n",
    "    qv_hist = torch.stack(qv_hist, 1)\n",
    "    rv_hist = torch.stack(rv_hist, 1)\n",
    "    nll_y_hist = torch.stack(nll_y_hist, 1)  # (B,Tseg)\n",
    "    nll_x_hist = torch.stack(nll_x_hist, 1)\n",
    "\n",
    "    start = cfg.burn_in\n",
    "    end = cfg.burn_in + cfg.L\n",
    "\n",
    "    x_loss = F.mse_loss(xhat[:, start:end, :], x_true_seg[:, start:end, :])\n",
    "    nll_y = nll_y_hist[:, start:end].mean()\n",
    "    nll_x = nll_x_hist[:, start:end].mean()\n",
    "    bce = F.binary_cross_entropy_with_logits(ppred[:, start:end, :], p_label_seg[:, start:end, :])\n",
    "\n",
    "    dq = (qv_hist[:, start+1:end, :] - qv_hist[:, start:end-1, :]).pow(2).mean()\n",
    "    dr = (rv_hist[:, start+1:end, :] - rv_hist[:, start:end-1, :]).pow(2).mean()\n",
    "    smooth = dq + dr\n",
    "\n",
    "    q_off = qv_hist[:, start:end, nx:]\n",
    "    r_off = rv_hist[:, start:end, ny:]\n",
    "    offdiag = (q_off.pow(2).mean() + r_off.pow(2).mean())\n",
    "\n",
    "    loss = (\n",
    "        cfg.w_state*x_loss\n",
    "        + cfg.w_nll_y*nll_y\n",
    "        + cfg.w_nll_x*nll_x\n",
    "        + cfg.w_bce*bce\n",
    "        + cfg.w_smooth*smooth\n",
    "        + cfg.w_offdiag*offdiag\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"x_loss\": x_loss.detach(),\n",
    "        \"nll_y\": nll_y.detach(),\n",
    "        \"nll_x\": nll_x.detach(),\n",
    "        \"bce\": bce.detach(),\n",
    "        \"smooth\": smooth.detach(),\n",
    "        \"offdiag\": offdiag.detach(),\n",
    "        \"xhat\": xhat.detach(),\n",
    "        \"thetahat\": thetahat.detach(),\n",
    "        \"ppred\": ppred.detach(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4605cc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minhy\\AppData\\Local\\Temp\\ipykernel_30692\\1310868581.py:21: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  self.enc = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n"
     ]
    },
    {
     "ename": "_LinAlgError",
     "evalue": "linalg.cholesky: (Batch element 6): The factorization could not be completed because the input is not positive-definite (the leading minor of order 4 is not positive-definite).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_LinAlgError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg.steps_per_epoch):\n\u001b[32m     19\u001b[39m     y_seg, x_seg, p_seg = sample_batch(train_data, cfg)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     out = \u001b[43mrollout_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_seg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_seg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_seg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     opt.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     23\u001b[39m     out[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m].backward()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mrollout_window\u001b[39m\u001b[34m(net, y_seg, x_true_seg, p_label_seg, cfg)\u001b[39m\n\u001b[32m     69\u001b[39m Px = P[:, :nx, :nx]\n\u001b[32m     70\u001b[39m dx = x_true_seg[:, t, :] - z[:, :nx]\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m Lx = \u001b[43msafe_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjitter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjitter_P\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m nll_x = _nll_gaussian_from_chol(dx, Lx)\n\u001b[32m     73\u001b[39m nll_x_hist.append(nll_x)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36msafe_cholesky\u001b[39m\u001b[34m(A, jitter, max_tries)\u001b[39m\n\u001b[32m     16\u001b[39m w = torch.clamp(w, \u001b[38;5;28mmin\u001b[39m=jitter)\n\u001b[32m     17\u001b[39m A_spd = V @ torch.diag_embed(w) @ V.transpose(-\u001b[32m1\u001b[39m,-\u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_spd\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mjitter\u001b[49m\u001b[43m*\u001b[49m\u001b[43mI\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31m_LinAlgError\u001b[39m: linalg.cholesky: (Batch element 6): The factorization could not be completed because the input is not positive-definite (the leading minor of order 4 is not positive-definite)."
     ]
    }
   ],
   "source": [
    "# --- Train ---\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "d_in = 2*cfg.ny + 2\n",
    "net = CausalWindowTransformerNoiseNet(\n",
    "    d_in=d_in, d_model=cfg.d_model, nx=cfg.nx, ny=cfg.ny,\n",
    "    W=cfg.W, n_layers=cfg.n_layers, n_heads=cfg.n_heads, dropout=cfg.dropout\n",
    ").to(cfg.device)\n",
    "\n",
    "opt = torch.optim.AdamW(net.parameters(), lr=cfg.lr, weight_decay=1e-4)\n",
    "\n",
    "for ep in range(cfg.epochs):\n",
    "    net.train()\n",
    "    loss_sum = x_sum = bce_sum = nly_sum = nlx_sum = 0.0\n",
    "\n",
    "    for it in range(cfg.steps_per_epoch):\n",
    "        y_seg, x_seg, p_seg = sample_batch(train_data, cfg)\n",
    "        out = rollout_window(net, y_seg, x_seg, p_seg, cfg)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        out[\"loss\"].backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        loss_sum += float(out[\"loss\"].detach())\n",
    "        x_sum += float(out[\"x_loss\"])\n",
    "        bce_sum += float(out[\"bce\"])\n",
    "        nly_sum += float(out[\"nll_y\"])\n",
    "        nlx_sum += float(out[\"nll_x\"])\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        yv, xv, pv = sample_batch(val_data, cfg)\n",
    "        outv = rollout_window(net, yv, xv, pv, cfg)\n",
    "\n",
    "    print(f\"[ep {ep+1}/{cfg.epochs}] \"\n",
    "          f\"train loss {loss_sum/cfg.steps_per_epoch:.4f} (x {x_sum/cfg.steps_per_epoch:.4f}, bce {bce_sum/cfg.steps_per_epoch:.4f}) | \"\n",
    "          f\"val loss {float(outv['loss']):.4f} (x {float(outv['x_loss']):.4f}, bce {float(outv['bce']):.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c5db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Full 2000-step rollout + probabilistic evaluation (theta GT used ONLY here) ---\n",
    "\n",
    "@torch.no_grad()\n",
    "def rollout_full(net: nn.Module, y: torch.Tensor, cfg: CFG):\n",
    "    \"\"\"\n",
    "    y: (T,ny)\n",
    "    returns:\n",
    "      zhat: (T,nz)\n",
    "      P:    (T,nz,nz)\n",
    "      p:    (T,1)  change probability\n",
    "      nis:  (T,1)  normalized innovation squared (should average ~ ny)\n",
    "    \"\"\"\n",
    "    device, dtype = cfg.device, cfg.dtype\n",
    "    T = y.shape[0]\n",
    "    ny, nx, nz = cfg.ny, cfg.nx, cfg.nz\n",
    "    d_in = 2*ny + 2\n",
    "\n",
    "    q0 = y[0:1, :]\n",
    "    v0 = torch.zeros(1,2, device=device, dtype=dtype)\n",
    "    theta0 = torch.ones(1,1, device=device, dtype=dtype)\n",
    "    z = torch.cat([q0, v0, theta0], dim=-1)\n",
    "\n",
    "    P = torch.diag(torch.tensor([1e-2,1e-2, 1e-1,1e-1, 1e-2], device=device, dtype=dtype)).unsqueeze(0)\n",
    "    phi_buf = torch.zeros(1, cfg.W, d_in, device=device, dtype=dtype)\n",
    "\n",
    "    zhist = torch.zeros(T, nz, device=device, dtype=dtype)\n",
    "    Phist = torch.zeros(T, nz, nz, device=device, dtype=dtype)\n",
    "    p = torch.zeros(T, 1, device=device, dtype=dtype)\n",
    "    nis = torch.zeros(T, 1, device=device, dtype=dtype)\n",
    "\n",
    "    for t in range(T):\n",
    "        qv, rv, p_logit = net(phi_buf)\n",
    "        p_t = torch.sigmoid(p_logit)\n",
    "\n",
    "        Lq = vec_to_cholesky(qv, nx)\n",
    "        Qx = chol_to_spd(Lq)\n",
    "        Lr = vec_to_cholesky(rv, ny)\n",
    "        R = chol_to_spd(Lr)\n",
    "\n",
    "        qtheta = (1.0 - p_t) * cfg.Qtheta_base + p_t * cfg.Qtheta_jump\n",
    "        Qt = qtheta.view(1,1,1)\n",
    "        Qz = blockdiag(Qx, Qt)\n",
    "\n",
    "        z, P, e, S = ukf_step(\n",
    "            z, P, y[t:t+1, :], Qz, R,\n",
    "            dt=cfg.dt, alpha=cfg.alpha, beta=cfg.beta, kappa=cfg.kappa,\n",
    "            jitter_P=cfg.jitter_P, jitter_S=cfg.jitter_S\n",
    "        )\n",
    "\n",
    "        # NIS = e^T S^{-1} e\n",
    "        Ls = safe_cholesky(S, jitter=cfg.jitter_S)\n",
    "        sol = torch.cholesky_solve(e.unsqueeze(-1), Ls)\n",
    "        quad = (e.unsqueeze(1) @ sol).squeeze(-1).squeeze(-1)  # (1,)\n",
    "        nis[t, 0] = quad[0]\n",
    "\n",
    "        phi_t = make_phi(e, S, jitter=cfg.jitter_S)\n",
    "        phi_buf = torch.cat([phi_buf[:, 1:, :], phi_t.unsqueeze(1)], dim=1)\n",
    "\n",
    "        zhist[t] = z[0]\n",
    "        Phist[t] = P[0]\n",
    "        p[t] = p_t[0]\n",
    "\n",
    "    return zhist, Phist, p, nis\n",
    "\n",
    "\n",
    "def _z_for_level(level: float) -> float:\n",
    "    # two-sided central interval\n",
    "    if level == 0.50: return 0.67448975\n",
    "    if level == 0.90: return 1.64485363\n",
    "    if level == 0.95: return 1.95996398\n",
    "    if level == 0.99: return 2.57582930\n",
    "    raise ValueError(\"Unsupported level\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_one(net: nn.Module, d: dict, cfg: CFG, levels=(0.50, 0.90, 0.99)):\n",
    "    y = d[\"y\"]\n",
    "    x_true = d[\"x_true\"]\n",
    "    theta_true = d[\"theta_true\"]\n",
    "    p_label = d[\"p\"]\n",
    "    kstar = d[\"kstar\"]\n",
    "\n",
    "    zhat, Phist, p, nis = rollout_full(net, y, cfg)\n",
    "    nx = cfg.nx\n",
    "\n",
    "    xhat = zhat[:, :nx]\n",
    "    thetahat = zhat[:, nx:nx+1]\n",
    "\n",
    "    # diag variances\n",
    "    Pxx = torch.diagonal(Phist[:, :nx, :nx], dim1=-2, dim2=-1)  # (T,nx)\n",
    "    Ptt = Phist[:, nx, nx].unsqueeze(-1)                        # (T,1)\n",
    "\n",
    "    # point errors\n",
    "    ex = xhat - x_true\n",
    "    et = thetahat - theta_true\n",
    "\n",
    "    rmse_x = torch.sqrt((ex**2).mean()).item()\n",
    "    rmse_theta = torch.sqrt((et**2).mean()).item()\n",
    "\n",
    "    # coverage (componentwise marginal)\n",
    "    cov = {}\n",
    "    for lv in levels:\n",
    "        zc = _z_for_level(lv)\n",
    "        cov[f\"x@{int(lv*100)}\"] = (ex.abs() <= zc*torch.sqrt(Pxx)).float().mean().item()\n",
    "        cov[f\"theta@{int(lv*100)}\"] = (et.abs() <= zc*torch.sqrt(Ptt)).float().mean().item()\n",
    "\n",
    "    # NEES (z) and NEES_theta (consistency checks for P)\n",
    "    dz = torch.cat([ex, et], dim=-1)  # (T,nz)\n",
    "    Lz = safe_cholesky(Phist, jitter=cfg.jitter_P)  # (T,nz,nz)\n",
    "    solz = torch.cholesky_solve(dz.unsqueeze(-1), Lz)  # (T,nz,1)\n",
    "    quadz = (dz.unsqueeze(1) @ solz).squeeze(-1).squeeze(-1)  # (T,)\n",
    "    nees = quadz.mean().item()\n",
    "    nees_theta = float(((et[:,0]**2) / (Ptt[:,0] + 1e-12)).mean().item())\n",
    "\n",
    "    # NIS mean\n",
    "    nis_mean = nis.mean().item()\n",
    "\n",
    "    # change detection metrics (AUC + first-detect delay)\n",
    "    # AUC via Mann-Whitney (no sklearn dependency)\n",
    "    y_true = p_label[:,0].detach().cpu().numpy().astype(np.int32)\n",
    "    y_score = p[:,0].detach().cpu().numpy()\n",
    "    n_pos = int(y_true.sum())\n",
    "    n_neg = int((1 - y_true).sum())\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        auc = float(\"nan\")\n",
    "    else:\n",
    "        order = np.argsort(y_score)\n",
    "        ranks = np.empty_like(order, dtype=np.float64)\n",
    "        ranks[order] = np.arange(len(y_score), dtype=np.float64) + 1.0\n",
    "        sum_ranks_pos = ranks[y_true == 1].sum()\n",
    "        auc = (sum_ranks_pos - n_pos*(n_pos+1)/2.0) / (n_pos*n_neg)\n",
    "\n",
    "    # detection delay after k* (thresholded)\n",
    "    thr = 0.5\n",
    "    after = np.where(np.arange(cfg.T) >= kstar)[0]\n",
    "    det_idx = after[np.where(y_score[after] >= thr)[0]]\n",
    "    det_delay = int(det_idx[0] - kstar) if det_idx.size > 0 else None\n",
    "\n",
    "    return {\n",
    "        \"kstar\": kstar,\n",
    "        \"rmse_x\": rmse_x,\n",
    "        \"rmse_theta\": rmse_theta,\n",
    "        \"nees_mean\": nees,\n",
    "        \"nees_theta_mean\": nees_theta,\n",
    "        \"nis_mean\": nis_mean,\n",
    "        \"auc_p\": float(auc),\n",
    "        \"det_delay\": det_delay,\n",
    "        **cov\n",
    "    }, (zhat, Phist, p, nis)\n",
    "\n",
    "\n",
    "# --- Example: visualize one test sequence ---\n",
    "d = random.choice(test_data)\n",
    "metrics, (zhat, Phist, p, nis) = evaluate_one(net, d, cfg)\n",
    "\n",
    "kstar = d[\"kstar\"]\n",
    "x_true = d[\"x_true\"]\n",
    "theta_true = d[\"theta_true\"]\n",
    "nx = cfg.nx\n",
    "\n",
    "xhat = zhat[:, :nx]\n",
    "thetahat = zhat[:, nx:nx+1]\n",
    "Ptt = Phist[:, nx, nx].unsqueeze(-1)\n",
    "\n",
    "print(\"One-sequence metrics:\")\n",
    "for k,v in metrics.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# plot theta with 99% band around k*\n",
    "z99 = _z_for_level(0.99)\n",
    "theta_std = torch.sqrt(Ptt + 1e-12)\n",
    "\n",
    "w = 250\n",
    "a = max(0, kstar - w)\n",
    "b = min(cfg.T, kstar + w)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(theta_true[a:b,0].cpu().numpy(), label=\"theta true\")\n",
    "plt.plot(thetahat[a:b,0].cpu().numpy(), label=\"theta hat\")\n",
    "plt.fill_between(np.arange(a,b)-a,\n",
    "                 (thetahat[a:b,0] - z99*theta_std[a:b,0]).cpu().numpy(),\n",
    "                 (thetahat[a:b,0] + z99*theta_std[a:b,0]).cpu().numpy(),\n",
    "                 alpha=0.2, label=\"99% band\")\n",
    "plt.axvline(x=kstar-a)\n",
    "plt.title(\"Theta jump and estimate (zoomed) + 99% interval\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(p[a:b,0].cpu().numpy())\n",
    "plt.axvline(x=kstar-a)\n",
    "plt.title(\"p_k (change probability) (zoomed)\")\n",
    "plt.show()\n",
    "\n",
    "# --- Aggregate evaluation over multiple test sequences ---\n",
    "rows = []\n",
    "for d in test_data:\n",
    "    m, _ = evaluate_one(net, d, cfg)\n",
    "    rows.append(m)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"\\nTest summary (mean ± std):\")\n",
    "cols = [\"rmse_x\",\"rmse_theta\",\"nis_mean\",\"nees_mean\",\"nees_theta_mean\",\"auc_p\",\"x@50\",\"x@90\",\"x@99\",\"theta@50\",\"theta@90\",\"theta@99\"]\n",
    "for c in cols:\n",
    "    if c in df.columns:\n",
    "        mu = df[c].mean()\n",
    "        sd = df[c].std()\n",
    "        print(f\"  {c:12s}: {mu:.4f} ± {sd:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
