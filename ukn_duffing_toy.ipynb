{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c5da64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0000 | loss=0.07372 | ukf=0.03682 | x=0.00069 th=0.03613 dom=0.01845\n",
      "step 0020 | loss=0.07697 | ukf=0.04056 | x=0.00070 th=0.03974 dom=0.01820\n",
      "step 0040 | loss=0.06876 | ukf=0.03508 | x=0.00080 th=0.03301 dom=0.01684\n",
      "step 0060 | loss=0.05213 | ukf=0.03837 | x=0.00084 th=0.02656 dom=0.00688\n",
      "step 0080 | loss=0.05047 | ukf=0.04470 | x=0.00108 th=0.02876 dom=0.00288\n",
      "step 0100 | loss=0.04859 | ukf=0.02326 | x=0.00113 th=0.01732 dom=0.01266\n",
      "step 0120 | loss=0.06457 | ukf=0.07377 | x=0.00127 th=0.04588 dom=0.00000\n",
      "step 0140 | loss=0.04172 | ukf=0.02655 | x=0.00102 th=0.01597 dom=0.00759\n",
      "step 0160 | loss=0.04623 | ukf=0.03765 | x=0.00113 th=0.02381 dom=0.00429\n",
      "step 0180 | loss=0.03966 | ukf=0.02625 | x=0.00105 th=0.01514 dom=0.00670\n",
      "\n",
      "[EVAL on dirty(outlier) test]\n",
      "RMSE_x   UKN=0.030296 | UKF=0.025612\n",
      "RMSE_th  UKN=0.126190 | UKF=0.153777\n"
     ]
    }
   ],
   "source": [
    "# ukn_duffing_toy.py\n",
    "# PyTorch implementation of Unscented KalmanNet (UKN) for the Duffing toy problem.\n",
    "# nx=2, ny=2, ntheta=2, theta only in f.\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Utility: SPD helpers\n",
    "# -----------------------------\n",
    "def symmetrize(A: torch.Tensor) -> torch.Tensor:\n",
    "    return 0.5 * (A + A.transpose(-1, -2))\n",
    "\n",
    "def project_spd(A: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"Project a symmetric matrix to SPD via eigenvalue clamping.\"\"\"\n",
    "    A = symmetrize(A)\n",
    "    evals, evecs = torch.linalg.eigh(A)\n",
    "    evals = torch.clamp(evals, min=eps)\n",
    "    return (evecs * evals.unsqueeze(-2)) @ evecs.transpose(-1, -2)\n",
    "\n",
    "def safe_cholesky(A: torch.Tensor, jitter: float = 1e-6, max_tries: int = 5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Batched safe Cholesky: tries A + jitter*I, increases jitter if needed.\n",
    "    Falls back to SPD projection if still failing.\n",
    "    \"\"\"\n",
    "    A = symmetrize(A)\n",
    "\n",
    "    if A.dim() == 2:\n",
    "        I = torch.eye(A.shape[-1], device=A.device, dtype=A.dtype)\n",
    "    else:\n",
    "        B = A.shape[0]\n",
    "        I = torch.eye(A.shape[-1], device=A.device, dtype=A.dtype).expand(B, -1, -1)\n",
    "\n",
    "    for i in range(max_tries):\n",
    "        try:\n",
    "            return torch.linalg.cholesky(A + (jitter * (10**i)) * I)\n",
    "        except RuntimeError:\n",
    "            continue\n",
    "\n",
    "    A_spd = project_spd(A, eps=jitter)\n",
    "    return torch.linalg.cholesky(A_spd)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Unscented transform (weights + sigma points)\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class UTParams:\n",
    "    alpha: float = 0.8\n",
    "    beta: float = 2.0\n",
    "    kappa: float = 0.0\n",
    "\n",
    "def unscented_weights(n: int, params: UTParams, device=None, dtype=None):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      wm: (2n+1,)\n",
    "      wc: (2n+1,)\n",
    "      sqrt_c: scalar float = sqrt(n + lambda)\n",
    "    \"\"\"\n",
    "    alpha, beta, kappa = params.alpha, params.beta, params.kappa\n",
    "    lam = alpha**2 * (n + kappa) - n\n",
    "    c = n + lam\n",
    "\n",
    "    wm = torch.full((2*n+1,), 1.0/(2.0*c), device=device, dtype=dtype)\n",
    "    wc = torch.full((2*n+1,), 1.0/(2.0*c), device=device, dtype=dtype)\n",
    "    wm[0] = lam / c\n",
    "    wc[0] = lam / c + (1.0 - alpha**2 + beta)\n",
    "\n",
    "    sqrt_c = math.sqrt(c)\n",
    "    return wm, wc, sqrt_c\n",
    "\n",
    "def sigma_points(mean: torch.Tensor, cov: torch.Tensor, sqrt_c: float, jitter: float = 1e-6):\n",
    "    \"\"\"\n",
    "    mean: (B,n), cov: (B,n,n)\n",
    "    returns sigma: (B,2n+1,n)\n",
    "    \"\"\"\n",
    "    B, n = mean.shape\n",
    "    L = safe_cholesky(cov, jitter=jitter)     # (B,n,n) lower\n",
    "    scaled = sqrt_c * L                       # (B,n,n)\n",
    "    A = scaled.transpose(1, 2)                # (B,n,n) where A[:,i,:] = column i of scaled\n",
    "\n",
    "    mean_exp = mean.unsqueeze(1)              # (B,1,n)\n",
    "    sig0 = mean_exp\n",
    "    sig_plus = mean_exp + A\n",
    "    sig_minus = mean_exp - A\n",
    "    return torch.cat([sig0, sig_plus, sig_minus], dim=1)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Toy dynamics: Duffing oscillator\n",
    "# z = [p, v, k, alpha] where theta=[k,alpha] only in f\n",
    "# y = [p, v] + noise\n",
    "# -----------------------------\n",
    "def duffing_f(z: torch.Tensor, u: torch.Tensor, dt: float = 0.05, c: float = 0.25):\n",
    "    \"\"\"\n",
    "    z: (...,4), u: broadcastable to (...)\n",
    "    returns z_next with same leading dims.\n",
    "    \"\"\"\n",
    "    p = z[..., 0]\n",
    "    v = z[..., 1]\n",
    "    k = z[..., 2]\n",
    "    alpha = z[..., 3]\n",
    "\n",
    "    p_next = p + dt * v\n",
    "    v_next = v + dt * (-c * v - k * p - alpha * (p**3) + u)\n",
    "\n",
    "    # theta random walk is handled via Q_theta in covariance; deterministic part keeps theta\n",
    "    k_next = k\n",
    "    alpha_next = alpha\n",
    "\n",
    "    return torch.stack([p_next, v_next, k_next, alpha_next], dim=-1)\n",
    "\n",
    "def h_identity_x(z: torch.Tensor):\n",
    "    \"\"\"Measurement model: y = x + noise, where x=[p,v].\"\"\"\n",
    "    return z[..., 0:2]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# UKF core ops (predict + measurement stats)\n",
    "# -----------------------------\n",
    "def ukf_predict(z: torch.Tensor, P: torch.Tensor, u_prev: torch.Tensor, Q: torch.Tensor,\n",
    "                wm: torch.Tensor, wc: torch.Tensor, sqrt_c: float,\n",
    "                dt: float = 0.05, c: float = 0.25,\n",
    "                jitter: float = 1e-6):\n",
    "    \"\"\"\n",
    "    z: (B,n), P: (B,n,n), u_prev: (B,)\n",
    "    Q: (n,n) or (B,n,n)\n",
    "    returns z_pred, P_pred, X_sigma (predicted sigma points)\n",
    "    \"\"\"\n",
    "    B, n = z.shape\n",
    "    sig = sigma_points(z, P, sqrt_c, jitter=jitter)  # (B,S,n)\n",
    "\n",
    "    u_b = u_prev.view(B, 1)                          # (B,1) broadcast to sigma dim\n",
    "    X_sigma = duffing_f(sig, u_b, dt=dt, c=c)        # (B,S,n)\n",
    "\n",
    "    z_pred = torch.sum(X_sigma * wm.view(1, -1, 1), dim=1)  # (B,n)\n",
    "\n",
    "    dX = X_sigma - z_pred.unsqueeze(1)                      # (B,S,n)\n",
    "    P_pred = torch.einsum('bsi,bsj,s->bij', dX, dX, wc)     # (B,n,n)\n",
    "\n",
    "    if Q.dim() == 2:\n",
    "        P_pred = P_pred + Q.unsqueeze(0)\n",
    "    else:\n",
    "        P_pred = P_pred + Q\n",
    "\n",
    "    P_pred = symmetrize(P_pred) + jitter * torch.eye(n, device=z.device, dtype=z.dtype).unsqueeze(0)\n",
    "    return z_pred, P_pred, X_sigma\n",
    "\n",
    "def ukf_measurement_stats(z_pred: torch.Tensor, P_pred: torch.Tensor, X_sigma: torch.Tensor,\n",
    "                          R: torch.Tensor, wm: torch.Tensor, wc: torch.Tensor,\n",
    "                          jitter: float = 1e-6):\n",
    "    \"\"\"\n",
    "    Build y_pred, S, P_zy from predicted sigma points.\n",
    "    \"\"\"\n",
    "    B, S, n = X_sigma.shape\n",
    "    ny = R.shape[-1]\n",
    "\n",
    "    Y_sigma = h_identity_x(X_sigma)                        # (B,S,ny)\n",
    "    y_pred = torch.sum(Y_sigma * wm.view(1, -1, 1), dim=1) # (B,ny)\n",
    "\n",
    "    dY = Y_sigma - y_pred.unsqueeze(1)                     # (B,S,ny)\n",
    "    dX = X_sigma - z_pred.unsqueeze(1)                     # (B,S,n)\n",
    "\n",
    "    S_cov = torch.einsum('bsi,bsj,s->bij', dY, dY, wc)      # (B,ny,ny)\n",
    "    if R.dim() == 2:\n",
    "        S_cov = S_cov + R.unsqueeze(0)\n",
    "    else:\n",
    "        S_cov = S_cov + R\n",
    "\n",
    "    S_cov = symmetrize(S_cov) + jitter * torch.eye(ny, device=z_pred.device, dtype=z_pred.dtype).unsqueeze(0)\n",
    "    P_zy = torch.einsum('bsi,bsj,s->bij', dX, dY, wc)       # (B,n,ny)\n",
    "    return y_pred, S_cov, P_zy\n",
    "\n",
    "def compute_K_ukf(P_zy: torch.Tensor, S: torch.Tensor, jitter: float = 1e-6):\n",
    "    \"\"\"\n",
    "    K = P_zy S^{-1} using Cholesky solve.\n",
    "    \"\"\"\n",
    "    cholS = safe_cholesky(S, jitter=jitter)  # (B,ny,ny)\n",
    "    K_T = torch.cholesky_solve(P_zy.transpose(-1, -2), cholS)  # (B,ny,n)\n",
    "    K = K_T.transpose(-1, -2)  # (B,n,ny)\n",
    "    return K, cholS\n",
    "\n",
    "def generalized_joseph(P_pred: torch.Tensor, K: torch.Tensor, S: torch.Tensor, P_zy: torch.Tensor,\n",
    "                       jitter: float = 1e-6):\n",
    "    \"\"\"\n",
    "    Generalized Joseph update (safe for non-optimal learned K):\n",
    "      P_post = P_pred - K P_yz - P_zy K^T + K S K^T\n",
    "    \"\"\"\n",
    "    P_yz = P_zy.transpose(-1, -2)                     # (B,ny,n)\n",
    "    P_post = P_pred - (K @ P_yz) - (P_zy @ K.transpose(-1, -2)) + (K @ S @ K.transpose(-1, -2))\n",
    "    P_post = symmetrize(P_post) + jitter * torch.eye(P_pred.shape[-1], device=P_pred.device, dtype=P_pred.dtype).unsqueeze(0)\n",
    "    return P_post\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# GainNet (Encoder-MLP -> GRUCell -> heads)\n",
    "# Outputs: deltaK(4x2), rho_x, rho_theta\n",
    "# -----------------------------\n",
    "class GainNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_in: int = 20,\n",
    "                 hidden: int = 32,\n",
    "                 embed: int = 32,\n",
    "                 deltaK_scale: float = 0.1,\n",
    "                 rho_theta_max: float = 0.3,\n",
    "                 init_rho_bias=(-3.0, -5.0)):\n",
    "        super().__init__()\n",
    "        self.deltaK_scale = deltaK_scale\n",
    "        self.rho_theta_max = rho_theta_max\n",
    "\n",
    "        self.ln = nn.LayerNorm(d_in)\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(d_in, embed), nn.ReLU(),\n",
    "            nn.Linear(embed, embed), nn.ReLU(),\n",
    "        )\n",
    "        self.gru = nn.GRUCell(embed, hidden)\n",
    "\n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.dk_head = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, 8),  # 4x2\n",
    "        )\n",
    "        self.gate_head = nn.Linear(hidden, 2)\n",
    "\n",
    "        # Init: start close to UKF (ΔK ~ 0, rho ~ small)\n",
    "        nn.init.zeros_(self.dk_head[-1].weight)\n",
    "        nn.init.zeros_(self.dk_head[-1].bias)\n",
    "\n",
    "        nn.init.zeros_(self.gate_head.weight)\n",
    "        self.gate_head.bias.data[:] = torch.tensor(init_rho_bias, dtype=self.gate_head.bias.dtype)\n",
    "\n",
    "    def forward(self, feat: torch.Tensor, h: torch.Tensor, K_ukf: torch.Tensor):\n",
    "        \"\"\"\n",
    "        feat: (B,20)\n",
    "        h: (B,hidden)\n",
    "        K_ukf: (B,4,2) for scaling\n",
    "        \"\"\"\n",
    "        x = self.ln(feat)\n",
    "        x = self.enc(x)\n",
    "        h_new = self.gru(x, h)\n",
    "\n",
    "        t = self.trunk(h_new)\n",
    "\n",
    "        dk_raw = self.dk_head(t)          # (B,8)\n",
    "        gate_raw = self.gate_head(t)      # (B,2)\n",
    "\n",
    "        rho_x = torch.sigmoid(gate_raw[:, 0:1])                       # (B,1)\n",
    "        rho_theta = self.rho_theta_max * torch.sigmoid(gate_raw[:, 1:2])\n",
    "\n",
    "        # ΔK scaled relative to ||K_ukf|| to prevent blow-up\n",
    "        scale = self.deltaK_scale * torch.linalg.norm(K_ukf.reshape(K_ukf.shape[0], -1),\n",
    "                                                      dim=-1, keepdim=True)  # (B,1)\n",
    "        deltaK = (scale * torch.tanh(dk_raw)).view(-1, 4, 2)\n",
    "\n",
    "        return deltaK, rho_x, rho_theta, h_new\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# UKN filter (Plan B): UKF backbone + GainNet correction\n",
    "# Feature: Core-20\n",
    "# -----------------------------\n",
    "class UKNFilter(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dt: float = 0.05,\n",
    "                 c: float = 0.25,\n",
    "                 ut_params: UTParams = UTParams(alpha=0.8, beta=2.0, kappa=0.0),\n",
    "                 Q: torch.Tensor = None,\n",
    "                 R: torch.Tensor = None,\n",
    "                 jitter: float = 1e-6,\n",
    "                 feature_eps: float = 1e-8,\n",
    "                 hidden: int = 32,\n",
    "                 deltaK_scale: float = 0.1,\n",
    "                 rho_theta_max: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.dt = dt\n",
    "        self.c = c\n",
    "        self.jitter = jitter\n",
    "        self.feature_eps = feature_eps\n",
    "\n",
    "        self.n_z = 4\n",
    "        self.n_y = 2\n",
    "\n",
    "        if Q is None:\n",
    "            # Q = diag([Qp, Qv, Qk, Qalpha])\n",
    "            Q = torch.diag(torch.tensor([1e-5, 1e-4, 1e-6, 1e-6], dtype=torch.float32))\n",
    "        if R is None:\n",
    "            R = torch.diag(torch.tensor([0.05**2, 0.05**2], dtype=torch.float32))\n",
    "\n",
    "        self.register_buffer(\"Q\", Q)\n",
    "        self.register_buffer(\"R\", R)\n",
    "\n",
    "        wm, wc, sqrt_c = unscented_weights(self.n_z, ut_params, device=Q.device, dtype=Q.dtype)\n",
    "        self.register_buffer(\"wm\", wm)\n",
    "        self.register_buffer(\"wc\", wc)\n",
    "        self.sqrt_c = sqrt_c\n",
    "\n",
    "        self.hidden = hidden\n",
    "        self.gain_net = GainNet(d_in=20, hidden=hidden, embed=32,\n",
    "                                deltaK_scale=deltaK_scale,\n",
    "                                rho_theta_max=rho_theta_max)\n",
    "\n",
    "    def _build_features(self,\n",
    "                        nu: torch.Tensor,\n",
    "                        cholS: torch.Tensor,\n",
    "                        z_pred: torch.Tensor,\n",
    "                        P_pred: torch.Tensor,\n",
    "                        P_zy: torch.Tensor,\n",
    "                        K_ukf: torch.Tensor,\n",
    "                        z_prev_ref: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Core-20 features:\n",
    "          [tilde_nu(2), NIS(1), logdetS(1), delta_z_pred(4),\n",
    "           logdiagP(4), vec(C_theta_y)(4), delta_z_ukf(4)] => 20\n",
    "        \"\"\"\n",
    "        B = nu.shape[0]\n",
    "\n",
    "        # whitened innovation: tilde_nu = L_S^{-1} nu\n",
    "        tilde_nu = torch.linalg.solve_triangular(cholS, nu.unsqueeze(-1), upper=False).squeeze(-1)  # (B,2)\n",
    "\n",
    "        nis = torch.sum(tilde_nu**2, dim=-1, keepdim=True)  # (B,1)\n",
    "\n",
    "        logdetS = 2.0 * torch.sum(\n",
    "            torch.log(torch.diagonal(cholS, dim1=-2, dim2=-1) + self.feature_eps),\n",
    "            dim=-1, keepdim=True\n",
    "        )  # (B,1)\n",
    "\n",
    "        delta_z_pred = z_pred - z_prev_ref  # (B,4)\n",
    "\n",
    "        logdiagP = torch.log(torch.diagonal(P_pred, dim1=-2, dim2=-1) + self.feature_eps)  # (B,4)\n",
    "\n",
    "        # whitened cross-cov for theta-y:\n",
    "        # C_theta_y = L_theta^{-1} P_theta_y L_S^{-T}\n",
    "        P_theta_y = P_zy[:, 2:4, :]                # (B,2,2)\n",
    "        P_theta_theta = P_pred[:, 2:4, 2:4]        # (B,2,2)\n",
    "\n",
    "        L_theta = safe_cholesky(P_theta_theta, jitter=self.jitter)   # (B,2,2)\n",
    "        C_temp = torch.linalg.solve_triangular(L_theta, P_theta_y, upper=False)  # (B,2,2)\n",
    "\n",
    "        I = torch.eye(self.n_y, device=nu.device, dtype=nu.dtype).expand(B, -1, -1)\n",
    "        inv_LS = torch.linalg.solve_triangular(cholS, I, upper=False)            # (B,2,2) = L_S^{-1}\n",
    "\n",
    "        C_theta_y = C_temp @ inv_LS.transpose(-1, -2)                            # (B,2,2)\n",
    "        vecC = C_theta_y.reshape(B, -1)                                          # (B,4)\n",
    "\n",
    "        delta_z_ukf = (K_ukf @ nu.unsqueeze(-1)).squeeze(-1)                     # (B,4)\n",
    "\n",
    "        feat = torch.cat([tilde_nu, nis, logdetS, delta_z_pred, logdiagP, vecC, delta_z_ukf], dim=-1)\n",
    "        return feat  # (B,20)\n",
    "\n",
    "    def forward(self,\n",
    "                y: torch.Tensor,      # (B,T,2)\n",
    "                u: torch.Tensor,      # (B,T)\n",
    "                z0: torch.Tensor,     # (B,4) prior mean at time 0\n",
    "                P0: torch.Tensor,     # (B,4,4) prior cov at time 0\n",
    "                return_debug: bool = False):\n",
    "        B, T, _ = y.shape\n",
    "        device = y.device\n",
    "        dtype = y.dtype\n",
    "\n",
    "        z = z0\n",
    "        P = P0\n",
    "        h = torch.zeros(B, self.hidden, device=device, dtype=dtype)\n",
    "\n",
    "        z_hist = []\n",
    "        debug = []\n",
    "\n",
    "        z_prev = z.clone()\n",
    "\n",
    "        for t in range(T):\n",
    "            if t == 0:\n",
    "                # no predict, use prior directly\n",
    "                z_pred, P_pred = z, P\n",
    "                sig = sigma_points(z_pred, P_pred, self.sqrt_c, jitter=self.jitter)\n",
    "                X_sigma = sig\n",
    "                z_prev_ref = z_pred  # makes delta_z_pred = 0 at t=0\n",
    "            else:\n",
    "                u_prev = u[:, t-1]\n",
    "                z_pred, P_pred, X_sigma = ukf_predict(z, P, u_prev, self.Q,\n",
    "                                                      self.wm, self.wc, self.sqrt_c,\n",
    "                                                      dt=self.dt, c=self.c, jitter=self.jitter)\n",
    "                z_prev_ref = z_prev\n",
    "\n",
    "            y_pred, S, P_zy = ukf_measurement_stats(z_pred, P_pred, X_sigma, self.R,\n",
    "                                                    self.wm, self.wc, jitter=self.jitter)\n",
    "\n",
    "            nu = y[:, t, :] - y_pred  # innovation (B,2)\n",
    "            K_ukf, cholS = compute_K_ukf(P_zy, S, jitter=self.jitter)\n",
    "\n",
    "            feat = self._build_features(nu, cholS, z_pred, P_pred, P_zy, K_ukf, z_prev_ref)\n",
    "\n",
    "            deltaK, rho_x, rho_theta, h = self.gain_net(feat, h, K_ukf)\n",
    "\n",
    "            # row-wise gating: first two rows for x, last two rows for theta\n",
    "            g_row = torch.cat([rho_x, rho_x, rho_theta, rho_theta], dim=-1)  # (B,4)\n",
    "            K = K_ukf + deltaK * g_row.unsqueeze(-1)                         # (B,4,2)\n",
    "\n",
    "            z_post = z_pred + (K @ nu.unsqueeze(-1)).squeeze(-1)             # (B,4)\n",
    "            P_post = generalized_joseph(P_pred, K, S, P_zy, jitter=self.jitter)\n",
    "\n",
    "            z_hist.append(z_post)\n",
    "\n",
    "            if return_debug:\n",
    "                debug.append({\n",
    "                    \"rho_x\": rho_x.detach(),\n",
    "                    \"rho_theta\": rho_theta.detach(),\n",
    "                    \"nu\": nu.detach(),\n",
    "                    \"K_ukf\": K_ukf.detach(),\n",
    "                    \"K\": K.detach(),\n",
    "                })\n",
    "\n",
    "            z_prev = z_post\n",
    "            z, P = z_post, P_post\n",
    "\n",
    "        z_filt = torch.stack(z_hist, dim=1)  # (B,T,4)\n",
    "        if return_debug:\n",
    "            return z_filt, debug\n",
    "        return z_filt\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Baseline: Augmented UKF (no NN)\n",
    "# -----------------------------\n",
    "class AugmentedUKF(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dt: float = 0.05,\n",
    "                 c: float = 0.25,\n",
    "                 ut_params: UTParams = UTParams(alpha=0.8, beta=2.0, kappa=0.0),\n",
    "                 Q: torch.Tensor = None,\n",
    "                 R: torch.Tensor = None,\n",
    "                 jitter: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.dt = dt\n",
    "        self.c = c\n",
    "        self.jitter = jitter\n",
    "        self.n_z = 4\n",
    "        self.n_y = 2\n",
    "\n",
    "        if Q is None:\n",
    "            Q = torch.diag(torch.tensor([1e-5, 1e-4, 1e-6, 1e-6], dtype=torch.float32))\n",
    "        if R is None:\n",
    "            R = torch.diag(torch.tensor([0.05**2, 0.05**2], dtype=torch.float32))\n",
    "\n",
    "        self.register_buffer(\"Q\", Q)\n",
    "        self.register_buffer(\"R\", R)\n",
    "\n",
    "        wm, wc, sqrt_c = unscented_weights(self.n_z, ut_params, device=Q.device, dtype=Q.dtype)\n",
    "        self.register_buffer(\"wm\", wm)\n",
    "        self.register_buffer(\"wc\", wc)\n",
    "        self.sqrt_c = sqrt_c\n",
    "\n",
    "    def forward(self, y: torch.Tensor, u: torch.Tensor, z0: torch.Tensor, P0: torch.Tensor):\n",
    "        B, T, _ = y.shape\n",
    "        z = z0\n",
    "        P = P0\n",
    "\n",
    "        z_hist = []\n",
    "        for t in range(T):\n",
    "            if t == 0:\n",
    "                z_pred, P_pred = z, P\n",
    "                sig = sigma_points(z_pred, P_pred, self.sqrt_c, jitter=self.jitter)\n",
    "                X_sigma = sig\n",
    "            else:\n",
    "                u_prev = u[:, t-1]\n",
    "                z_pred, P_pred, X_sigma = ukf_predict(z, P, u_prev, self.Q,\n",
    "                                                      self.wm, self.wc, self.sqrt_c,\n",
    "                                                      dt=self.dt, c=self.c, jitter=self.jitter)\n",
    "\n",
    "            y_pred, S, P_zy = ukf_measurement_stats(z_pred, P_pred, X_sigma, self.R,\n",
    "                                                    self.wm, self.wc, jitter=self.jitter)\n",
    "            nu = y[:, t, :] - y_pred\n",
    "            K_ukf, _ = compute_K_ukf(P_zy, S, jitter=self.jitter)\n",
    "\n",
    "            z_post = z_pred + (K_ukf @ nu.unsqueeze(-1)).squeeze(-1)\n",
    "            P_post = generalized_joseph(P_pred, K_ukf, S, P_zy, jitter=self.jitter)\n",
    "\n",
    "            z_hist.append(z_post)\n",
    "            z, P = z_post, P_post\n",
    "\n",
    "        return torch.stack(z_hist, dim=1)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Data simulation (Duffing toy with optional outliers)\n",
    "# -----------------------------\n",
    "def simulate_duffing_batch(B: int = 64,\n",
    "                           T: int = 200,\n",
    "                           dt: float = 0.05,\n",
    "                           c: float = 0.25,\n",
    "                           outlier: bool = True,\n",
    "                           seed: int = 0):\n",
    "    \"\"\"\n",
    "    Returns torch tensors:\n",
    "      x: (B,T,2) true state\n",
    "      y: (B,T,2) measurements\n",
    "      theta: (B,2) true params [k, alpha]\n",
    "      u: (B,T) input (piecewise constant)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # True process noise (simulation)\n",
    "    Qx = np.diag([1e-5, 1e-4])\n",
    "\n",
    "    # Measurement noise\n",
    "    sigma = 0.05\n",
    "    R0 = np.diag([sigma**2, sigma**2])\n",
    "    p_out = 0.05\n",
    "    Rout = 25.0 * R0\n",
    "\n",
    "    # input\n",
    "    M = 5\n",
    "\n",
    "    x = np.zeros((B, T, 2), dtype=np.float32)\n",
    "    y = np.zeros((B, T, 2), dtype=np.float32)\n",
    "    u = np.zeros((B, T), dtype=np.float32)\n",
    "    theta = np.zeros((B, 2), dtype=np.float32)\n",
    "\n",
    "    for b in range(B):\n",
    "        k = rng.uniform(0.8, 1.2)\n",
    "        alpha = rng.uniform(0.2, 0.6)\n",
    "        theta[b] = [k, alpha]\n",
    "\n",
    "        for t in range(T):\n",
    "            if t % M == 0:\n",
    "                u[b, t] = rng.uniform(-1.0, 1.0)\n",
    "            else:\n",
    "                u[b, t] = u[b, t-1]\n",
    "\n",
    "        # initial state: make nonlinearity visible\n",
    "        x[b, 0, 0] = rng.normal(1.0, 0.2)  # p0\n",
    "        x[b, 0, 1] = rng.normal(0.0, 0.2)  # v0\n",
    "\n",
    "        for t in range(T-1):\n",
    "            p, v = x[b, t]\n",
    "            w = rng.multivariate_normal(np.zeros(2), Qx)\n",
    "\n",
    "            p_next = p + dt * v + w[0]\n",
    "            v_next = v + dt * (-c*v - k*p - alpha*(p**3) + u[b, t]) + w[1]\n",
    "            x[b, t+1] = [p_next, v_next]\n",
    "\n",
    "        # measurement\n",
    "        for t in range(T):\n",
    "            if outlier and (rng.uniform() < p_out):\n",
    "                v_meas = rng.multivariate_normal(np.zeros(2), Rout)\n",
    "            else:\n",
    "                v_meas = rng.multivariate_normal(np.zeros(2), R0)\n",
    "            y[b, t] = x[b, t] + v_meas\n",
    "\n",
    "    return (torch.tensor(x), torch.tensor(y), torch.tensor(theta), torch.tensor(u))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Demo: forward + optional training loop\n",
    "# -----------------------------\n",
    "def make_initial_prior(y: torch.Tensor,\n",
    "                       theta_prior=(1.0, 0.4),\n",
    "                       P0_diag=(0.1**2, 0.1**2, 0.5**2, 0.5**2)):\n",
    "    \"\"\"\n",
    "    Use y[:,0] as x0 prior, fixed theta prior.\n",
    "    \"\"\"\n",
    "    B = y.shape[0]\n",
    "    z0 = torch.zeros((B, 4), dtype=y.dtype, device=y.device)\n",
    "    z0[:, 0:2] = y[:, 0, :]  # since we measure p,v\n",
    "\n",
    "    z0[:, 2] = float(theta_prior[0])\n",
    "    z0[:, 3] = float(theta_prior[1])\n",
    "\n",
    "    P0 = torch.diag(torch.tensor(P0_diag, dtype=y.dtype, device=y.device)).unsqueeze(0).repeat(B, 1, 1)\n",
    "    return z0, P0\n",
    "\n",
    "def train_quick_demo(device=\"cpu\"):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # Models\n",
    "    ukn = UKNFilter().to(device)\n",
    "    ukf = AugmentedUKF().to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(ukn.parameters(), lr=1e-3)\n",
    "\n",
    "    lambda_theta = 1.0\n",
    "    lambda_smooth = 0.1\n",
    "    lambda_dK = 1e-3\n",
    "    margin = 0.0         # (원하면 >0로 해서 \"UKF보다 더 좋아져라\" 마진 부여)\n",
    "    lambda_dom = 1.0\n",
    "\n",
    "    for step in range(200):\n",
    "        x, y, theta, u = simulate_duffing_batch(B=64, T=120, outlier=True, seed=step)\n",
    "        x = x.to(device); y = y.to(device); theta = theta.to(device); u = u.to(device)\n",
    "\n",
    "        z0, P0 = make_initial_prior(y)\n",
    "        z0 = z0.to(device); P0 = P0.to(device)\n",
    "\n",
    "        z_hat = ukn(y, u, z0, P0)               # (B,T,4)\n",
    "        x_hat = z_hat[..., 0:2]\n",
    "        th_hat = z_hat[..., 2:4]\n",
    "\n",
    "        # supervised losses\n",
    "        loss_x = F.mse_loss(x_hat, x)\n",
    "        th_true_seq = theta[:, None, :].expand_as(th_hat)\n",
    "        loss_th = F.mse_loss(th_hat, th_true_seq)\n",
    "\n",
    "        # theta smoothness (since theta is constant per episode)\n",
    "        loss_smooth = F.mse_loss(th_hat[:, 1:, :] - th_hat[:, :-1, :], torch.zeros_like(th_hat[:, 1:, :]))\n",
    "\n",
    "        # small regularization to keep ΔK small (optional)\n",
    "        # (간단히 rho*deltaK를 직접 꺼내기 귀찮으면, 모델 파라미터 L2로도 대체 가능)\n",
    "        reg = 0.0\n",
    "        for p in ukn.parameters():\n",
    "            reg = reg + (p**2).mean()\n",
    "\n",
    "        # baseline dominance vs UKF (optional but fits your \"must be better\" goal)\n",
    "        with torch.no_grad():\n",
    "            z_ukf = ukf(y, u, z0, P0)\n",
    "            x_ukf = z_ukf[..., 0:2]\n",
    "            th_ukf = z_ukf[..., 2:4]\n",
    "            loss_ukf = F.mse_loss(x_ukf, x) + lambda_theta * F.mse_loss(th_ukf, th_true_seq)\n",
    "\n",
    "        loss_ukn = loss_x + lambda_theta * loss_th + lambda_smooth * loss_smooth + lambda_dK * reg\n",
    "        dom = F.relu(loss_ukn - loss_ukf + margin)\n",
    "        loss = loss_ukn + lambda_dom * dom\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(ukn.parameters(), max_norm=5.0)\n",
    "        opt.step()\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            print(f\"step {step:04d} | loss={loss.item():.5f} | ukf={loss_ukf.item():.5f} \"\n",
    "                  f\"| x={loss_x.item():.5f} th={loss_th.item():.5f} dom={dom.item():.5f}\")\n",
    "\n",
    "    # quick eval\n",
    "    x, y, theta, u = simulate_duffing_batch(B=256, T=200, outlier=True, seed=999)\n",
    "    x = x.to(device); y = y.to(device); theta = theta.to(device); u = u.to(device)\n",
    "    z0, P0 = make_initial_prior(y)\n",
    "    z0 = z0.to(device); P0 = P0.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z_hat = ukn(y, u, z0, P0)\n",
    "        z_ukf = ukf(y, u, z0, P0)\n",
    "\n",
    "        rmse_ukn_x = torch.sqrt(((z_hat[...,0:2]-x)**2).mean()).item()\n",
    "        rmse_ukf_x = torch.sqrt(((z_ukf[...,0:2]-x)**2).mean()).item()\n",
    "\n",
    "        th_true_seq = theta[:, None, :].expand_as(z_hat[...,2:4])\n",
    "        rmse_ukn_th = torch.sqrt(((z_hat[...,2:4]-th_true_seq)**2).mean()).item()\n",
    "        rmse_ukf_th = torch.sqrt(((z_ukf[...,2:4]-th_true_seq)**2).mean()).item()\n",
    "\n",
    "    print(\"\\n[EVAL on dirty(outlier) test]\")\n",
    "    print(f\"RMSE_x   UKN={rmse_ukn_x:.6f} | UKF={rmse_ukf_x:.6f}\")\n",
    "    print(f\"RMSE_th  UKN={rmse_ukn_th:.6f} | UKF={rmse_ukf_th:.6f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For CPU demo:\n",
    "    train_quick_demo(device=\"cpu\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
