{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69497288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Tuple, List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1efd0adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 0. CUDA 설정\n",
    "# ==========================================\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# (선택) matmul precision\n",
    "if hasattr(torch, \"set_float32_matmul_precision\"):\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "pin_mem = (device.type == \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ab0ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. Linear System Dataset (CPU 생성)\n",
    "# ==========================================\n",
    "class LinearSystemDataset:\n",
    "    \"\"\"\n",
    "    x_{t+1} = F x_t + w_t,  w ~ N(0,Q)\n",
    "    y_t     = H x_t + v_t,  v ~ N(0,R)\n",
    "\n",
    "    여기서는 q_scale, r_scale를 \"표준편차\"로 보고 Q=(q_scale^2)I, R=(r_scale^2)I로 둠.\n",
    "    \"\"\"\n",
    "    def __init__(self, T=50, N=600, m=2, n=2, q_scale=0.1, r_scale=1.0):\n",
    "        self.T = T\n",
    "        self.N = N\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "\n",
    "        theta = np.pi / 20  # rotation angle\n",
    "        self.F = torch.tensor([[np.cos(theta), -np.sin(theta)],\n",
    "                               [np.sin(theta),  np.cos(theta)]], dtype=torch.float32)\n",
    "        self.H = torch.eye(n, m, dtype=torch.float32)\n",
    "\n",
    "        self.Q = (q_scale**2) * torch.eye(m, dtype=torch.float32)\n",
    "        self.R = (r_scale**2) * torch.eye(n, dtype=torch.float32)\n",
    "\n",
    "    def generate_data(self):\n",
    "        X = torch.zeros(self.N, self.T, self.m, dtype=torch.float32)\n",
    "        Y = torch.zeros(self.N, self.T, self.n, dtype=torch.float32)\n",
    "\n",
    "        x_curr = torch.randn(self.N, self.m, dtype=torch.float32)\n",
    "\n",
    "        LQ = torch.linalg.cholesky(self.Q)\n",
    "        LR = torch.linalg.cholesky(self.R)\n",
    "\n",
    "        for t in range(self.T):\n",
    "            w = torch.randn(self.N, self.m) @ LQ.T\n",
    "            x_next = (self.F @ x_curr.T).T + w\n",
    "\n",
    "            v = torch.randn(self.N, self.n) @ LR.T\n",
    "            y_curr = (self.H @ x_next.T).T + v\n",
    "\n",
    "            X[:, t, :] = x_next\n",
    "            Y[:, t, :] = y_curr\n",
    "            x_curr = x_next\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047cc0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. UCKN Model (Unscented + Cholesky PDEL)\n",
    "# ==========================================\n",
    "def safe_cholesky(P, jitter=1e-6):\n",
    "    # 항상 약간의 jitter를 더해 안정화 (분기/try-except 없이 autograd friendly)\n",
    "    m = P.shape[-1]\n",
    "    I = torch.eye(m, device=P.device, dtype=P.dtype)\n",
    "    P = 0.5 * (P + P.transpose(-1, -2))\n",
    "    return torch.linalg.cholesky(P + jitter * I)\n",
    "\n",
    "class UnscentedCholeskyKalmanNet(nn.Module):\n",
    "    \"\"\"\n",
    "    - Unscented Transform로 prior 예측\n",
    "    - GRU 기반으로 Kalman Gain K_t 예측\n",
    "    - CKN 스타일 PDEL: Cholesky 파라미터화로 P_post SPD 보장\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n, F, H, Q, R,\n",
    "                 alpha=1, beta=0.0, kappa=0.0,\n",
    "                 hidden_dim=None,\n",
    "                 pdel_clip=5.0,\n",
    "                 diag_min=1e-4):\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "\n",
    "        # system matrices (buffers so .to(device) moves them)\n",
    "        self.register_buffer(\"F_sys\", F.clone())\n",
    "        self.register_buffer(\"H_sys\", H.clone())\n",
    "        self.register_buffer(\"Q_sys\", Q.clone())\n",
    "        self.register_buffer(\"R_sys\", R.clone())\n",
    "\n",
    "        # UT params\n",
    "        self.alpha = float(alpha)\n",
    "        self.beta = float(beta)\n",
    "        self.kappa = float(kappa)\n",
    "\n",
    "        lam = self.alpha**2 * (m + self.kappa) - m\n",
    "        c = m + lam\n",
    "        if c <= 0:\n",
    "            raise ValueError(f\"Unscented params invalid: m+lambda must be > 0, got {c}. \"\n",
    "                             f\"Try smaller alpha (e.g., 1e-3~0.2).\")\n",
    "        self.lam = lam\n",
    "        self.c = c\n",
    "        self.scale = math.sqrt(c)\n",
    "\n",
    "        # weights (register as buffers)\n",
    "        Wm = torch.full((2*m + 1,), 1.0/(2.0*c), dtype=torch.float32)\n",
    "        Wc = torch.full((2*m + 1,), 1.0/(2.0*c), dtype=torch.float32)\n",
    "        Wm[0] = lam / c\n",
    "        Wc[0] = lam / c + (1.0 - self.alpha**2 + self.beta)\n",
    "        self.register_buffer(\"Wm\", Wm)\n",
    "        self.register_buffer(\"Wc\", Wc)\n",
    "\n",
    "        # features: innovation(n) + state_diff(m)\n",
    "        feat_dim = n + m\n",
    "\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = m * 10  # user style\n",
    "\n",
    "        # \"GRU 양쪽 FC\" 구조: pre_fc -> GRUCell -> post_fc\n",
    "        self.pre_fc = nn.Sequential(\n",
    "            nn.Linear(feat_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.gru = nn.GRUCell(hidden_dim, hidden_dim)\n",
    "        self.post_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc_kg = nn.Linear(hidden_dim, m * n)\n",
    "\n",
    "        self.pdel_dim = m * (m + 1) // 2\n",
    "        self.fc_pdel = nn.Linear(hidden_dim, self.pdel_dim)\n",
    "\n",
    "        # stability knobs\n",
    "        self.pdel_clip = float(pdel_clip)\n",
    "        self.diag_min = float(diag_min)\n",
    "\n",
    "        # init small (안정)\n",
    "        nn.init.zeros_(self.fc_kg.weight); nn.init.zeros_(self.fc_kg.bias)\n",
    "        nn.init.zeros_(self.fc_pdel.weight); nn.init.zeros_(self.fc_pdel.bias)\n",
    "\n",
    "    def pdel_layer(self, out_vec):\n",
    "        \"\"\"\n",
    "        out_vec: [B, pdel_dim] -> P = L L^T, L lower-tri with softplus diag\n",
    "        \"\"\"\n",
    "        B = out_vec.shape[0]\n",
    "        m = self.m\n",
    "        out_vec = torch.clamp(out_vec, -self.pdel_clip, self.pdel_clip)\n",
    "\n",
    "        L = torch.zeros(B, m, m, device=out_vec.device, dtype=out_vec.dtype)\n",
    "        idx = torch.tril_indices(row=m, col=m, offset=0, device=out_vec.device)\n",
    "        L[:, idx[0], idx[1]] = out_vec\n",
    "\n",
    "        # diag positive\n",
    "        d = torch.arange(m, device=out_vec.device)\n",
    "        L[:, d, d] = torch.nn.functional.softplus(L[:, d, d]) + self.diag_min\n",
    "\n",
    "        P = L @ L.transpose(1, 2)\n",
    "        P = 0.5 * (P + P.transpose(1, 2))\n",
    "        return P\n",
    "\n",
    "    def sigma_points(self, x, P):\n",
    "        \"\"\"\n",
    "        x:[B,m], P:[B,m,m] -> Xi:[B,2m+1,m]\n",
    "        \"\"\"\n",
    "        B, m = x.shape\n",
    "        L = safe_cholesky(P, jitter=1e-6)          # [B,m,m]\n",
    "        U = self.scale * L                         # [B,m,m]\n",
    "        Ucols = U.transpose(1, 2)                  # [B,m,m] each row is a column-vector\n",
    "\n",
    "        x0 = x.unsqueeze(1)                        # [B,1,m]\n",
    "        Xi_plus = x0 + Ucols                       # [B,m,m]\n",
    "        Xi_minus = x0 - Ucols                      # [B,m,m]\n",
    "        Xi = torch.cat([x0, Xi_plus, Xi_minus], dim=1)  # [B,2m+1,m]\n",
    "        return Xi\n",
    "\n",
    "    def unscented_predict(self, x_post, P_post):\n",
    "        \"\"\"\n",
    "        prior from UT:\n",
    "        X_pred = f(Xi). 여기선 linear: f(x)=F x\n",
    "        P_prior += Q\n",
    "        \"\"\"\n",
    "        Xi = self.sigma_points(x_post, P_post)         # [B,L,m]\n",
    "        X_pred = Xi @ self.F_sys.T                     # [B,L,m]\n",
    "\n",
    "        Wm = self.Wm.view(1, -1, 1)                    # [1,L,1]\n",
    "        Wc = self.Wc.view(1, -1, 1)                    # [1,L,1]\n",
    "\n",
    "        x_prior = torch.sum(Wm * X_pred, dim=1)        # [B,m]\n",
    "        diff = X_pred - x_prior.unsqueeze(1)           # [B,L,m]\n",
    "        P_prior = diff.transpose(1, 2) @ (diff * Wc)   # [B,m,m]\n",
    "        P_prior = 0.5 * (P_prior + P_prior.transpose(1, 2)) + self.Q_sys.unsqueeze(0)\n",
    "        return x_prior, P_prior, X_pred\n",
    "\n",
    "    def unscented_obs_predict(self, X_pred):\n",
    "        \"\"\"\n",
    "        y_sig = h(X_pred). 여기선 linear: h(x)=H x\n",
    "        y_prior = sum Wm * y_sig\n",
    "        \"\"\"\n",
    "        y_sig = X_pred @ self.H_sys.T                  # [B,L,n]\n",
    "        Wm = self.Wm.view(1, -1, 1)\n",
    "        y_prior = torch.sum(Wm * y_sig, dim=1)         # [B,n]\n",
    "        return y_prior\n",
    "\n",
    "    def forward(self, Y, x_init, P_init):\n",
    "        \"\"\"\n",
    "        Y:[B,T,n]\n",
    "        returns x_est:[B,T,m], P_est:[B,T,m,m]\n",
    "        \"\"\"\n",
    "        B, T, _ = Y.shape\n",
    "        x_post = x_init\n",
    "        P_post = P_init\n",
    "        h = torch.zeros(B, self.gru.hidden_size, device=Y.device, dtype=Y.dtype)\n",
    "\n",
    "        x_hist = []\n",
    "        P_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "            x_prior, P_prior, X_pred = self.unscented_predict(x_post, P_post)\n",
    "            y_prior = self.unscented_obs_predict(X_pred)\n",
    "\n",
    "            innovation = Y[:, t, :] - y_prior          # [B,n]\n",
    "            state_diff = x_prior - x_post              # [B,m]\n",
    "            feat = torch.cat([innovation, state_diff], dim=1)  # [B,n+m]\n",
    "\n",
    "            u = self.pre_fc(feat)\n",
    "            h = self.gru(u, h)\n",
    "            h2 = self.post_fc(h)\n",
    "\n",
    "            K = self.fc_kg(h2).view(B, self.m, self.n)\n",
    "            P_post = self.pdel_layer(self.fc_pdel(h2))\n",
    "\n",
    "            correction = (K @ innovation.unsqueeze(-1)).squeeze(-1)  # [B,m]\n",
    "            x_post = x_prior + correction\n",
    "\n",
    "            x_hist.append(x_post)\n",
    "            P_hist.append(P_post)\n",
    "\n",
    "        return torch.stack(x_hist, dim=1), torch.stack(P_hist, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "488a4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. Loss: NLL + (small) MSE\n",
    "# ==========================================\n",
    "def nll_plus_mse(x_est, x_true, P_est, w_mse=0.5, eps=1e-5):\n",
    "    \"\"\"\n",
    "    NLL = 0.5 * (e^T P^{-1} e + logdet(P))\n",
    "    total = NLL + w_mse * MSE\n",
    "    \"\"\"\n",
    "    B, T, m = x_est.shape\n",
    "    e = (x_est - x_true)                              # [B,T,m]\n",
    "    P = P_est                                         # [B,T,m,m]\n",
    "\n",
    "    # flatten\n",
    "    e2 = e.reshape(B*T, m, 1)\n",
    "    P2 = P.reshape(B*T, m, m)\n",
    "\n",
    "    I = torch.eye(m, device=P2.device, dtype=P2.dtype).unsqueeze(0)\n",
    "    P2 = 0.5 * (P2 + P2.transpose(-1, -2)) + eps * I\n",
    "\n",
    "    L = torch.linalg.cholesky(P2)                      # [B*T,m,m]\n",
    "    sol = torch.cholesky_solve(e2, L)                  # [B*T,m,1]\n",
    "    maha = (e2.transpose(1, 2) @ sol).squeeze(-1).squeeze(-1)  # [B*T]\n",
    "\n",
    "    logdet = 2.0 * torch.log(torch.diagonal(L, dim1=-2, dim2=-1) + 1e-12).sum(dim=-1)  # [B*T]\n",
    "\n",
    "    nll = 0.5 * (maha + logdet).mean()\n",
    "    mse = (e**2).mean()\n",
    "    loss = nll + w_mse * mse\n",
    "\n",
    "    # NaN 보호 (혹시라도)\n",
    "    loss = torch.nan_to_num(loss, nan=1e6, posinf=1e6, neginf=1e6)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "329dab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. Train / Eval (mini-batch + GPU friendly)\n",
    "# ==========================================\n",
    "def train_linear_demo(\n",
    "    m=2, n=2,\n",
    "    T=50,\n",
    "    N_train=500,\n",
    "    N_test=100,\n",
    "    q_scale=0.1,\n",
    "    r_scale=1.0,\n",
    "    batch_size=128,\n",
    "    test_batch_size=256,\n",
    "    num_epochs=50,\n",
    "    lr=1e-3,\n",
    "    ut_alpha=0.5,\n",
    "    ut_beta=2.0,\n",
    "    ut_kappa=0.0,\n",
    "    w_mse=0.05,\n",
    "):\n",
    "    # ---- data (CPU) ----\n",
    "    dataset = LinearSystemDataset(T=T, N=N_train+N_test, m=m, n=n, q_scale=q_scale, r_scale=r_scale)\n",
    "    X, Y = dataset.generate_data()   # CPU tensors\n",
    "    F_sys, H_sys = dataset.F, dataset.H\n",
    "    Q_sys, R_sys = dataset.Q, dataset.R\n",
    "\n",
    "    X_train = X[:N_train]\n",
    "    Y_train = Y[:N_train]\n",
    "    X_test  = X[N_train:]\n",
    "    Y_test  = Y[N_train:]\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(Y_train, X_train),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,          # Windows 안정\n",
    "        pin_memory=pin_mem\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        TensorDataset(Y_test, X_test),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=pin_mem\n",
    "    )\n",
    "\n",
    "    # ---- model ----\n",
    "    model = UnscentedCholeskyKalmanNet(\n",
    "        m, n, F_sys, H_sys, Q_sys, R_sys,\n",
    "        alpha=ut_alpha, beta=ut_beta, kappa=ut_kappa,\n",
    "        hidden_dim=m*10,\n",
    "        pdel_clip=5.0,\n",
    "        diag_min=1e-4,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    I_m = torch.eye(m, device=device).unsqueeze(0)  # [1,m,m]\n",
    "\n",
    "    train_loss_hist = []\n",
    "    test_loss_hist  = []\n",
    "\n",
    "    print(\"Training...\")\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # -----------------\n",
    "        # Train\n",
    "        # -----------------\n",
    "        model.train()\n",
    "        total_train = 0.0\n",
    "        n_train = 0\n",
    "\n",
    "        for yb_cpu, xb_cpu in train_loader:\n",
    "            yb = yb_cpu.to(device, non_blocking=True)\n",
    "            xb = xb_cpu.to(device, non_blocking=True)\n",
    "            B = yb.size(0)\n",
    "\n",
    "            x_init = torch.zeros(B, m, device=device)\n",
    "            P_init = I_m.expand(B, -1, -1).contiguous()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            x_est, P_est = model(yb, x_init, P_init)\n",
    "\n",
    "            loss = nll_plus_mse(x_est, xb, P_est, w_mse=w_mse, eps=1e-5)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train += loss.item() * B\n",
    "            n_train += B\n",
    "\n",
    "        train_loss = total_train / max(n_train, 1)\n",
    "        train_loss_hist.append(train_loss)\n",
    "\n",
    "        # -----------------\n",
    "        # Test\n",
    "        # -----------------\n",
    "        model.eval()\n",
    "        total_test = 0.0\n",
    "        n_test = 0\n",
    "        with torch.no_grad():\n",
    "            for yb_cpu, xb_cpu in test_loader:\n",
    "                yb = yb_cpu.to(device, non_blocking=True)\n",
    "                xb = xb_cpu.to(device, non_blocking=True)\n",
    "                B = yb.size(0)\n",
    "\n",
    "                x_init = torch.zeros(B, m, device=device)\n",
    "                P_init = I_m.expand(B, -1, -1).contiguous()\n",
    "\n",
    "                x_est, P_est = model(yb, x_init, P_init)\n",
    "                loss_t = nll_plus_mse(x_est, xb, P_est, w_mse=w_mse, eps=1e-5)\n",
    "\n",
    "                total_test += loss_t.item() * B\n",
    "                n_test += B\n",
    "\n",
    "        test_loss = total_test / max(n_test, 1)\n",
    "        test_loss_hist.append(test_loss)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:03d} | Train Loss: {train_loss:.6f} | Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 5. Plots: Training curves\n",
    "    # ==========================================\n",
    "    epochs = np.arange(1, num_epochs + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_loss_hist, label=\"Train Loss\")\n",
    "    plt.plot(epochs, test_loss_hist, label=\"Test Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss (NLL + w*MSE)\")\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    # ==========================================\n",
    "    # 6. Final eval on full test set (one pass)\n",
    "    # ==========================================\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_all = Y_test.to(device)\n",
    "        x_all = X_test.to(device)\n",
    "        B = y_all.size(0)\n",
    "\n",
    "        x_init = torch.zeros(B, m, device=device)\n",
    "        P_init = I_m.expand(B, -1, -1).contiguous()\n",
    "\n",
    "        x_est_all, P_est_all = model(y_all, x_init, P_init)\n",
    "\n",
    "    # CPU로 내림\n",
    "    x_est_np = x_est_all.cpu().numpy()      # [N_test,T,m]\n",
    "    x_true_np = X_test.numpy()\n",
    "    P_np = P_est_all.cpu().numpy()          # [N_test,T,m,m]\n",
    "\n",
    "    err = (x_true_np - x_est_np)            # [N_test,T,m]\n",
    "    mean_err = err.mean(axis=0)             # [T,m]\n",
    "\n",
    "    # CI band: time별로 std를 샘플 평균내서 사용 (user가 쓰던 방식 유지)\n",
    "    var = np.diagonal(P_np, axis1=2, axis2=3)          # [N_test,T,m]\n",
    "    std = np.sqrt(np.maximum(var, 1e-12))              # [N_test,T,m]\n",
    "    mean_std = std.mean(axis=0)                        # [T,m]\n",
    "    z99 = 2.5758293035489004\n",
    "    ci = z99 * mean_std                                # [T,m]\n",
    "\n",
    "    t = np.arange(T)\n",
    "\n",
    "    # ==========================================\n",
    "    # 7. Plot: all errors (light gray) + mean + 99% CI\n",
    "    # ==========================================\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for i in range(m):\n",
    "        plt.subplot(1, m, i+1)\n",
    "\n",
    "        # 모든 샘플 error를 연한 회색으로\n",
    "        for s in range(err.shape[0]):\n",
    "            plt.plot(t, err[s, :, i], color=\"0.85\", linewidth=0.7, alpha=0.5)\n",
    "\n",
    "        # 95% CI band (0 중심)\n",
    "        plt.fill_between(t, -ci[:, i], ci[:, i], alpha=0.2, label=\"95% CI (from P)\")\n",
    "\n",
    "        # mean error\n",
    "        plt.plot(t, mean_err[:, i], label=\"Mean Error\")\n",
    "\n",
    "        plt.axhline(0.0, color=\"k\", linestyle=\"--\", alpha=0.5)\n",
    "        plt.title(f\"Error + 99% CI : state x{i+1}\")\n",
    "        plt.xlabel(\"Time step\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ==========================================\n",
    "    # 8. Plot: one sample trajectory + sample error/CI\n",
    "    # ==========================================\n",
    "    sample_idx = 0\n",
    "    xs_true = x_true_np[sample_idx]     # [T,m]\n",
    "    xs_est  = x_est_np[sample_idx]      # [T,m]\n",
    "    Ps      = P_np[sample_idx]          # [T,m,m]\n",
    "    err_s   = xs_true - xs_est          # [T,m]\n",
    "    std_s   = np.sqrt(np.maximum(np.diagonal(Ps, axis1=1, axis2=2), 1e-12))  # [T,m]\n",
    "    ci_s    = z99 * std_s               # [T,m]\n",
    "\n",
    "    # trajectory\n",
    "    for i in range(m):\n",
    "        plt.figure()\n",
    "        plt.plot(t, xs_true[:, i], label=f\"True x{i+1}\")\n",
    "        plt.plot(t, xs_est[:, i], label=f\"Est  x{i+1}\")\n",
    "        plt.xlabel(\"Time step\")\n",
    "        plt.ylabel(f\"x{i+1}\")\n",
    "        plt.title(f\"Sample Trajectory (idx={sample_idx})\")\n",
    "        plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # sample error + CI\n",
    "    for i in range(m):\n",
    "        plt.figure()\n",
    "        plt.plot(t, err_s[:, i], label=f\"Error x{i+1}\")\n",
    "        plt.fill_between(t, -ci_s[:, i], ci_s[:, i], alpha=0.2, label=\"95% CI (from P)\")\n",
    "        plt.axhline(0.0, color=\"k\", linestyle=\"--\", alpha=0.5)\n",
    "        plt.xlabel(\"Time step\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.title(f\"Sample Error + 95% CI (idx={sample_idx}, x{i+1})\")\n",
    "        plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.5)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return model, (train_loss_hist, test_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ce7d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 001 | Train Loss: 2.234144 | Test Loss: 0.868780\n",
      "Epoch 010 | Train Loss: -0.420100 | Test Loss: -0.450373\n",
      "Epoch 020 | Train Loss: -0.813008 | Test Loss: -0.826002\n",
      "Epoch 030 | Train Loss: -0.988051 | Test Loss: -0.937741\n",
      "Epoch 040 | Train Loss: -1.017907 | Test Loss: -0.969115\n",
      "Epoch 050 | Train Loss: -1.037259 | Test Loss: -0.978675\n",
      "Epoch 060 | Train Loss: -1.043826 | Test Loss: -0.984680\n",
      "Epoch 070 | Train Loss: -1.049619 | Test Loss: -0.991547\n",
      "Epoch 080 | Train Loss: -1.053043 | Test Loss: -0.996571\n",
      "Epoch 090 | Train Loss: -1.055289 | Test Loss: -0.994144\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 9. Run\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    model, (train_hist, test_hist) = train_linear_demo(\n",
    "        m=2, n=2,\n",
    "        T=50,\n",
    "        N_train=500,\n",
    "        N_test=100,\n",
    "        q_scale=0.1,\n",
    "        r_scale=1.0,\n",
    "        batch_size=128,\n",
    "        test_batch_size=256,\n",
    "        num_epochs=200,      # <-- epoch 늘리려면 여기\n",
    "        lr=1e-3,\n",
    "        ut_alpha=0.5,       # <-- 불안정하면 0.2 또는 1e-3로 낮춰보세요\n",
    "        ut_beta=2.0,\n",
    "        ut_kappa=0.0,\n",
    "        w_mse=1.0\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ukn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
