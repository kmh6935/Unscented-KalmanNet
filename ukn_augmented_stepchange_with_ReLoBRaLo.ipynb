{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3397a4b",
   "metadata": {},
   "source": [
    "## 9) Loss functions: MSE + Gaussian NLL (for P_hat)\n",
    "\n",
    "We combine them with **ReLoBRaLo** adaptive weights.\n",
    "\n",
    "- Compute component losses: `L_mse`, `L_nll`.\n",
    "- ReLoBRaLo assigns weights based on **relative progress** of each loss compared to a random lookback reference (initial vs previous),\n",
    "  using an EMA-smoothed loss signal.\n",
    "- We log `w_mse` and `w_nll` over training.\n",
    "\n",
    "> This replaces the previous learnable log-variance weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d26ae7e",
   "metadata": {},
   "source": [
    "## 0) Imports & device\n",
    "This cell imports dependencies and sets CPU/GPU device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0abf922",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a5a8b",
   "metadata": {},
   "source": [
    "## 1) Configuration\n",
    "All hyperparameters are grouped here for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e7e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Dimensions\n",
    "    nx: int = 3\n",
    "    nth: int = 2\n",
    "    ny: int = 2\n",
    "    nz: int = 5  # nx+nth\n",
    "\n",
    "    # Simulation\n",
    "    dt: float = 0.1\n",
    "    T: int = 200\n",
    "    n_train: int = 256\n",
    "    n_val: int = 64\n",
    "    n_test: int = 128\n",
    "\n",
    "    # Parameter step-change (single jump, then hold)\n",
    "    jump_frac: float = 0.10\n",
    "    jump_window: Tuple[int,int] = (60, 140)\n",
    "\n",
    "    # True noises (for synthetic data)\n",
    "    Qx_true_diag: Tuple[float,float,float] = (1e-4, 1e-4, 5e-5)\n",
    "    R_true_diag: Tuple[float,float] = (2e-3, 2e-3)\n",
    "\n",
    "    # Parameter random-walk base noise (relative to theta_ref)\n",
    "    qth_base_rel: float = 1e-5\n",
    "\n",
    "    # Jump detection via NIS (optional)\n",
    "    jump_M: int = 2\n",
    "    jump_J: int = 3\n",
    "    nis_tau: float = 9.21  # chi2(df=2) 99%\n",
    "\n",
    "    # Training\n",
    "    epochs: int = 30\n",
    "    batch_size: int = 32\n",
    "    lr: float = 5e-3\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # UKN model\n",
    "    gru_hidden: int = 32\n",
    "    freeze_R_during_jump: bool = True\n",
    "\n",
    "    # Jump logic toggle\n",
    "    enable_jump_logic_train: bool = False\n",
    "    enable_jump_logic_eval: bool = True\n",
    "\n",
    "CFG = Config()\n",
    "print(CFG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd36eb3c",
   "metadata": {},
   "source": [
    "## 2) Synthetic nonlinear system: f(x, θ), h(x)\n",
    "\n",
    "Example system (replace with your own):\n",
    "- State: `x=[p, v, b]`\n",
    "- Parameters: `θ=[k, c]` (enter the **state dynamics**)\n",
    "- Measurement: `y=[p + 0.1 b, v]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ffdede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f_x_theta(x: torch.Tensor, th: torch.Tensor, dt: float) -> torch.Tensor:\n",
    "    p, v, b = x[..., 0], x[..., 1], x[..., 2]\n",
    "    k, c = th[..., 0], th[..., 1]\n",
    "\n",
    "    p_next = p + dt * v\n",
    "    v_next = v + dt * (-k * torch.sin(p) - c * v + 0.1 * torch.sin(b))\n",
    "    b_next = b + dt * (0.05 * torch.cos(p))\n",
    "\n",
    "    return torch.stack([p_next, v_next, b_next], dim=-1)\n",
    "\n",
    "def h_x(x: torch.Tensor) -> torch.Tensor:\n",
    "    p, v, b = x[..., 0], x[..., 1], x[..., 2]\n",
    "    y1 = p + 0.1 * b\n",
    "    y2 = v\n",
    "    return torch.stack([y1, y2], dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b56f38",
   "metadata": {},
   "source": [
    "## 3) EKF Jacobians (augmented z=[x;θ])\n",
    "EKF baseline needs Jacobians:\n",
    "- `A = ∂f_aug/∂z` (5×5)\n",
    "- `H = ∂h/∂z` (2×5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec89ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jacobian_A_aug(z: torch.Tensor, dt: float) -> torch.Tensor:\n",
    "    p, v, b = z[..., 0], z[..., 1], z[..., 2]\n",
    "    k, c = z[..., 3], z[..., 4]\n",
    "\n",
    "    A = torch.zeros(z.shape[:-1] + (5, 5), device=z.device, dtype=z.dtype)\n",
    "\n",
    "    # p_next = p + dt v\n",
    "    A[..., 0, 0] = 1.0\n",
    "    A[..., 0, 1] = dt\n",
    "\n",
    "    # v_next = v + dt( -k sin(p) - c v + 0.1 sin(b) )\n",
    "    A[..., 1, 0] = dt * (-k * torch.cos(p))\n",
    "    A[..., 1, 1] = 1.0 + dt * (-c)\n",
    "    A[..., 1, 2] = dt * (0.1 * torch.cos(b))\n",
    "    A[..., 1, 3] = dt * (-torch.sin(p))\n",
    "    A[..., 1, 4] = dt * (-v)\n",
    "\n",
    "    # b_next = b + dt(0.05 cos(p))\n",
    "    A[..., 2, 0] = dt * (-0.05 * torch.sin(p))\n",
    "    A[..., 2, 2] = 1.0\n",
    "\n",
    "    # theta random walk\n",
    "    A[..., 3, 3] = 1.0\n",
    "    A[..., 4, 4] = 1.0\n",
    "    return A\n",
    "\n",
    "def jacobian_H_aug(device=None, dtype=None) -> torch.Tensor:\n",
    "    H = torch.zeros((2,5), dtype=dtype or torch.get_default_dtype(), device=device)\n",
    "    H[0,0] = 1.0\n",
    "    H[0,2] = 0.1\n",
    "    H[1,1] = 1.0\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b820b5",
   "metadata": {},
   "source": [
    "## 4) Data generation (single step-change event in θ)\n",
    "We simulate sequences where θ jumps once (±10%) at a random time within a window and then stays constant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f7c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_dataset(cfg: Config, n_seq: int, seed: int = 0) -> Dict[str, np.ndarray]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    T = cfg.T\n",
    "    dt = cfg.dt\n",
    "\n",
    "    th_ref = np.array([1.0, 0.2], dtype=np.float64)\n",
    "\n",
    "    Qx = np.diag(cfg.Qx_true_diag)\n",
    "    R  = np.diag(cfg.R_true_diag)\n",
    "\n",
    "    x_all = np.zeros((n_seq, T, cfg.nx), dtype=np.float64)\n",
    "    th_all = np.zeros((n_seq, T, cfg.nth), dtype=np.float64)\n",
    "    y_all = np.zeros((n_seq, T, cfg.ny), dtype=np.float64)\n",
    "    jump_t_all = np.zeros((n_seq,), dtype=np.int64)\n",
    "\n",
    "    for n in range(n_seq):\n",
    "        x0 = np.array([0.2, 0.0, 0.1], dtype=np.float64) + rng.normal(0, 0.05, size=(cfg.nx,))\n",
    "        th0 = th_ref * (1.0 + rng.normal(0, 0.03, size=(cfg.nth,)))\n",
    "\n",
    "        t_jump = rng.integers(cfg.jump_window[0], cfg.jump_window[1])\n",
    "        jump_t_all[n] = t_jump\n",
    "\n",
    "        sign = rng.choice([-1.0, 1.0], size=(cfg.nth,))\n",
    "        th_jump = th0 * (1.0 + sign * cfg.jump_frac)\n",
    "\n",
    "        x = x0.copy()\n",
    "        th = th0.copy()\n",
    "\n",
    "        for t in range(T):\n",
    "            if t == t_jump:\n",
    "                th = th_jump.copy()\n",
    "\n",
    "            x_t = torch.tensor(x, dtype=np.float64)\n",
    "            th_t = torch.tensor(th, dtype=np.float64)\n",
    "            x_next = f_x_theta(x_t, th_t, dt).cpu().numpy()\n",
    "            x_next = x_next + rng.multivariate_normal(np.zeros(cfg.nx), Qx)\n",
    "\n",
    "            y = h_x(torch.tensor(x_next, dtype=np.float64)).cpu().numpy()\n",
    "            y = y + rng.multivariate_normal(np.zeros(cfg.ny), R)\n",
    "\n",
    "            x_all[n, t] = x_next\n",
    "            th_all[n, t] = th\n",
    "            y_all[n, t] = y\n",
    "\n",
    "            x = x_next\n",
    "\n",
    "    return {\"x_true\": x_all, \"th_true\": th_all, \"y_meas\": y_all, \"jump_t\": jump_t_all, \"th_ref\": th_ref}\n",
    "\n",
    "train_data = simulate_dataset(CFG, CFG.n_train, seed=1)\n",
    "val_data   = simulate_dataset(CFG, CFG.n_val, seed=2)\n",
    "test_data  = simulate_dataset(CFG, CFG.n_test, seed=3)\n",
    "\n",
    "print(\"train shapes:\", train_data[\"x_true\"].shape, train_data[\"y_meas\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608cca0a",
   "metadata": {},
   "source": [
    "## 5) Robust UT helpers + SPD utilities\n",
    "\n",
    "This section includes:\n",
    "- Robust Cholesky with automatic jitter escalation.\n",
    "- Sigma point generation.\n",
    "- UT mean/cov and cross-cov.\n",
    "- SPD construction from diagonal parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def symmetrize(P: torch.Tensor) -> torch.Tensor:\n",
    "    return 0.5 * (P + P.transpose(-1, -2))\n",
    "\n",
    "def chol_psd(P: torch.Tensor, jitter: float = 1e-6, max_tries: int = 8) -> torch.Tensor:\n",
    "    P = symmetrize(P)\n",
    "    I = torch.eye(P.shape[-1], device=P.device, dtype=P.dtype)\n",
    "    jit = jitter\n",
    "    for _ in range(max_tries):\n",
    "        try:\n",
    "            return torch.linalg.cholesky(P + jit * I)\n",
    "        except Exception:\n",
    "            jit *= 10.0\n",
    "    w, V = torch.linalg.eigh(P)\n",
    "    w = torch.clamp(w, min=jitter)\n",
    "    P_fix = V @ torch.diag_embed(w) @ V.transpose(-1, -2)\n",
    "    return torch.linalg.cholesky(P_fix + jitter * I)\n",
    "\n",
    "def inv_via_cholesky(S: torch.Tensor) -> torch.Tensor:\n",
    "    L = chol_psd(S)\n",
    "    I = torch.eye(S.shape[-1], device=S.device, dtype=S.dtype).expand(S.shape[0], -1, -1)\n",
    "    return torch.cholesky_solve(I, L)\n",
    "\n",
    "def spd_from_diag(raw: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    s = torch.nn.functional.softplus(raw) + eps\n",
    "    return torch.diag_embed(s**2)\n",
    "\n",
    "def sigma_points(mean: torch.Tensor, cov: torch.Tensor, gamma: float) -> torch.Tensor:\n",
    "    if mean.dim() == 1:\n",
    "        mean = mean.unsqueeze(0)\n",
    "        cov = cov.unsqueeze(0)\n",
    "        squeeze = True\n",
    "    else:\n",
    "        squeeze = False\n",
    "    B, n = mean.shape\n",
    "    S = gamma * chol_psd(cov)\n",
    "    Xi0 = mean.unsqueeze(1)\n",
    "    cols = S.transpose(-1, -2)\n",
    "    Xi = torch.cat([Xi0, Xi0 + cols, Xi0 - cols], dim=1)\n",
    "    return Xi.squeeze(0) if squeeze else Xi\n",
    "\n",
    "def ut_weights(n: int, alpha: float = 1e-3, beta: float = 2.0, kappa: float = 0.0):\n",
    "    lam = alpha**2 * (n + kappa) - n\n",
    "    c = n + lam\n",
    "    Wm = torch.full((2*n+1,), 0.5/c, dtype=torch.get_default_dtype(), device=DEVICE)\n",
    "    Wc = torch.full((2*n+1,), 0.5/c, dtype=torch.get_default_dtype(), device=DEVICE)\n",
    "    Wm[0] = lam / c\n",
    "    Wc[0] = lam / c + (1 - alpha**2 + beta)\n",
    "    gamma = math.sqrt(c)\n",
    "    return Wm, Wc, gamma\n",
    "\n",
    "def ut_mean_cov(Xi: torch.Tensor, Wm: torch.Tensor, Wc: torch.Tensor):\n",
    "    mean = (Xi * Wm.view(1, -1, 1)).sum(dim=1)\n",
    "    d = Xi - mean.unsqueeze(1)\n",
    "    cov = torch.einsum(\"bln,blm,l->bnm\", d, d, Wc)\n",
    "    return mean, symmetrize(cov)\n",
    "\n",
    "def ut_cross_cov(Xi: torch.Tensor, Yi: torch.Tensor, x_mean: torch.Tensor, y_mean: torch.Tensor, Wc: torch.Tensor):\n",
    "    dx = Xi - x_mean.unsqueeze(1)\n",
    "    dy = Yi - y_mean.unsqueeze(1)\n",
    "    return torch.einsum(\"bln,blm,l->bnm\", dx, dy, Wc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c05ae3d",
   "metadata": {},
   "source": [
    "## 6) EKF-aug baseline\n",
    "EKF uses Jacobians to propagate covariance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45e40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def ekf_aug_run(cfg: Config, y: torch.Tensor, z0: torch.Tensor, P0: torch.Tensor,\n",
    "                Qx: torch.Tensor, R: torch.Tensor, th_ref: torch.Tensor,\n",
    "                enable_jump_logic: bool = True):\n",
    "    B, T, _ = y.shape\n",
    "    H = jacobian_H_aug(device=y.device, dtype=y.dtype)\n",
    "\n",
    "    QxB = Qx.unsqueeze(0).expand(B, -1, -1) if Qx.dim() == 2 else Qx\n",
    "    RB  = R.unsqueeze(0).expand(B, -1, -1)  if R.dim() == 2 else R\n",
    "\n",
    "    z, P = z0, P0\n",
    "    z_out, P_out = [], []\n",
    "\n",
    "    consec = torch.zeros((B,), device=y.device, dtype=torch.int64)\n",
    "    jump_count = torch.zeros((B,), device=y.device, dtype=torch.int64)\n",
    "\n",
    "    sigma_base = cfg.qth_base_rel * th_ref.abs()\n",
    "    Qth_base = torch.diag_embed(sigma_base**2).unsqueeze(0).expand(B, -1, -1)\n",
    "    sigma_jump = (cfg.jump_frac / math.sqrt(cfg.jump_J)) * th_ref.abs()\n",
    "    Qth_jump = torch.diag_embed(sigma_jump**2).unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "    I5 = torch.eye(cfg.nz, device=y.device, dtype=y.dtype).unsqueeze(0).expand(B, -1, -1)\n",
    "    I2 = torch.eye(cfg.ny, device=y.device, dtype=y.dtype).unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "    for t in range(T):\n",
    "        x = z[:, :3]\n",
    "        th = z[:, 3:]\n",
    "        x_pred = f_x_theta(x, th, cfg.dt)\n",
    "        z_pred = torch.cat([x_pred, th], dim=-1)\n",
    "\n",
    "        A = jacobian_A_aug(z, cfg.dt)\n",
    "\n",
    "        Qth = Qth_base\n",
    "        if enable_jump_logic:\n",
    "            Qth = torch.where(jump_count.view(B,1,1) > 0, Qth_jump, Qth_base)\n",
    "\n",
    "        Q_aug = torch.zeros((B,5,5), device=y.device, dtype=y.dtype)\n",
    "        Q_aug[:, :3, :3] = QxB\n",
    "        Q_aug[:, 3:, 3:] = Qth\n",
    "\n",
    "        P_pred = A @ P @ A.transpose(-1,-2) + Q_aug\n",
    "        P_pred = symmetrize(P_pred) + 1e-9 * I5\n",
    "\n",
    "        y_pred = h_x(x_pred)\n",
    "        e = y[:, t] - y_pred\n",
    "\n",
    "        S = symmetrize(H @ P_pred @ H.transpose(-1,-2) + RB) + 1e-9 * I2\n",
    "        Sinv = inv_via_cholesky(S)\n",
    "\n",
    "        K = P_pred @ H.transpose(-1,-2) @ Sinv\n",
    "        z = z_pred + torch.einsum(\"bmn,bn->bm\", K, e)\n",
    "\n",
    "        KH = K @ H\n",
    "        P = (I5 - KH) @ P_pred @ (I5 - KH).transpose(-1,-2) + K @ RB @ K.transpose(-1,-2)\n",
    "        P = symmetrize(P) + 1e-9 * I5\n",
    "\n",
    "        if enable_jump_logic:\n",
    "            nis = torch.einsum(\"bi,bij,bj->b\", e, Sinv, e)\n",
    "            exceed = nis > cfg.nis_tau\n",
    "            consec = torch.where(exceed, consec + 1, torch.zeros_like(consec))\n",
    "            trigger = consec >= cfg.jump_M\n",
    "            jump_count = torch.where(trigger, torch.full_like(jump_count, cfg.jump_J), jump_count)\n",
    "            consec = torch.where(trigger, torch.zeros_like(consec), consec)\n",
    "            jump_count = torch.clamp(jump_count - 1, min=0)\n",
    "\n",
    "        z_out.append(z); P_out.append(P)\n",
    "\n",
    "    return torch.stack(z_out, dim=1), torch.stack(P_out, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c905046",
   "metadata": {},
   "source": [
    "## 7) UKF-aug baseline\n",
    "Uses the standard UKF covariance update: `P = P_pred - K S K^T`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c221f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def ukf_aug_run(cfg: Config, y: torch.Tensor, z0: torch.Tensor, P0: torch.Tensor,\n",
    "                Qx: torch.Tensor, R: torch.Tensor, th_ref: torch.Tensor,\n",
    "                enable_jump_logic: bool = True,\n",
    "                ut_alpha: float = 1e-3, ut_beta: float = 2.0, ut_kappa: float = 0.0):\n",
    "    B, T, _ = y.shape\n",
    "    n = cfg.nz\n",
    "    Wm, Wc, gamma = ut_weights(n, alpha=ut_alpha, beta=ut_beta, kappa=ut_kappa)\n",
    "    Wm, Wc = Wm.to(y.device), Wc.to(y.device)\n",
    "\n",
    "    QxB = Qx.unsqueeze(0).expand(B, -1, -1) if Qx.dim() == 2 else Qx\n",
    "    RB  = R.unsqueeze(0).expand(B, -1, -1)  if R.dim() == 2 else R\n",
    "\n",
    "    z, P = z0, P0\n",
    "    z_out, P_out = [], []\n",
    "\n",
    "    consec = torch.zeros((B,), device=y.device, dtype=torch.int64)\n",
    "    jump_count = torch.zeros((B,), device=y.device, dtype=torch.int64)\n",
    "\n",
    "    sigma_base = cfg.qth_base_rel * th_ref.abs()\n",
    "    Qth_base = torch.diag_embed(sigma_base**2).unsqueeze(0).expand(B, -1, -1)\n",
    "    sigma_jump = (cfg.jump_frac / math.sqrt(cfg.jump_J)) * th_ref.abs()\n",
    "    Qth_jump = torch.diag_embed(sigma_jump**2).unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "    I5 = torch.eye(cfg.nz, device=y.device, dtype=y.dtype).unsqueeze(0).expand(B, -1, -1)\n",
    "    I2 = torch.eye(cfg.ny, device=y.device, dtype=y.dtype).unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "    for t in range(T):\n",
    "        Xi = sigma_points(z, P, gamma)\n",
    "        Xi_x, Xi_th = Xi[..., :3], Xi[..., 3:]\n",
    "        X_pred = f_x_theta(Xi_x, Xi_th, cfg.dt)\n",
    "        Zi_pred = torch.cat([X_pred, Xi_th], dim=-1)\n",
    "\n",
    "        z_pred, P_pred_noQ = ut_mean_cov(Zi_pred, Wm, Wc)\n",
    "\n",
    "        Qth = Qth_base\n",
    "        if enable_jump_logic:\n",
    "            Qth = torch.where(jump_count.view(B,1,1) > 0, Qth_jump, Qth_base)\n",
    "\n",
    "        Q_aug = torch.zeros((B,5,5), device=y.device, dtype=y.dtype)\n",
    "        Q_aug[:, :3, :3] = QxB\n",
    "        Q_aug[:, 3:, 3:] = Qth\n",
    "\n",
    "        P_pred = symmetrize(P_pred_noQ + Q_aug) + 1e-9 * I5\n",
    "\n",
    "        Yi = h_x(Zi_pred[..., :3])\n",
    "        y_pred, S_noR = ut_mean_cov(Yi, Wm, Wc)\n",
    "        Pzy = ut_cross_cov(Zi_pred, Yi, z_pred, y_pred, Wc)\n",
    "\n",
    "        S = symmetrize(S_noR + RB) + 1e-9 * I2\n",
    "        Sinv = inv_via_cholesky(S)\n",
    "        K = torch.einsum(\"bmn,bnk->bmk\", Pzy, Sinv)\n",
    "\n",
    "        e = y[:, t] - y_pred\n",
    "        z = z_pred + torch.einsum(\"bmn,bn->bm\", K, e)\n",
    "\n",
    "        P = symmetrize(P_pred - K @ S @ K.transpose(-1, -2)) + 1e-9 * I5\n",
    "\n",
    "        if enable_jump_logic:\n",
    "            nis = torch.einsum(\"bi,bij,bj->b\", e, Sinv, e)\n",
    "            exceed = nis > cfg.nis_tau\n",
    "            consec = torch.where(exceed, consec + 1, torch.zeros_like(consec))\n",
    "            trigger = consec >= cfg.jump_M\n",
    "            jump_count = torch.where(trigger, torch.full_like(jump_count, cfg.jump_J), jump_count)\n",
    "            consec = torch.where(trigger, torch.zeros_like(consec), consec)\n",
    "            jump_count = torch.clamp(jump_count - 1, min=0)\n",
    "\n",
    "        z_out.append(z); P_out.append(P)\n",
    "\n",
    "    return torch.stack(z_out, dim=1), torch.stack(P_out, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d91c1",
   "metadata": {},
   "source": [
    "## 8) UKN model: UKF backbone + learnable Qx + GRU-adaptive Rt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e2100b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UKN(nn.Module):\n",
    "    def __init__(self, cfg: Config, th_ref: np.ndarray):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.qx_raw = nn.Parameter(torch.zeros(cfg.nx, dtype=torch.get_default_dtype()))\n",
    "\n",
    "        self.gru = nn.GRU(input_size=11, hidden_size=cfg.gru_hidden, batch_first=True)\n",
    "        self.head_R = nn.Linear(cfg.gru_hidden, cfg.ny)\n",
    "\n",
    "        self.uR0 = nn.Parameter(torch.zeros(cfg.ny, dtype=torch.get_default_dtype()))\n",
    "\n",
    "        th_ref_t = torch.tensor(th_ref, device=DEVICE, dtype=torch.get_default_dtype())\n",
    "        self.register_buffer(\"th_ref\", th_ref_t)\n",
    "\n",
    "        Wm, Wc, gamma = ut_weights(cfg.nz, alpha=1e-3, beta=2.0, kappa=0.0)\n",
    "        self.register_buffer(\"Wm\", Wm)\n",
    "        self.register_buffer(\"Wc\", Wc)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def Qx(self, B: int, device) -> torch.Tensor:\n",
    "        return spd_from_diag(self.qx_raw).to(device).unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "    def Qtheta_base_jump(self, B: int, device):\n",
    "        sigma_base = self.cfg.qth_base_rel * self.th_ref.abs()\n",
    "        Qth_base = torch.diag_embed(sigma_base**2).to(device).unsqueeze(0).expand(B, -1, -1)\n",
    "        sigma_jump = (self.cfg.jump_frac / math.sqrt(self.cfg.jump_J)) * self.th_ref.abs()\n",
    "        Qth_jump = torch.diag_embed(sigma_jump**2).to(device).unsqueeze(0).expand(B, -1, -1)\n",
    "        return Qth_base, Qth_jump\n",
    "\n",
    "    def forward(self, y: torch.Tensor, z0: torch.Tensor, P0: torch.Tensor, enable_jump_logic: bool = True):\n",
    "        cfg = self.cfg\n",
    "        device = y.device\n",
    "        B, T, _ = y.shape\n",
    "        Wm, Wc = self.Wm.to(device), self.Wc.to(device)\n",
    "        gamma = self.gamma\n",
    "\n",
    "        QxB = self.Qx(B, device)\n",
    "        Qth_base, Qth_jump = self.Qtheta_base_jump(B, device)\n",
    "\n",
    "        z, P = z0, P0\n",
    "        uR = self.uR0.to(device).unsqueeze(0).expand(B, -1)\n",
    "        h = torch.zeros((1, B, cfg.gru_hidden), device=device, dtype=y.dtype)\n",
    "\n",
    "        consec = torch.zeros((B,), device=device, dtype=torch.int64)\n",
    "        jump_count = torch.zeros((B,), device=device, dtype=torch.int64)\n",
    "\n",
    "        e_prev = torch.zeros((B, cfg.ny), device=device, dtype=y.dtype)\n",
    "\n",
    "        z_list, P_list = [], []\n",
    "        Rt_diag_list, duR_list, uR_list, nis_list, jc_list = [], [], [], [], []\n",
    "\n",
    "        I5 = torch.eye(cfg.nz, device=device, dtype=y.dtype).unsqueeze(0).expand(B, -1, -1)\n",
    "        I2 = torch.eye(cfg.ny, device=device, dtype=y.dtype).unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        for t in range(T):\n",
    "            Xi = sigma_points(z, P, gamma)\n",
    "            Xi_x, Xi_th = Xi[..., :3], Xi[..., 3:]\n",
    "            X_pred = f_x_theta(Xi_x, Xi_th, cfg.dt)\n",
    "            Zi_pred = torch.cat([X_pred, Xi_th], dim=-1)\n",
    "            z_pred, P_pred_noQ = ut_mean_cov(Zi_pred, Wm, Wc)\n",
    "\n",
    "            Qth = Qth_base\n",
    "            if enable_jump_logic:\n",
    "                Qth = torch.where(jump_count.view(B,1,1) > 0, Qth_jump, Qth_base)\n",
    "\n",
    "            Q_aug = torch.zeros((B,5,5), device=device, dtype=y.dtype)\n",
    "            Q_aug[:, :3, :3] = QxB\n",
    "            Q_aug[:, 3:, 3:] = Qth\n",
    "            P_pred = symmetrize(P_pred_noQ + Q_aug) + 1e-9 * I5\n",
    "\n",
    "            Yi = h_x(Zi_pred[..., :3])\n",
    "            y_pred, S_noR = ut_mean_cov(Yi, Wm, Wc)\n",
    "            Pzy = ut_cross_cov(Zi_pred, Yi, z_pred, y_pred, Wc)\n",
    "\n",
    "            e = y[:, t] - y_pred\n",
    "            de = e - e_prev\n",
    "            e_prev = e\n",
    "\n",
    "            diagP_pred_x = torch.diagonal(P_pred[:, :3, :3], dim1=-2, dim2=-1)\n",
    "            diagS_noR = torch.diagonal(S_noR, dim1=-2, dim2=-1)\n",
    "            feat = torch.cat([e, de, e.abs(), diagP_pred_x, diagS_noR], dim=-1)\n",
    "\n",
    "            out, h = self.gru(feat.unsqueeze(1), h)\n",
    "            duR = self.head_R(out.squeeze(1))\n",
    "\n",
    "            if cfg.freeze_R_during_jump and enable_jump_logic:\n",
    "                duR = torch.where((jump_count > 0).unsqueeze(-1), torch.zeros_like(duR), duR)\n",
    "\n",
    "            uR = uR + duR\n",
    "            uR = torch.clamp(uR, min=-20.0, max=20.0)\n",
    "\n",
    "            R_t = spd_from_diag(uR)\n",
    "            Rt_diag_list.append(torch.diagonal(R_t, dim1=-2, dim2=-1))\n",
    "            duR_list.append(duR)\n",
    "            uR_list.append(uR)\n",
    "\n",
    "            S = symmetrize(S_noR + R_t) + 1e-9 * I2\n",
    "            Sinv = inv_via_cholesky(S)\n",
    "            K = torch.einsum(\"bmn,bnk->bmk\", Pzy, Sinv)\n",
    "\n",
    "            z = z_pred + torch.einsum(\"bmn,bn->bm\", K, e)\n",
    "            P = symmetrize(P_pred - K @ S @ K.transpose(-1, -2)) + 1e-9 * I5\n",
    "\n",
    "            nis = torch.einsum(\"bi,bij,bj->b\", e, Sinv, e)\n",
    "            nis_list.append(nis)\n",
    "            jc_list.append(jump_count)\n",
    "\n",
    "            if enable_jump_logic:\n",
    "                exceed = nis > cfg.nis_tau\n",
    "                consec = torch.where(exceed, consec + 1, torch.zeros_like(consec))\n",
    "                trigger = consec >= cfg.jump_M\n",
    "                jump_count = torch.where(trigger, torch.full_like(jump_count, cfg.jump_J), jump_count)\n",
    "                consec = torch.where(trigger, torch.zeros_like(consec), consec)\n",
    "                jump_count = torch.clamp(jump_count - 1, min=0)\n",
    "\n",
    "            z_list.append(z); P_list.append(P)\n",
    "\n",
    "        return {\n",
    "            \"z_filt\": torch.stack(z_list, dim=1),\n",
    "            \"P_filt\": torch.stack(P_list, dim=1),\n",
    "            \"Rt_diag\": torch.stack(Rt_diag_list, dim=1),\n",
    "            \"duR\": torch.stack(duR_list, dim=1),\n",
    "            \"uR\": torch.stack(uR_list, dim=1),\n",
    "            \"nis\": torch.stack(nis_list, dim=1),\n",
    "            \"jump_count\": torch.stack(jc_list, dim=1),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb32efb3",
   "metadata": {},
   "source": [
    "## 9) Loss functions: MSE + Gaussian NLL (for P_hat)\n",
    "\n",
    "We combine them with adaptive learnable weights:\n",
    "`Loss = exp(-s1)*MSE + s1 + exp(-s2)*NLL + s2`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38994489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_nll(z_true: torch.Tensor, z_hat: torch.Tensor, P_hat: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    # Mean Gaussian NLL over batch/time for z ~ N(z_hat, P_hat)\n",
    "    B, T, D = z_true.shape\n",
    "    I = torch.eye(D, device=z_true.device, dtype=z_true.dtype).view(1,1,D,D)\n",
    "\n",
    "    P = symmetrize(P_hat) + eps * I\n",
    "    e = (z_true - z_hat).unsqueeze(-1)\n",
    "\n",
    "    P2 = P.reshape(B*T, D, D)\n",
    "    e2 = e.reshape(B*T, D, 1)\n",
    "\n",
    "    L2 = chol_psd(P2, jitter=eps)\n",
    "    sol2 = torch.cholesky_solve(e2, L2)\n",
    "    quad2 = (e2.transpose(-1,-2) @ sol2).squeeze(-1).squeeze(-1)\n",
    "\n",
    "    logdet2 = 2.0 * torch.log(torch.diagonal(L2, dim1=-2, dim2=-1)).sum(-1)\n",
    "    const = D * math.log(2.0 * math.pi)\n",
    "    nll2 = 0.5 * (logdet2 + quad2 + const)\n",
    "\n",
    "    return nll2.mean()\n",
    "\n",
    "\n",
    "class ReLoBRaLo:\n",
    "    # ReLoBRaLo-style adaptive loss weighting for K losses.\n",
    "    # - Maintains EMA-smoothed losses.\n",
    "    # - Uses random lookback reference: initial (L0) vs previous (L_prev).\n",
    "    # - Weights ∝ (EMA / ref)^alpha, normalized so sum=K.\n",
    "    def __init__(self, K: int, alpha: float = 1.0, ema: float = 0.99, eps: float = 1e-12, p_lookback_init: float = 0.5):\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.ema = ema\n",
    "        self.eps = eps\n",
    "        self.p_init = p_lookback_init\n",
    "\n",
    "        self.L0 = None\n",
    "        self.L_prev = None\n",
    "        self.L_ema = None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, losses: torch.Tensor) -> torch.Tensor:\n",
    "        # losses: (K,) tensor (detached). returns: (K,) weights\n",
    "        if self.L0 is None:\n",
    "            self.L0 = losses.clone()\n",
    "            self.L_prev = losses.clone()\n",
    "            self.L_ema = losses.clone()\n",
    "\n",
    "        # EMA update\n",
    "        self.L_ema = self.ema * self.L_ema + (1.0 - self.ema) * losses\n",
    "\n",
    "        # random lookback\n",
    "        if torch.rand((), device=losses.device) < self.p_init:\n",
    "            ref = self.L0\n",
    "        else:\n",
    "            ref = self.L_prev\n",
    "\n",
    "        ratio = self.L_ema / (ref + self.eps)\n",
    "        w = ratio.pow(self.alpha)\n",
    "\n",
    "        # normalize: sum = K\n",
    "        w = self.K * w / (w.sum() + self.eps)\n",
    "\n",
    "        # update previous snapshot\n",
    "        self.L_prev = self.L_ema.clone()\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7997f80",
   "metadata": {},
   "source": [
    "## 10) Training utilities (batching + init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbe2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_torch_batch(batch_np: Dict[str, np.ndarray], idx: np.ndarray, device=DEVICE):\n",
    "    x = torch.tensor(batch_np[\"x_true\"][idx], device=device)\n",
    "    th = torch.tensor(batch_np[\"th_true\"][idx], device=device)\n",
    "    y = torch.tensor(batch_np[\"y_meas\"][idx], device=device)\n",
    "    z_true = torch.cat([x, th], dim=-1)\n",
    "    return y, z_true\n",
    "\n",
    "def init_filter_states(cfg: Config, B: int, th_ref: np.ndarray, device=DEVICE):\n",
    "    x0 = torch.zeros((B, cfg.nx), device=device, dtype=torch.get_default_dtype())\n",
    "    th0 = torch.tensor(th_ref, device=device, dtype=torch.get_default_dtype()).unsqueeze(0).expand(B, -1)\n",
    "    z0 = torch.cat([x0, th0], dim=-1)\n",
    "    P0 = torch.diag_embed(\n",
    "        torch.tensor([0.5, 0.5, 0.5, 0.2, 0.2], device=device, dtype=torch.get_default_dtype())**2\n",
    "    ).expand(B, -1, -1)\n",
    "    return z0, P0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3bacf1",
   "metadata": {},
   "source": [
    "## 11) Train UKN with ReLoBRaLo adaptive weights\n",
    "\n",
    "We train UKN using two supervised objectives:\n",
    "- **MSE** on the filtered augmented state `z=[x;θ]`\n",
    "- **Gaussian NLL** using the filter covariance `P`\n",
    "\n",
    "**ReLoBRaLo** updates weights every mini-batch based on relative progress (EMA / random lookback).\n",
    "We log:\n",
    "- total training loss (weighted)\n",
    "- component losses (MSE, NLL)\n",
    "- adaptive weights `w_mse`, `w_nll`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e146c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ukn = UKN(CFG, train_data[\"th_ref\"]).to(DEVICE)\n",
    "\n",
    "# ReLoBRaLo hyperparameters (tune if needed)\n",
    "RELOB_ALPHA = 1.0\n",
    "RELOB_EMA   = 0.99\n",
    "RELOB_P_INIT = 0.5  # probability of using initial loss as lookback reference\n",
    "\n",
    "relobalo = ReLoBRaLo(K=2, alpha=RELOB_ALPHA, ema=RELOB_EMA, p_lookback_init=RELOB_P_INIT)\n",
    "\n",
    "opt = optim.Adam(ukn.parameters(), lr=CFG.lr)\n",
    "\n",
    "train_loss_hist, val_loss_hist = [], []\n",
    "mse_hist, nll_hist = [], []\n",
    "w_mse_hist, w_nll_hist = [], []\n",
    "\n",
    "for epoch in range(1, CFG.epochs + 1):\n",
    "    ukn.train()\n",
    "    perm = np.random.permutation(CFG.n_train)\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    epoch_mse = 0.0\n",
    "    epoch_nll = 0.0\n",
    "    epoch_wmse = 0.0\n",
    "    epoch_wnll = 0.0\n",
    "    nbatch = 0\n",
    "\n",
    "    for i0 in range(0, CFG.n_train, CFG.batch_size):\n",
    "        idx = perm[i0:i0+CFG.batch_size]\n",
    "        B = len(idx)\n",
    "\n",
    "        y_b, z_true_b = to_torch_batch(train_data, idx, device=DEVICE)\n",
    "        z0, P0 = init_filter_states(CFG, B, train_data[\"th_ref\"], device=DEVICE)\n",
    "\n",
    "        out = ukn(y_b, z0, P0, enable_jump_logic=CFG.enable_jump_logic_train)\n",
    "        z_hat = out[\"z_filt\"]\n",
    "        P_hat = out[\"P_filt\"]\n",
    "\n",
    "        L_mse = torch.mean((z_hat - z_true_b)**2)\n",
    "        L_nll = gaussian_nll(z_true_b, z_hat, P_hat, eps=1e-6)\n",
    "\n",
    "        # ReLoBRaLo weights (no-grad)\n",
    "        with torch.no_grad():\n",
    "            w = relobalo.update(torch.stack([L_mse.detach(), L_nll.detach()]))\n",
    "        w_mse_t, w_nll_t = w[0], w[1]\n",
    "\n",
    "        loss = w_mse_t * L_mse + w_nll_t * L_nll\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(ukn.parameters(), CFG.grad_clip)\n",
    "        opt.step()\n",
    "\n",
    "        epoch_loss += float(loss.detach().cpu())\n",
    "        epoch_mse  += float(L_mse.detach().cpu())\n",
    "        epoch_nll  += float(L_nll.detach().cpu())\n",
    "        epoch_wmse += float(w_mse_t.detach().cpu())\n",
    "        epoch_wnll += float(w_nll_t.detach().cpu())\n",
    "        nbatch += 1\n",
    "\n",
    "    # epoch averages\n",
    "    train_loss_hist.append(epoch_loss / max(nbatch, 1))\n",
    "    mse_hist.append(epoch_mse / max(nbatch, 1))\n",
    "    nll_hist.append(epoch_nll / max(nbatch, 1))\n",
    "    w_mse_epoch = epoch_wmse / max(nbatch, 1)\n",
    "    w_nll_epoch = epoch_wnll / max(nbatch, 1)\n",
    "    w_mse_hist.append(w_mse_epoch)\n",
    "    w_nll_hist.append(w_nll_epoch)\n",
    "\n",
    "    # validation (no weight-state updates; use epoch-average weights)\n",
    "    ukn.eval()\n",
    "    with torch.no_grad():\n",
    "        permv = np.random.permutation(CFG.n_val)\n",
    "        vloss, vb = 0.0, 0\n",
    "        for j0 in range(0, CFG.n_val, CFG.batch_size):\n",
    "            idx = permv[j0:j0+CFG.batch_size]\n",
    "            B = len(idx)\n",
    "\n",
    "            y_b, z_true_b = to_torch_batch(val_data, idx, device=DEVICE)\n",
    "            z0, P0 = init_filter_states(CFG, B, val_data[\"th_ref\"], device=DEVICE)\n",
    "\n",
    "            out = ukn(y_b, z0, P0, enable_jump_logic=CFG.enable_jump_logic_train)\n",
    "            z_hat = out[\"z_filt\"]\n",
    "            P_hat = out[\"P_filt\"]\n",
    "\n",
    "            L_mse = torch.mean((z_hat - z_true_b)**2)\n",
    "            L_nll = gaussian_nll(z_true_b, z_hat, P_hat, eps=1e-6)\n",
    "\n",
    "            loss = w_mse_epoch * L_mse + w_nll_epoch * L_nll\n",
    "            vloss += float(loss.detach().cpu())\n",
    "            vb += 1\n",
    "\n",
    "        val_loss_hist.append(vloss / max(vb, 1))\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"epoch {epoch:03d} | train {train_loss_hist[-1]:.6e} | val {val_loss_hist[-1]:.6e} | \"\n",
    "              f\"w_mse {w_mse_hist[-1]:.3e} w_nll {w_nll_hist[-1]:.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a4d78c",
   "metadata": {},
   "source": [
    "## 12) Test evaluation + save results\n",
    "Saves to a local file in the current working directory (Windows-friendly).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3010222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ukn.eval()\n",
    "\n",
    "y_test = torch.tensor(test_data[\"y_meas\"], device=DEVICE)\n",
    "x_true = torch.tensor(test_data[\"x_true\"], device=DEVICE)\n",
    "th_true = torch.tensor(test_data[\"th_true\"], device=DEVICE)\n",
    "z_true = torch.cat([x_true, th_true], dim=-1)\n",
    "\n",
    "Btest = y_test.shape[0]\n",
    "z0, P0 = init_filter_states(CFG, Btest, test_data[\"th_ref\"], device=DEVICE)\n",
    "\n",
    "Qx_true = torch.diag(torch.tensor(CFG.Qx_true_diag, device=DEVICE, dtype=torch.get_default_dtype()))\n",
    "R_true  = torch.diag(torch.tensor(CFG.R_true_diag, device=DEVICE, dtype=torch.get_default_dtype()))\n",
    "th_ref_t = torch.tensor(test_data[\"th_ref\"], device=DEVICE, dtype=torch.get_default_dtype())\n",
    "\n",
    "z_ekf, P_ekf = ekf_aug_run(CFG, y_test, z0, P0, Qx_true, R_true, th_ref_t, enable_jump_logic=CFG.enable_jump_logic_eval)\n",
    "z_ukf, P_ukf = ukf_aug_run(CFG, y_test, z0, P0, Qx_true, R_true, th_ref_t, enable_jump_logic=CFG.enable_jump_logic_eval)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_ukn = ukn(y_test, z0, P0, enable_jump_logic=CFG.enable_jump_logic_eval)\n",
    "    z_ukn, P_ukn = out_ukn[\"z_filt\"], out_ukn[\"P_filt\"]\n",
    "    Rt_diag = out_ukn[\"Rt_diag\"]\n",
    "    duR = out_ukn[\"duR\"]\n",
    "    uR = out_ukn[\"uR\"]\n",
    "    nis = out_ukn[\"nis\"]\n",
    "    jump_count = out_ukn[\"jump_count\"]\n",
    "\n",
    "Z995 = 2.5758293035489004\n",
    "diag_cov = lambda P: torch.diagonal(P, dim1=-2, dim2=-1)\n",
    "\n",
    "e_ekf = z_ekf - z_true\n",
    "e_ukf = z_ukf - z_true\n",
    "e_ukn = z_ukn - z_true\n",
    "\n",
    "ci_ekf = Z995 * torch.sqrt(torch.clamp(diag_cov(P_ekf), min=1e-12))\n",
    "ci_ukf = Z995 * torch.sqrt(torch.clamp(diag_cov(P_ukf), min=1e-12))\n",
    "ci_ukn = Z995 * torch.sqrt(torch.clamp(diag_cov(P_ukn), min=1e-12))\n",
    "\n",
    "rmse_t_ekf = torch.sqrt(torch.mean(e_ekf**2, dim=(0,2)))\n",
    "rmse_t_ukf = torch.sqrt(torch.mean(e_ukf**2, dim=(0,2)))\n",
    "rmse_t_ukn = torch.sqrt(torch.mean(e_ukn**2, dim=(0,2)))\n",
    "\n",
    "RESULTS_PATH = os.path.join(os.getcwd(), \"ukn_augmented_stepchange_results_with_adaptive_weights.npz\")\n",
    "np.savez(\n",
    "    RESULTS_PATH,\n",
    "    train_loss=np.array(train_loss_hist),\n",
    "    val_loss=np.array(val_loss_hist),\n",
    "    mse=np.array(mse_hist),\n",
    "    nll=np.array(nll_hist),\n",
    "    w_mse=np.array(w_mse_hist),\n",
    "    w_nll=np.array(w_nll_hist),\n",
    "\n",
    "    z_true=z_true.detach().cpu().numpy(),\n",
    "    z_ekf=z_ekf.detach().cpu().numpy(),\n",
    "    z_ukf=z_ukf.detach().cpu().numpy(),\n",
    "    z_ukn=z_ukn.detach().cpu().numpy(),\n",
    "\n",
    "    ci_ekf=ci_ekf.detach().cpu().numpy(),\n",
    "    ci_ukf=ci_ukf.detach().cpu().numpy(),\n",
    "    ci_ukn=ci_ukn.detach().cpu().numpy(),\n",
    "\n",
    "    rmse_t_ekf=rmse_t_ekf.detach().cpu().numpy(),\n",
    "    rmse_t_ukf=rmse_t_ukf.detach().cpu().numpy(),\n",
    "    rmse_t_ukn=rmse_t_ukn.detach().cpu().numpy(),\n",
    "\n",
    "    jump_t=test_data[\"jump_t\"],\n",
    "\n",
    "    Rt_diag=Rt_diag.detach().cpu().numpy(),\n",
    "    duR=duR.detach().cpu().numpy(),\n",
    "    uR=uR.detach().cpu().numpy(),\n",
    "    nis=nis.detach().cpu().numpy(),\n",
    "    jump_count=jump_count.detach().cpu().numpy(),\n",
    ")\n",
    "print(\"Saved results to:\", RESULTS_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf26b32",
   "metadata": {},
   "source": [
    "## 13) FINAL: Plotting\n",
    "Includes:\n",
    "- Training curve\n",
    "- ReLoBRaLo adaptive weights\n",
    "- Estimate vs True (one sequence)\n",
    "- Method-wise error + 99% CI (subplots)\n",
    "- RMSE over time (direct comparison)\n",
    "- UKN Rt diagnostics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf4916",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = np.load(RESULTS_PATH, allow_pickle=True)\n",
    "\n",
    "train_loss = data[\"train_loss\"]\n",
    "val_loss = data[\"val_loss\"]\n",
    "mse_hist = data[\"mse\"]\n",
    "nll_hist = data[\"nll\"]\n",
    "w_mse = data[\"w_mse\"]\n",
    "w_nll = data[\"w_nll\"]\n",
    "\n",
    "z_true = data[\"z_true\"]\n",
    "z_ekf  = data[\"z_ekf\"]\n",
    "z_ukf  = data[\"z_ukf\"]\n",
    "z_ukn  = data[\"z_ukn\"]\n",
    "\n",
    "ci_ekf = data[\"ci_ekf\"]\n",
    "ci_ukf = data[\"ci_ukf\"]\n",
    "ci_ukn = data[\"ci_ukn\"]\n",
    "\n",
    "rmse_t_ekf = data[\"rmse_t_ekf\"]\n",
    "rmse_t_ukf = data[\"rmse_t_ukf\"]\n",
    "rmse_t_ukn = data[\"rmse_t_ukn\"]\n",
    "\n",
    "jump_t = data[\"jump_t\"]\n",
    "Rt_diag = data[\"Rt_diag\"]\n",
    "duR = data[\"duR\"]\n",
    "nis = data[\"nis\"]\n",
    "\n",
    "seq_id = 0\n",
    "t = np.arange(CFG.T)\n",
    "tj = int(jump_t[seq_id])\n",
    "\n",
    "state_names = [\"x1\", \"x2\", \"x3\"]\n",
    "param_names = [\"θ1\", \"θ2\"]\n",
    "all_names = state_names + param_names\n",
    "\n",
    "# (1) training curve\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(train_loss, label=\"train total\")\n",
    "plt.plot(val_loss, label=\"val total\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss (log)\")\n",
    "plt.title(\"UKN training curve (total loss)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# adaptive weights\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(w_mse, label=\"w_mse\")\n",
    "plt.plot(w_nll, label=\"w_nll\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.title(\"Adaptive loss weights over epochs\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(mse_hist, label=\"MSE component\")\n",
    "plt.plot(nll_hist, label=\"NLL component\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.title(\"Component losses over epochs\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (2) estimate vs true\n",
    "zT = z_true[seq_id]\n",
    "zE = z_ekf[seq_id]\n",
    "zU = z_ukf[seq_id]\n",
    "zN = z_ukn[seq_id]\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(12, 12), sharex=True)\n",
    "for i in range(5):\n",
    "    axes[i].plot(t, zT[:, i], label=\"True\", linewidth=2.0)\n",
    "    axes[i].plot(t, zE[:, i], label=\"EKF\")\n",
    "    axes[i].plot(t, zU[:, i], label=\"UKF\")\n",
    "    axes[i].plot(t, zN[:, i], label=\"UKN\")\n",
    "    axes[i].set_title(f\"Estimate vs True: {all_names[i]}\")\n",
    "    axes[i].axvline(tj, linestyle=\"--\", linewidth=1.0)\n",
    "    axes[i].grid(True, alpha=0.2)\n",
    "axes[-1].set_xlabel(\"time step\")\n",
    "axes[0].legend(loc=\"upper right\", ncol=4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "def plot_method_errors_with_ci(method_name, err, ci, titles, t, jump_step):\n",
    "    fig, axes = plt.subplots(5, 1, figsize=(12, 12), sharex=True)\n",
    "    for i in range(5):\n",
    "        axes[i].plot(t, err[:, i], label=f\"{method_name} error\")\n",
    "        axes[i].fill_between(t, -ci[:, i], ci[:, i], alpha=0.15, label=\"99% CI band\")\n",
    "        axes[i].axhline(0.0, linewidth=1.0)\n",
    "        axes[i].axvline(jump_step, linestyle=\"--\", linewidth=1.0)\n",
    "        axes[i].set_title(f\"{method_name}: error + 99% CI ({titles[i]})\")\n",
    "        axes[i].grid(True, alpha=0.2)\n",
    "    axes[-1].set_xlabel(\"time step\")\n",
    "    axes[0].legend(loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# (3) errors + CI per method\n",
    "plot_method_errors_with_ci(\"EKF\", zE - zT, ci_ekf[seq_id], all_names, t, tj)\n",
    "plot_method_errors_with_ci(\"UKF\", zU - zT, ci_ukf[seq_id], all_names, t, tj)\n",
    "plot_method_errors_with_ci(\"UKN\", zN - zT, ci_ukn[seq_id], all_names, t, tj)\n",
    "\n",
    "# (4) RMSE direct comparison\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(t, rmse_t_ekf, label=\"EKF\")\n",
    "plt.plot(t, rmse_t_ukf, label=\"UKF\")\n",
    "plt.plot(t, rmse_t_ukn, label=\"UKN\")\n",
    "plt.axvline(tj, linestyle=\"--\", linewidth=1.0)\n",
    "plt.xlabel(\"time step\")\n",
    "plt.ylabel(\"RMSE (batch & all dims)\")\n",
    "plt.title(\"Time-varying RMSE on test set (direct comparison)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# UKN Rt diagnostics\n",
    "fig, ax = plt.subplots(3, 1, figsize=(12, 8), sharex=True)\n",
    "ax[0].plot(t, Rt_diag[seq_id,:,0], label=\"Rt_diag[0]\")\n",
    "ax[0].plot(t, Rt_diag[seq_id,:,1], label=\"Rt_diag[1]\")\n",
    "ax[0].axvline(tj, linestyle=\"--\", linewidth=1.0)\n",
    "ax[0].set_title(\"UKN: Adaptive measurement noise (Rt_diag)\")\n",
    "ax[0].legend()\n",
    "ax[0].grid(True, alpha=0.2)\n",
    "\n",
    "ax[1].plot(t, duR[seq_id,:,0], label=\"ΔuR[0]\")\n",
    "ax[1].plot(t, duR[seq_id,:,1], label=\"ΔuR[1]\")\n",
    "ax[1].axvline(tj, linestyle=\"--\", linewidth=1.0)\n",
    "ax[1].set_title(\"UKN: GRU output driving R adaptation (duR)\")\n",
    "ax[1].legend()\n",
    "ax[1].grid(True, alpha=0.2)\n",
    "\n",
    "ax[2].plot(t, nis[seq_id], label=\"NIS\")\n",
    "ax[2].axhline(CFG.nis_tau, linestyle=\"--\", label=\"tau (chi2 99%)\")\n",
    "ax[2].axvline(tj, linestyle=\"--\", linewidth=1.0)\n",
    "ax[2].set_title(\"UKN: Normalized Innovation Squared (NIS)\")\n",
    "ax[2].legend()\n",
    "ax[2].grid(True, alpha=0.2)\n",
    "\n",
    "ax[2].set_xlabel(\"time step\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
